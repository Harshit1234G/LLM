from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.runnables.base import RunnableSerializable
from langchain.retrievers.multi_query import MultiQueryRetriever


class ChatBot:
    # class attributes
    MODEL: str = 'gpt-4o-mini'
    EMBEDDING_MODEL: str = 'text-embedding-3-large'

    def __init__(self, vector_db_path: str, *, temperature: int = 0.3) -> None:
        """This class creates the chatbot, loads the vector store, initializes the LLM, retriever, chain and history buffer.

        Args:
            vector_db_path (str): Path to the vector database.
            temperature (int, optional): Temperature for the llm. Defaults to 0.3.
        """
        # initializing vector_store, llm, retriever and chain
        self.vector_store = self._load_faiss_index(vector_db_path)
        self.llm = ChatOpenAI(
            model= self.MODEL,
            temperature= temperature
        )
        self.retriever = self._create_retriever()
        self.chain = self._create_chain()


    def _load_faiss_index(self, path: str) -> FAISS:
        """Calls `FAISS.load_local`.

        Args:
            path (str): Path to the `faiss_index` folder.

        Returns:
            FAISS: Loaded vector database.
        """
        vector_store = FAISS.load_local(
            folder_path= path,
            embeddings= OpenAIEmbeddings(model= self.EMBEDDING_MODEL), # using open ai embeddings
            allow_dangerous_deserialization= True                      #! A dangerous deserializaion of pickle file activated, although it is completely safe for non-server applications.
        )

        return vector_store
    

    def _create_retriever(self) -> MultiQueryRetriever:
        """Creates a retriever for RAG Pipeline.

        Returns:
            MultiQueryRetriever: Retrieved documents.
        """
        # prompt for generating different versions of the query
        QUERY_PROMPT = PromptTemplate.from_template(
            template= """ROLE: You improve search queries for document retrieval.
TASK: Generate exactly 4 topic-style search queries (not questions) for the user's question.
CONSTRAINTS:
- Preserve all key entities/terms from the user question.
- Provide: 1 broader, 1 narrower, and 2 specific variants.
- Each line must be distinct, <=12 words, noun-heavy, no filler words.
- Use at least one domain-specific/jargon synonym where natural.
- Maintain the original intent.
USER QUESTION: {question}
OUTPUT FORMAT: Return exactly 4 lines, one query per line. No numbering, bullets, quotes, or extra text.
"""
        )

        # creating the MultiQueryRetriever from the llm and prompt
        retriever = MultiQueryRetriever.from_llm(
            retriever= self.vector_store.as_retriever(
                search_type= 'mmr',                    # maximal margin relevance
                search_kwargs= {'k': 5, 'fetch_k': 20} # fetches 20 documents, select only 5
            ),
            llm= self.llm,
            prompt= QUERY_PROMPT,
            include_original= True                     # including original for question related retrieval
        )

        return retriever
    

    def _create_chain(self) -> RunnableSerializable:
        """Creates a RunnableSerializable chain for actual response generation.

        Returns:
            RunnableSerializable: A `langchain` chain consisting the `chat_history`, `context`, `question`, `PROMPT`, `llm`, and `StrOutputParser`.
        """
        # instructions for the system
        system_instructions = """ROLE: You are an AI clone of Harshitâ€”acting as him in professional conversations. Whether the user refers to you as "Harshit", "you", "he", or "your" you should always respond in Harshit's voice as though you are him.
TASK: Answer questions about Harshit's background, education, experience, projects and skills strictly based on the provided context.
AUDIENCE: Primarily HR professionals and recruiters.
STYLE: Maintain a human-like, professional, friendly, and conversational tone.
CONSTRAINTS: 
- Answer strictly using the context.
- No speculation or external facts.
- If context is very short, give a brief 1-2 sentence answer.  
- If context is rich, answer in <=200 words unless user explicitly asks for detail.  
- If context is missing or irrelevant, reply strictly with: "Sorry! I can only talk about Harshit's Portfolio.".
- End every response with a natural follow-up question that encourages engagement.
"""

        PROMPT = ChatPromptTemplate([
            ('system', system_instructions),
            ('human', 'CONTEXT: {context}\nQUESTION: {question}'),
        ])

        chain = (
            {'context': self.retriever, 'question': RunnablePassthrough()}    # context is the document generated by self.retriever and question is whatever the input passed to the `chain.invoke()`
            | PROMPT
            | self.llm
            | StrOutputParser()    # for getting the text output
        )

        return chain


if __name__ == '__main__':
    from dotenv import load_dotenv
    import logging

    load_dotenv()

    # Set logging for the queries
    logging.basicConfig()
    logging.getLogger("langchain.retrievers.multi_query").setLevel(logging.INFO)

    chat_bot = ChatBot(vector_db_path= r"E:\Python\LLM\NeuroHarshit\Databases\faiss_index")

    while True:
        question = input('\nQuestion: ')

        if question == 'quit':
            break
        
        print('\nResponse:', chat_bot.chain.invoke(question))
