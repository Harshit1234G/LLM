{
  "topic": "Attention Is All You Need",
  "source": "arxiv",
  "wikipedia_docs": "",
  "arxiv_docs": "Index: 1\nTitle: Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\nPublished: 2024-07-22\nAuthors: Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini\nSource: Arxiv research paper\nContent: Attention Is All You Need But You Don’t Need All Of It\nFor Inference of Large Language Models\nGeorgy Tyukin * 1 Gbetondji J-S Dovonon 1 Jean Kaddour 1 Pasquale Minervini 2\nAbstract\nThe inference demand for LLMs has skyrocketed\nin recent months, and serving models with low\nlatencies remains challenging due to the quadratic\ninput length complexity of the attention layers.\nIn this work, we investigate the effect of drop-\nping MLP and attention layers at inference time\non the performance of Llama-v2 models. We\nfind that dropping dreeper attention layers only\nmarginally decreases performance but leads to the\nbest speedups alongside dropping entire layers.\nFor example, removing 33% of attention layers\nin a 13B Llama2 model results in a 1.8% drop in\naverage performance over the OpenLLM bench-\nmark. We also observe that skipping layers except\nthe latter layers reduces performances for more\nlayers skipped, except for skipping the attention\nlayers.\n1. Introduction\nThe ubiquitous deployment of Large Language Models\n(LLMs) results in ever-growing amounts of compute spent\non inference (Patterson et al., 2021; Chen et al., 2023; Kad-\ndour et al., 2023a; Xia et al., 2024; Reid et al., 2024). Fur-\nther, serving models with low latencies remains challenging\nbecause contemporary Transformer architectures employ\nthe self-attention mechanism with quadratic input complex-\nity (Touvron et al., 2023b; Jiang et al., 2023; Bi et al., 2024).\nIn this work, we delve deeper into the concept of layer\nskipping (Fan et al., 2019; Wang et al., 2022a) to reduce\nthe computation on superfluous LLM components. Our\nfindings demonstrate that pruning deeper attention layers\ndoes not significantly affect performance. When applied\nto Llama-v2 (Touvron et al., 2023b), we maintain good\nperformance on the OpenLLM (ARC (Clark et al., 2018),\n*Equal contribution\n1University College London,\nUK\n2University of Edinburgh, UK. Correspondence to: Georgy Tyukin\n<tyukinegor@gmail.com>.\nWork presented at TF2M workshop at ICML 2024, Vienna, Austria.\nPMLR 235, 2024. Copyright 2024 by the author(s).\nHellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al.,\n2021), TruthfulQA (Lin et al., 2022)) benchmarks (Beech-\ning et al., 2023), recording only minimal performance devi-\nations compared to the full model.\n2. Method\n0\n5\n10\n15\n20\n25\n30\n35\n40\nLayer\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nCosine Similarity\nCosine Similarity with previous layer for LLaMA-v2 7b and LLaMA-v2 13\nLLaMA-v2 7b\nLLaMA-v2 13b\nFigure 1. Cosine similarity of Llama-v2 layers with the previous\nlayer: We observe that the deeper the layer, the more its features\nare similar to the previous layer except for the very last layer.\n2.1. Layer skipping\nConsider a Transformer model M with L layers, each\nconsisting of an attention sub-layer followed by a multi-\nlayer perceptron (MLP) sub-layer. We denote each layer as\nMi = (Attentioni, MLPi) for i ∈{1, 2, . . . , L}.\nTo compare the performance of Transformer models when\nskipping specific sub-layers, we create two variants of the\nmodel:\n1. Skipping MLP Layers: We construct a model Mskip MLP\n1\narXiv:2407.15516v1  [cs.LG]  22 Jul 2024\nAttention Is All You Need But You Don’t Need All Of It\nby skipping the MLP sub-layer from the last k layers. The\nresulting model is Mskip MLP = {(Attentioni, MLPi) | i ∈\n{1, 2, . . . , L −k}} ∪{(Attentioni, ∅) | i ∈{L −k +\n1, . . . , L}}.\n2. Skipping Attention Layers: We construct a model\nMskip Attention by skipping the attention sub-layer from the\nlast k layers.\nThe resulting model is Mskip Attention =\n{(Attentioni, MLPi)\n|\ni\n∈\n{1, 2, . . . , L −k}} ∪\n{(∅, MLPi) | i ∈{L −k + 1, . . . , L}}.\n3. Skipping Transformer Blocks: We construct a model\nMskip Attention by skipping the entire last k layers. The re-\nsulting model is Mskip Block = {(Attentioni, MLPi) | i ∈\n{1, 2, . . . , L −k}} ∪{(∅) | i ∈{L −k + 1, . . . , L}}.\nWe then evaluate the performance of these modified models\non the OpenLLM benchmark (Beeching et al., 2023), com-\nparing metrics such as accuracy, computational efficiency,\nand memory usage. This comparison helps in understand-\ning the individual contributions of the attention and MLP\nsub-layers to the overall performance of the Transformer\nmodel.\n(a) Skip attention lay-\ners.\n(b) Skip attention lay-\ners,\nkeep last full\nblock.\n(c) Skip ffwd layers.\n(d) Skip ffwd layers,\nkeep last full block.\n(e) Skip full blocks.\n(f) Skip full blocks,\nkeep last full block.\nFigure 2. Skip mechanisms for skipping single layers and entire\nTransformer blocks (ffwd and attention layers) during inference.\n2.2. Motivation: Are Deeper Layers More Redundant?\nIn Transformer models, the last layers have been shown to\ncontribute less information than earlier layers, making it\npossible to drop those layers at a minimal performance cost\n(Fan et al., 2019; Zhang & He, 2020; Wang et al., 2022a;\nSchuster et al., 2022; Kaddour et al., 2023b; Belrose et al.,\n2023).\nTo verify this, we experiment with removing either the at-\ntention sublayers or the MLP sublayers. Figure 1 shows the\ncosine similarities between a layer’s features and the previ-\nous layer showing that deeper layers have a lower impact\non the features than earlier layers. One notable exception\nto this trend is that the last layer for both Llama-v2 7B and\n13B has the lowest cosine similarity with the previous layer.\nPrevious analysis of the attention mechanism has shown\nthat they can converge to the same value due to attention\ncollapse (Zhai et al., 2023) and token features that also con-\nverge to the same value due to over-smoothing (Wang et al.,\n2022b; Dovonon et al., 2024) or rank collapse (Dong et al.,\n2023), with solutions to these issues typically improving\nperformance (Ali et al., 2023; Choi et al., 2024).\n3. Results\nExperimental Setup\nFor all experiments, we use either\nLlama-v2-7B or Llama-v2-13B (Touvron et al., 2023a;b),\ntwo LLMs trained on trillions of publically available tokens.\nWe experiment with keeping 66%, 75%, 90% and 100% of\nthe network and report the corresponding results in Table 1.\nWe also experiment with removing attention sublayers in\nTable 2, MLP sublayers in Table 3, and a varying number of\nlayers similar to Table 1 but keeping the last layer in Table 4.\n3.1. Chopping Layers\nTable 1. Llama-v2 skipping full layer\nModel\nPerformances\nARC\nHellaSwag\nTruthfulQA\nMMLU\nAverage\n7B-66%\n35.2\n46.8\n46.2\n40.3\n42.1\n7B-75%\n38.3\n53.0\n45.1\n45.9\n45.6\n7B-90%\n47.7\n69.3\n39.6\n46.4\n50.8\n7B-100%\n53.1\n78.6\n38.8\n46.6\n54.3\n13B-66%\n37.8\n46.8\n45.3\n51.8\n45.4\n13B-75%\n40.9\n53.6\n42.5\n53.2\n47.6\n13B-90%\n51.3\n71.3\n37.1\n54.8\n53.6\n13B-100%\n59.6\n82.1\n36.9\n55.4\n58.5\nOn all datasets except TruthfulQA, performance drops\nwhich is expected. It had already been observed that larger\nlanguage models are less truthful (Lin et al., 2022), but we\nnow also observe that reducing the size of already trained\nmodels can also make them more truthful. The observa-\ntion still holds when the last layer is preserved. Skipping\n2\nAttention Is All You Need But You Don’t Need All Of It\nTable 2. Llama-v2 skipping attention sublayers\nModel\nPerformances\nARC\nHellaSwag\nTruthfulQA\nMMLU\nAverage\n7B-66%\n51.2\n77.0\n42.2\n39.4\n52.5\n7B-75%\n52.5\n78.3\n42.3\n41.4\n53.6\n7B-90%\n52.8\n78.9\n40.0\n44.0\n53.9\n7B-100%\n53.1\n78.6\n38.8\n46.6\n54.3\n13B-66%\n55.6\n80.1\n40.1\n51.3\n56.8\n13B-75%\n55.9\n79.7\n39.9\n52.1\n56.9\n13B-90%\n57.0\n81.3\n38.2\n54.8\n57.8\n13B-100%\n59.6\n82.1\n36.9\n55.4\n58.5\nTable 3. Llama-v2 skipping ffwd sublayers\nModel\nPerformances\nARC\nHellaSwag\nTruthfulQA\nMMLU\nAverage\n7B-66%\n35.1\n52.5\n42.2\n43.9\n43.4\n7B-75%\n40.4\n60.3\n39.2\n46.3\n46.6\n7B-90%\n48.5\n71.4\n38.0\n46.1\n51.0\n7B-100%\n53.1\n78.6\n38.8\n46.6\n54.3\n13B-66%\n41.6\n56.9\n40.7\n53.4\n48.2\n13B-75%\n47.3\n65.2\n40.0\n53.2\n51.4\n13B-90%\n54.2\n75.8\n38.3\n54.7\n55.8\n13B-100%\n59.6\n82.1\n36.9\n55.4\n58.5\nattention layers only leads to better results with only a 1.8%\ndecrease in performance when keeping 66% of the network\ncompared to a 13.1% decrease in performance when drop-\nping dropping the MLP layers only. This seems to indicate\nthat MLP layers are more important than attention layers, at\nleast in deeper parts of the network.\n3.2. Last Layer Inclusion\nTable 4. Llama-v2 skip full layers with last layer\nModel\nPerformances\nARC\nHellaSwag\nTruthfulQA\nMMLU\nAverage\n7B-66%\n32.0\n45.8\n46.9\n40.7\n41.3\n7B-75%\n34.5\n49.4\n45.9\n38.3\n42.0\n7B-90%\n46.5\n73.1\n41.8\n41.4\n50.7\n7B-100%\n53.1\n78.6\n38.8\n46.6\n54.3\n13B-66%\n35.1\n50.0\n46.9\n19.1\n37.8\n13B-75%\n38.7\n56.6\n43.7\n25.2\n41.1\n13B-90%\n51.2\n78.1\n38.0\n27.1\n47.9\n13B-100%\n59.6\n82.1\n36.9\n55.4\n58.5\nSurprisingly, we notice that skipping layers except the lat-\nter layers reduces performances for more layers skipped,\nexcept for skipping the attention layers. This is even more\nexaggerated compared to just dropping layers, including the\nlast one. The reason for this could be attributed to the (lack\nof) robustness of feedforward sublayers, as the last layer\nnow has to process perturbed information from earlier lay-\ners. For future work, it would be interesting to see if these\nperformance drops can be compensated by a small amount\nTable 5. Llama-v2 skip attention sublayers with last layer\nModel\nPerformances\nARC\nHellaSwag\nTruthfulQA\nMMLU\nAverage\n7B-66%\n49.3\n77.1\n40.5\n42.5\n52.4\n7B-75%\n51.8\n78.3\n41.1\n44.1\n53.8\n7B-90%\n51.9\n78.7\n39.4\n45.7\n53.9\n7B-100%\n53.1\n78.6\n38.8\n46.6\n54.3\n13B-66%\n56.8\n82.1\n38.0\n50.3\n56.8\n13B-75%\n57.5\n82.1\n37.0\n51.4\n57.0\n13B-90%\n58.9\n82.4\n36.6\n54.5\n58.1\n13B-100%\n59.6\n82.1\n36.9\n55.4\n58.5\nTable 6. Llama-v2 skip ffwd sublayers with last layer\nModel\nPerformances\nARC\nHellaSwag\nTruthfulQA\nMMLU\nAverage\n7B-66%\n32.0\n45.8\n46.9\n39.4\n41.0\n7B-75%\n34.5\n49.4\n45.9\n40.2\n42.5\n7B-90%\n46.5\n73.1\n41.8\n40.2\n50.4\n7B-100%\n53.1\n78.6\n38.8\n46.6\n54.3\n13B-66%\n35.1\n50.0\n46.9\n20.4\n38.1\n13B-75%\n38.7\n56.6\n43.7\n33.6\n43.2\n13B-90%\n51.2\n78.1\n38.0\n34.4\n50.4\n13B-100%\n59.6\n82.1\n36.9\n55.4\n58.5\nof continued training; since model growing techniques for\ntraining seem to not suffer from instabilities (Kaddour et al.,\n2023b).\n3.3. Compute-matched Comparison\nTo measure the efficiency of the networks we conducted\na separate experiment, where we record the time it takes\nfor the model to output a sequence of length 1, averaging\nover 1000 sequences. We conducted this experiment for\nboth 50 and 100 length input sequences. We notice that full\nlayer droppings do improve time costs the best, followed by\nattention sublayers, and then feedforward sublayers which\ndo not impact the speed of processing a lot.\nWe report the time×102 (for clarity) it takes to predict 1\ntoken for 1000 sequences as well as the percentage improve-\nment. We show the results of this experiment for Llama 2\n7B with 0%, 10%, 25%, 33% of layers skipped and we label\nthese as 7B-100%, 7B-90%, 7B-75%, 7B-66% respectively.\nTable 7. Llama-v2 time results, 50 length sequence, no last layer\nModel\nFull\nAttention\nffwd\nTime(s) ×102\n(%)\nTime(s) ×102\n(%)\nTime(s) ×102\n(%)\n7B-66%\n31.35\n32.96\n36.72\n21.47\n43.51\n6.95\n7B-75%\n35.48\n24.12\n39.46\n15.61\n42.88\n8.30\n7B-90%\n43.31\n7.38\n42.93\n8.19\n44.17\n5.53\n7B-100%\n46.76\n0\n-\n-\n-\n-\n3\nAttention Is All You Need But You Don’t Need All Of It\nTable 8. Llama-v2 time results, 50 length sequence, last layer in-\ncluded\nModel\nFull\nAttention\nffwd\nTime(s) ×102\n(%)\nTime(s) ×102\n(%)\nTime(s) ×102\n(%)\n7B-66%\n31.78\n32.04\n36.92\n21.04\n41.31\n11.66\n7B-75%\n34.98\n25.19\n40.24\n13.94\n42.62\n8.85\n7B-90%\n40.92\n12.49\n42.43\n9.26\n43.51\n6.95\n7B-100%\n46.76\n0\n-\n-\n-\n-\nTable 9. Llama-v2 time results, 100 length sequence, no last layer\nModel\nFull\nAttention\nffwd\nTime(s) ×102\n(%)\nTime(s) ×102\n(%)\nTime(s) ×102\n(%)\n7B-66%\n32.36\n32.58\n38.97\n18.18\n43.08\n10.25\n7B-75%\n36.58\n23.79\n41.27\n14.02\n44.13\n8.06\n7B-90%\n43.65\n9.06\n44.62\n7.04\n46.30\n3.54\n7B-100%\n48.00\n0\n-\n-\n-\n-\nTable 10. Llama-v2 time results, 100 length sequence, last layer\nincluded\nModel\nFull\nAttention\nffwd\nTime(s) ×102\n(%)\nTime(s) ×102\n(%)\nTime(s) ×102\n(%)\n7B-66%\n32.05\n33.23\n38.52\n19.75\n42.66\n11.13\n7B-75%\n36.41\n24.15\n41.00\n14.58\n43.92\n8.50\n7B-90%\n43.28\n9.83\n44.27\n7.77\n45.20\n5.83\n7B-100%\n48.00\n0\n-\n-\n-\n-\n4. Related Work\nEarly Exit during inference\nEarly exit methods have also\nbeen proposed in other domains (Graves, 2017; Teerapit-\ntay\n\n---\n\nIndex: 2\nTitle: All the attention you need: Global-local, spatial-channel attention for image retrieval\nPublished: 2021-07-16\nAuthors: Chull Hwan Song, Hye Joo Han, Yannis Avrithis\nSource: Arxiv research paper\nContent: All the attention you need:\nGlobal-local, spatial-channel attention for image retrieval\nChull Hwan Song\nOdd Concepts\nHye Joo Han\nOdd Concepts\nYannis Avrithis\nInria, Univ Rennes, CNRS, IRISA\nAbstract\nWe address representation learning for large-scale\ninstance-level image retrieval. Apart from backbone, train-\ning pipelines and loss functions, popular approaches have\nfocused on different spatial pooling and attention mecha-\nnisms, which are at the core of learning a powerful global\nimage representation. There are different forms of attention\naccording to the interaction of elements of the feature tensor\n(local and global) and the dimensions where it is applied\n(spatial and channel). Unfortunately, each study addresses\nonly one or two forms of attention and applies it to different\nproblems like classiﬁcation, detection or retrieval.\nWe present global-local attention module (GLAM),\nwhich is attached at the end of a backbone network and\nincorporates all four forms of attention: local and global,\nspatial and channel. We obtain a new feature tensor and, by\nspatial pooling, we learn a powerful embedding for image\nretrieval. Focusing on global descriptors, we provide em-\npirical evidence of the interaction of all forms of attention\nand improve the state of the art on standard benchmarks.\n1. Introduction\nInstance-level image retrieval is at the core of visual rep-\nresentation learning and is connected with many problems\nof visual recognition and machine learning, for instance\nmetric learning [30, 26], few-shot learning [42] and unsu-\npervised learning [8]. Many large-scale open datasets [3,\n37, 16, 29, 53], and competitions1 have accelerated progress\nin instance-level image retrieval, which has been trans-\nformed by deep learning [3].\nMany studies on instance-level image retrieval focus\non learning features from convolutional neural networks\n(CNN), while others focus on re-ranking, for instance by\ngraph-based methods [11]. The former can be distinguished\naccording to feature types: local descriptors, reminiscent of\nSIFT [27], where an image is mapped to a few hundred vec-\ntors; and global descriptors, where an image is mapped to a\n1https://www.kaggle.com/c/landmark-retrieval-2020\nsingle vector. In fact, deep learning has brought global de-\nscriptors with astounding performance, while allowing efﬁ-\ncient search. Our study belongs to this type.\nStudies on global descriptors have focused on spatial\npooling [2, 37]. The need for compact, discriminative rep-\nresentations that are resistant to clutter has naturally given\nrise to spatial attention methods [24, 28]. Different kinds\nof attention have been studied in many areas of computer\nvision research. There is also channel attention [20, 9]; lo-\ncal attention, applied independently to elements of the rep-\nresentation (feature map) [54, 25]; global attention, based\non interaction between elements [52, 9]; and combinations\nthereof. Unfortunately, each study has been limited to one or\ntwo kinds of attention only; attention is not always learned;\nand applications vary.\nIt is the objective of our work to perform a compre-\nhensive study of all forms of attention above, apply them\nto instance-level image retrieval and provide a detailed ac-\ncount of their interaction and impact on performance. As\nshown in Figure 1, we collect contextual information from\nimages with both local and global attention, giving rise to\ntwo parallel network streams. Importantly, each operates\non both spatial locations and feature channels. Local at-\ntention is about individual locations and channels; global is\nabout interaction between locations and between channels.\nThe extracted information is separately embedded in local\nand global attention feature maps, which are combined in a\nglobal-local attention feature map before pooling.\nOur contributions can be summarized as follows:\n1. We propose a novel network that consists of both\nglobal and local attention for image retrieval. This is\nthe ﬁrst study that employs both mechanisms.\n2. Each of the global and local attention mechanisms\ncomprises both spatial and channel attention.\n3. Focusing on global descriptors, we provide empirical\nevidence of the interaction of all forms of attention and\nimprove the state of the art on standard benchmarks.\n1\narXiv:2107.08000v1  [cs.CV]  16 Jul 2021\nAl\nc\nc × 1 × 1\n×\n+\nFl\nc\nAl\ns\n1 × h × w\n×\n+\nFl\n×\nc × h × w\nF\n×\n+\nc × h × w\nFgl\nAg\nc\nc × c\n×\nFg\nc\nAg\ns\nhw × hw\n×\n+\nFg\n×\nwl\nw\nwg\nchannel attention\nspatial attention\nfusion\nlocal attention\nglobal attention\nFigure 1: Our global-local attention module (GLAM) involves both channel and spatial attention, as well as both local atten-\ntion (channels/locations weighted independently, based on contextual information obtained by pooling) and global attention\n(based on pairwise interaction between channels/locations). As a result, four attention maps are used: local channel (Al\nc),\nlocal spatial (Al\ns), global channel (Ag\nc) and global spatial (Ag\ns). The input feature map F is weighted into local (Fl) and\nglobal (Fg) attention feature maps, which are fused with F to yield the global-local attention feature map Fgl. The diagram\nis abstract: The four attention modules are shown in more detail in Figures 2, 3, 4, 5.\n2. Related work\nInstance-level image retrieval\nStudies on instance-level\nimage retrieval can be roughly, but not exclusively, di-\nvided into three types: (1) studies on global descriptors\n[3, 16, 24, 53, 2, 37]; (2) studies on local descriptors and\ngeometry-based re-ranking [29, 45, 40, 53]; (3) re-ranking\nby graph-based methods [11, 21, 55]. The ﬁrst two types\nof studies focus on the feature representation, while the last\ntype focuses on re-ranking extracted features.\nStudies on global descriptors focus on spatial pooling\nof CNN feature maps into vectors, including MAC [38],\nSPoC [2], CroW [24], R-MAC [48, 15, 16], GeM [37],\nand NetVLAD [1, 25], as well as learning the representa-\ntion [3, 15, 16, 36, 37]. Studies before deep learning dom-\ninated image retrieval were mostly based on local descrip-\ntors like SIFT [27] and bag-of-words representation [32] or\naggregated descriptors like VLAD [22] or ASMK [46]. Lo-\ncal descriptors have been revived in deep learning, e.g. with\nDELF [29], DELG [5] and ASMK extensions [45, 47].\nWe focus on learning a global descriptor in this work, be-\ncause it is the most efﬁcient in terms of storage and search.\nHowever, our generic attention mechanism produces a fea-\nture tensor and could be applicable to local descriptors as\nwell, if global pooling were replaced by local feature detec-\ntion. Re-ranking methods are complementary to the repre-\nsentation and we do not consider them in this work.\nAttention\nAttention mechanisms have been ﬁrst proposed\nin image classiﬁcation studies focusing on channel at-\nMETHOD\nLOCAL\nGLOBAL\nLRN RET\nSpatial Channel Spatial Channel\nSENet [20]\n✓\n✓\nECA-Net [51]\n✓\n✓\nGCNet [6]\n✓\n✓\nCBAM [54]\n✓\n✓\n✓\nGE [19]\n✓\n✓\nNL-Net [52]\n✓\n✓\nAA-Net [4]\n✓\n✓\nSAN [59]\n✓\n✓\nN3Net [34]\n✓\n✓\nA2-Net [9]\n✓\n✓\nGSoP [14]\n✓\n✓\nOnA [23]\n✓\n✓\nAGeM [17]\n✓\n✓\nCroW [24]\n✓\n✓\n✓\nCRN [25]\n✓\n✓\n✓\nDELF [29]\n✓\n✓\n✓\nDELG [5]\n✓\n✓\n✓\nTolias et al. [47]\n✓\n✓\n✓\nSOLAR [28]\n✓\n✓\n✓\nOurs\n✓\n✓\n✓\n✓\n✓\n✓\nTable 1: Related work on attention. LRN: learned; RET: ap-\nplied to instance-level image retrieval.\ntention [20, 51, 6], spatial attention [19] or both, like\nCBAM [54]. In image retrieval, CroW [24] also employs\n2\nfeature map\nGAP\nconv1d(k)\nsigmoid\nattention map\nc × h × w\nc × 1 × 1\nc × 1 × 1\nF\nAl\nc\nFigure 2: Local channel attention.\nboth spatial and channel attention and can be seen as a pre-\ncursor of CBAM, but, like other studies of spatial attention\non retrieval [41, 23, 17], it is not learned. CRN [25] ap-\nplies spatial attention for feature reweighting and is learned.\nLearned spatial attention mechanisms are common for local\ndescriptors [29, 5, 47].\nWe call the above methods local attention, in the sense\nthat elements of the feature tensor (channels / spatial loca-\ntions), are weighted independently, based on contextual in-\nformation obtained by pooling or learned. By constrast, by\nglobal attention we refer to mechanisms that model inter-\naction between elements of the feature tensor, for example\nbetween channels or between locations.\nIn image classiﬁcation, non-local neural network (NL-\nNet) [52] is maybe the ﬁrst global attention mechanism, fol-\nlowed by similar studies [4, 59, 34]. It is global spatial at-\ntention, allowing interaction between any pair of spatial lo-\ncations. Similarly, there are studies of global channel atten-\ntion, allowing interaction between channels [9, 14]. Global\nattention has focused mostly on image recognition and has\nbeen applied to either spatial or channel attention so far, not\nboth. In image retrieval, SOLAR [28] is a direct application\nof the global spatial attention mechanism of [52].\nTable 1 attempts to categorize related work on atten-\ntion according to whether attention is local or global, spa-\ntial or channel, whether it is learned and whether it is ap-\nplied to instance-level image retrieval. We observe that all\nmethods limit to one or two forms of attention only. Of\nthose studies that focus on image retrieval, many are not\nlearned [23, 17, 24], and of those that are, some are de-\nsigned for local descriptors [29, 47].\nBy contrast, we provide a comprehensive study of all\nforms of attention, global and local, spatial and channel, to\nobtain a learned representation in the form of a tensor that\ncan be used in any way. We spatially pool it into a global\ndescriptor and we study the relative gain of different forms\nof attention in image retrieval.\nfeature map\nconv 1 × 1\nconv 3 × 3\nconv 5 × 5\nconv 7 × 7\nconcat\nconv 1 × 1\nattention map\nc × h × w\n4c′ × h × w\n1 × h × w\nc′ × h × w\ndilated\nconv\nF\nF′\nAl\ns\nFigure 3: Local spatial attention. Convolutional layers in\nblue implemented by dilated convolutions with kernel size\n3 × 3 and dilation factors 1, 3, 5.\n3. Global-local attention\nWe design a global-local attention module (GLAM),\nwhich is attached at the end of a backbone network. Figure 1\nillustrates its main components. We are given a c × h × w\nfeature tensor F, where c is the number of channels, and\nh × w is the spatial resolution. Local attention collects con-\ntext from the image and applies pooling to obtain a c×1×1\nlocal channel attention map Al\nc and a 1 × h × w local spa-\ntial attention map Al\ns. Global attention allows interaction\nbetween channels, resulting in a c × c global channel at-\ntention map Ag\nc, and between spatial locations, resulting in\na hw × hw global spatial attention map Ag\ns. The feature\nmaps produced by the two attention streams are combined\nwith the original one by a learned fusion mechanism into\nthe global-local attention feature map Fgl before being spa-\ntially pooled into a global image descriptor.\n3.1. Local attention\nWe extract an 1D channel and a 2D spatial attention map\nto weigh the feature map in the corresponding dimensions.\nLocal channel attention\nFollowing ECA-Net [51], this\nattention captures local channel information. As shown in\nFigure 2, we are given a c×h×w feature tensor F from our\nbackbone. We ﬁrst reduce it to a c × 1 × 1 tensor by global\naverage pooling (GAP). Channel attention is then captured\nby a 1D convolution of kernel size k along the channel di-\nmension, where k controls the extent of cross-channel inter-\naction. This is followed by a sigmoid function, resulting in\nthe c × 1 × 1 local channel attention map Al\nc.\nLocal spatial attention\nInspired by the inception mod-\nule [43] and similar to [25], this attention map captures local\nspatial information at different scales. As shown in Figure 3,\n3\nfeature map\nGAP\nconv1d(k)\nconv1d(k)\nsigmoid\nsigmoid\n×\n×\nsoftmax\nattention feature map\n1 × c\n1 × c\n1 × c\nQc\nc × c\nhw × c\nVc\nAg\nc\nc × h × w\n1 × c\n1 × c\nKc\nF\nGc\nFigure 4: Global channel attention.\ngiven the same c × h × w feature tensor F from our back-\nbone, we obtain a new tensor F′ with channels reduced to\nc′, using a 1 × 1 convolution. We then extract local spatial\ncontextual in\n\n---\n\nIndex: 3\nTitle: RITA: Group Attention is All You Need for Timeseries Analytics\nPublished: 2023-06-02\nAuthors: Jiaming Liang, Lei Cao, Samuel Madden, Zachary Ives, Guoliang Li\nSource: Arxiv research paper\nContent: RITA: Group Attention is All You Need for Timeseries Analytics\nJiaming Liang\nUniversity of Pennsylvania\nPhiladelphia, PA, USA\nliangjm@seas.upenn.edu\nLei Cao∗\nMassachusetts Institute of Technology\nCambridge, MA, USA\nlcao@csail.mit.edu\nSamuel Madden\nMassachusetts Institute of Technology\nCambridge, MA, USA\nmadden@csail.mit.edu\nZachary Ives\nUniversity of Pennsylvania\nPhiladelphia, PA, USA\nzives@cis.upenn.edu\nGuoliang Li\nTsinghua University\nBeijing, China\nliguoliang@tsinghua.edu.cn\nABSTRACT\nTimeseries analytics is of great importance in many real-world\napplications. Recently, the Transformer model, popular in natu-\nral language processing, has been leveraged to learn high quality\nfeature embeddings from timeseries, core to the performance of\nvarious timeseries analytics tasks. However, the quadratic time and\nspace complexities limit Transformers’ scalability, especially for\nlong timeseries. To address these issues, we develop a timeseries an-\nalytics tool, RITA, which uses a novel attention mechanism, named\ngroup attention, to address this scalability issue. Group attention dy-\nnamically clusters the objects based on their similarity into a small\nnumber of groups and approximately computes the attention at\nthe coarse group granularity. It thus significantly reduces the time\nand space complexity, yet provides a theoretical guarantee on the\nquality of the computed attention. The dynamic scheduler of RITA\ncontinuously adapts the number of groups and the batch size in the\ntraining process, ensuring group attention always uses the fewest\ngroups needed to meet the approximation quality requirement. Ex-\ntensive experiments on various timeseries datasets and analytics\ntasks demonstrate that RITA outperforms the state-of-the-art in\naccuracy and is significantly faster — with speedups of up to 63X.\n1\nINTRODUCTION\nMotivation. Many data driven applications involve processing\nmassive timeseries data, including IoT [11], medical AI [14], stock\nmarket [27], and so on. As such, there is a great need for timeseries\nanalytics, such as forecasting [8], classification [20], clustering [31],\nsimilarity search [39], and anomaly detection [50], with applications\nranging from automatically diagnosing diseases [5], recognizing\nhuman activities [29], to stopping financial fraud [59].\nEffective feature extraction [40] lies at the core of almost all\nthese timeseries analytics tasks. Recently researchers [61] have\nstarted leveraging the self-supervised pre-training methodology of\nTransformers [4, 16, 52], which have proven remarkably successful\nin natural language processing (NLP), to automatically learn high\nquality feature embeddings from timeseries. In NLP, self-supervised\npre-training exploits the sequential patterns (correlations) among\nthe words in sentences to produce contextualized feature embed-\ndings. Timeseries bear similarity to natural language, because in\ntimeseries data the sequential order among the values (stock price,\nvolume, etc.) over time matters. That is, each value is highly cor-\nrelated with other values observed before or after it. Therefore,\n∗Corresponding Author\npre-training a Transformer model which takes the correlations\namong different observations into account is a natural idea to learn\nfeature embeddings from timeseries. Indeed, the experiments in [61]\nconfirm that Transformer-based methods outperform traditional\ntimeseries analytics techniques.\nHowever, existing work [61] that directly applies Transformers\nto learn features from timeseries data have been shown not to be\nscalable to long timeseries [30]. The idea of self-attention [52] is\ncentral to pre-training methods in NLP: It computes pairwise cor-\nrelations among different semantic units in a sequence (in NLP, a\nsentence); as such, it has quadratic time and space complexity in\nthe length of the input sequence. Such an approach places limits on\nthe model’s scalability, especially when handling large sequences,\nwhich are common in real-world timeseries applications such as\nIoT, medical AI, and finance [6, 34, 62]. Predictions about timeseries\nmay need to look at months or years of historical data to make ac-\ncurate predictions, spanning hundreds of thousands of samples. As\nan example, in collaboration with a research hospital we have been\ndeveloping a seizure classifier that automatically detects seizures\nbased on EEG signals (timeseries) collected during the clinical ob-\nservation of patients. As seizures last only a few seconds, we chunk\nlong EEG data into many 2 second segments and detect seizures at\na segment level. However, the classification of a particular segment\ndepends on up to 12 hours of prior signal to determine if one 2\nsecond segment indicates seizure or not, because seizure diagnosis\nneeds to consider long-term trends in the EEG data [6]. The number\nof segments in 12 hours is more than 21k. This is far larger than\nthe number of semantic units the typical NLP tasks expect. For\nexample, BERT [16] limits the number of units to 512 and even\nmassive models like GPT-3 [4] limit the number of units to 2048.\nAlthough in NLP some lower-complexity methods have been\nproposed to approximately compute self-attention [10, 26, 54], their\nperformance degrades dramatically when used on timeseries, due\nto the gap between natural language and timeseries, as we will\nshow in our experiments.\nProposed Approach. To tackle the aforementioned problem, we\ndevelop RITA, a Transformer-based timeseries analytics tool, which\nuses a novel attention mechanism, called group attention, to scale\nto long timeseries.\nLeveraging the periodicity of timeseries, RITA chunks the input\ntimeseries into segments and dynamically clusters the segments\ninto a small number (denoted as 𝑁) of groups. Segments in the\nsame group possess similar feature embeddings during the current\ntraining iteration, thus enabling them to approximately share the\n1\narXiv:2306.01926v1  [cs.LG]  2 Jun 2023\ncomputation of attention. As the timeseries increases in length,\nmore sharing opportunities become available. RITA then computes\nthe self-attention at a group level and produces a compressed group\nattention matrix. In this way, group attention eliminates both com-\nputation and memory bottlenecks in Transformer-style models and\nthus more scalable to long timeseries.\nHowever, making this idea effective and efficient in Transformer\narchitectures is challenging for several reasons:\n• Efficiently Producing High Quality Feature Embeddings.\nAlthough RITA computes the attention matrix at a group level, to\npreserve the quality of the feature embeddings, it still has to pro-\nduce different embeddings for different segments. This is because\neven if some segments share the attention score temporally, it does\nnot mean they should have the same feature embedding. However,\nusing the group attention matrix, the existing self-attention mech-\nanism will only produce a single feature vector for each group. A\nnaive solution would be to restore the original attention matrix\nfrom the group attention matrix. However, in this case we again\nget an attention matrix with quadratic space complexity. Because\nGPUs have limited memory, GPU memory will remain a bottleneck\nin group attention.\n• The Number of Groups N. In RITA, the number of groups\n𝑁is a crucial factor that balances the speed up and the quality of\nattention approximation. A small 𝑁will lead to a large speedup,\nbut the approximation errors can also be significant. On the other\nhand, although a large 𝑁tends to produce high-quality approxima-\ntions, it inevitably slows down the training process. Therefore, an\nappropriate 𝑁is essential to the performance of group attention.\nHowever, 𝑁depends on the distributional properties of the dataset.\nFurthermore, like the classical transformer models, RITA stacks\nmultiple attention layers to produce better embeddings. Ideally,\ndifferent layers should also use different values of 𝑁. In addition,\nduring the model training phrase, group attention should use dif-\nferent values of 𝑁at different iterations to adapt to the varying\nfeature embeddings. This makes manually setting appropriate 𝑁\nalmost impossible.\n• Batch Size. Moreover, as we want to dynamically adjust 𝑁\nduring training, a fixed batch size is sub-optimal: as 𝑁decreases,\nthe memory usage of a single sample decreases. This allows a larger\nbatch size which is beneficial, because: (1) it makes full use of GPU\nmemory; (2) high-parallelism across the samples in a big batch\nbrings better performance. Our experimental study shows that\ndoubling the batch size reduces the training time by 30%, while still\npreserving the quality of the model. Thus, RITA should dynamically\nadjust batch size as 𝑁changes.\nTo address the above problems, we first propose an embedding\naggregation strategy and a customized group softmax function to\nreplace the classical softmax function [52]. Together they ensure\nRITA is able to directly use the compressed attention matrix to\nproduce different feature embeddings for different segments. We\ntheoretically show the embeddings RITA produces in this way are\nidentical to those produced by first re-storing the original large\nattention matrix. Thus RITA is able to produce high quality embed-\ndings without introducing extra overhead. Further, we design a GPU\nfriendly algorithm to group the segments in parallel, effectively\nminimizing the grouping cost.\nP0\nPosition\nEmbedding\nW1\n+\n+\n+\nWindow \nEmbedding\n+\nE0\nRaw\nTimeseries\nTime-aware \nConvolution\nW[CLS]\nW2\n⊗\n.....\nWn\nP1\nP2\n.....\nPn\n.....\nE1\nE2\nEn\n.....\nO0\nO1\nO2\nOn\n.....\nRITA Encoder\nScale & Input\nFigure 1: RITA Architecture\nSecond, we design an adaptive scheduler which dynamically de-\ncides an appropriate 𝑁for each group attention layer during the\ntraining process. It starts with a large 𝑁and iteratively merges\ngroups that are similar to each other. Guided by an error bound on\nthe approximated self-attention that users can tolerate, it automati-\ncally determines if two groups are mergeable, performing merging\nefficiently in a GPU-friendly way.\nMoreover, we propose a learning-based method to model the\ncorrelation between the number of groups 𝑁and the batch size 𝐵.\nThis model is used to predict 𝐵for a given 𝑁when training RITA.\nSpecifically, we first sample some 𝑁values in a reasonable range.\nFor each sampled 𝑁, we find a batch size that consumes up to a\ncertain percentage of GPU memory in a cost-efficient way. Using a\nsmall set of mathematical functions as a prior, RITA learns a model\nwith only a few <N, B> pairs as ground truth labels.\nOur experiments on public timeseries benchmarks and the MGH\nEEG data [6] confirm that RITA outperforms state-of-the-art meth-\nods in accuracy on various timeseries analytics tasks, while our\ngroup attention mechanism achieves a 63X speedup with much\nless memory required, compared to existing self-attention mecha-\nnisms [10, 52, 54].\nContributions. The key contributions of this work include:\n• Our group attention mechanism leverages the periodicity of\ntimeseries, reducing the time and space complexity of the self-\nattention mechanism with accuracy guarantees, allowing RITA to\nscale to long timeseries data.\n• Guided by an approximation error bound, our adaptive sched-\nuler dynamically adapts the number of groups and the batch size\nto the distribution properties of the evolving feature embeddings,\nmaking group attention efficient and easily tunable.\n• We conduct experiments on various datasets and different ana-\nlytics tasks, demonstrating that RITA is 4 to 63 times faster than\nthe state-of-the-art while achieving better accuracy when handling\nlong timeseries (length ≥2000).\n2\n2\nBACKGROUND\nWe provide some background on the canonical self-attention mod-\nule in the Transformer[52]. A self-attention module takes 𝑛hidden\nembedding vectors 𝐻∈R𝑛∗𝑑ℎas input, then projects them to\nqueries (𝑄), keys (𝐾) and values (𝑉) and performs Scaled-dot Prod-\nuct Attention, which given input hidden state 𝐻, is computed by:\n𝑄= 𝐻𝑊𝑄, 𝐾= 𝐻𝑊𝐾,𝑉= 𝐻𝑊𝑉\n𝑂= 𝐴𝑉= 𝑆𝑜𝑓𝑡𝑀𝑎𝑥( 𝑄𝐾𝑇\n√︁\n𝑑𝑘\n)𝑉\n(1)\nWhere 𝑊\n\n---\n",
  "news": [
    {
      "title": "Paper Walkthrough: Attention Is All You Need - Towards Data Science",
      "description": "Paper Walkthrough: Attention Is All You Need  Towards Data Science",
      "published date": "Sun, 03 Nov 2024 07:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMikgFBVV95cUxOZ3dyR2dBc2YyRjRkVTVVVng2Y1NfSTAzbHNwdjJxWXRNUXBfSmRJSVdRbjNnMy1LS3MxeE9MalRaZ0hRREVIbmY0ZUlQaEtRRlRBRTNtUzFHSkJHSVBsV2JmT3NBaVBCeTBhOEd4TWh1WkFfRzYtOFBTeHU3UVIzMUZFY3J6UkV5Q2hEcWpORW1tdw?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://towardsdatascience.com",
        "title": "Towards Data Science"
      }
    },
    {
      "title": "It turns out that attention is not all you need in LLMs - Substack",
      "description": "It turns out that attention is not all you need in LLMs  Substack",
      "published date": "Tue, 10 Dec 2024 08:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMieEFVX3lxTE1zdThpdnlxaHk5TjR1LW9majVJWVRKYWxpMDhUN1ZsUk44dVNVQS05Q0s4dUE4aGtSNF8tb19FRGxhVUxTWFZGbC1CQ1lXWl9NOGVJOFFuMElTZnRIRGUyN0NzQW4ydDNjdU5jdXZjY2JmNkVjX0Qwcw?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://bdtechtalks.substack.com",
        "title": "Substack"
      }
    },
    {
      "title": "What is a Transformer Model? - IBM",
      "description": "What is a Transformer Model?  IBM",
      "published date": "Fri, 28 Mar 2025 07:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMiX0FVX3lxTFBQMmxPdmlucTE5clNETkgxOXdVMnZhbnMtQ2VoanlvY0RIV2JnMDFOUExjckJoQWxNNUlTMzZQR3Y0LUN3dzJXZmVMVGxWc2pMYnNRSXZ2X1dlN3lvd3U4?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://www.ibm.com",
        "title": "IBM"
      }
    },
    {
      "title": "Attention is all you need — until you need people - Medium",
      "description": "Attention is all you need — until you need people  Medium",
      "published date": "Thu, 29 May 2025 07:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMilwFBVV95cUxQZnZyVm1KNVgyT2J5MExGTVA1em1sSE1GYjh2NnRZMlNjV3g2YkdRQzdNLVJHU3VaakttcHYydDFEYlA3c0F2Y2ljaGxXUmkyaHBDZ29yWkpuQWttVlozVVV3ano4aVY1TURsVmY2TFAtSnR2NE1OOGVpYVVQN3lOQW9hdEpKY2pkazJKOWR0SmJiT1ZZSzJn?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://medium.com",
        "title": "Medium"
      }
    },
    {
      "title": "What Researchers Behind OpenAI’s Foundational Framework Are Doing Today - CCN.com",
      "description": "What Researchers Behind OpenAI’s Foundational Framework Are Doing Today  CCN.com",
      "published date": "Sun, 13 Oct 2024 07:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMiggFBVV95cUxQbTZTSVNTcVZ5SFJ6WkdnVTJzUTBDRXBnbUQxNzFRdVg3OGl2LVZQd0N6cDRiU2o3Y3ByandnYWtVdVdKZU85Z0J2T1VSZkswNWlNTGs4X3dsSDR3eG13dFBxZGpjZHRIOVl1SE5lbmxFbGRESmJYb1BoZkFZeUxMMXd3?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://www.ccn.com",
        "title": "CCN.com"
      }
    },
    {
      "title": "ChatGPT’s success could have come sooner, says former Google AI researcher - Ars Technica",
      "description": "ChatGPT’s success could have come sooner, says former Google AI researcher  Ars Technica",
      "published date": "Thu, 14 Nov 2024 08:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMirAFBVV95cUxQVVhiMmttLWI4Q0ZiTnJWcTBxNGZscnQzdnR2MEVmazRpVG9LbGhZLU90VnZyS0FRTlNhUnFVeUQxNmJSeWR2djBtSWkwcmRHT1dNZ2ZhSEwwdnBTYU9ONlZjbzNvejUyeTV0SUFrZDJEcXJ4ekR3VVZjN19PUC1kcWNxVHJKZlRQcTBGb243QVNhcjV4djBxc3lwcC1uQTM1UzVuZUpjSG9qZ0hB?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://arstechnica.com",
        "title": "Ars Technica"
      }
    },
    {
      "title": "AI Explained: How Attention Mechanisms Took Over AI - PYMNTS.com",
      "description": "AI Explained: How Attention Mechanisms Took Over AI  PYMNTS.com",
      "published date": "Mon, 09 Dec 2024 08:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMimwFBVV95cUxNVllPSXREUkF0RXNNamxuckdDcVluZU91TjBOQURnYl84bFhPMnZ4U1UteW4xMVEwWUJ2NEVRZ1VVTnlrUFZpWHJnU3ZNSlRzX2pYbHNFREdHeDNLWjBwcUNfUHRaNzlGclVHOW9jWFJwUkRjNThKdEFIZnFaZDVmb1Z6NlhTSE1rNm80ZWJoXzVuTWphZmNPa21TSQ?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://www.pymnts.com",
        "title": "PYMNTS.com"
      }
    },
    {
      "title": "A look under the hood of transfomers, the engine driving AI model evolution - VentureBeat",
      "description": "A look under the hood of transfomers, the engine driving AI model evolution  VentureBeat",
      "published date": "Sat, 15 Feb 2025 08:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMiowFBVV95cUxPTWdoWGlUUGZKZGdPdWtfWEdUQkctcU1saUhZajJ0U2dCVGJPT3k0Q2tyTEhLYVRlczA4d1B5TFktSHR5UV9oeU1uMUttOGZIcURnSHdEWXZ5NTMzbjFqQU1NLW11UElWdUNSTEVkWk9Mczg4N3V0Q2xMUk1ZT2l6RWc4OG9JREU5WVUxSWVWeENxVDdYb3hiUzF3R3B0UG5WQlFv?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://venturebeat.com",
        "title": "VentureBeat"
      }
    },
    {
      "title": "What If We’re Doing AI All Wrong? - Bloomberg",
      "description": "What If We’re Doing AI All Wrong?  Bloomberg",
      "published date": "Wed, 03 Sep 2025 09:00:21 GMT",
      "url": "https://news.google.com/rss/articles/CBMiugFBVV95cUxOOTZIRXRISENYeTlodkZjT2U0SGZCT2R6Tk80OTFMTzN1RzZyOVVmbUVJalZhTkNfREgwQmVvd3RRTEV5M3ZCSkNHMGx4cmw5QXU1NFhHLXJGeVRFQlppVVlGUGROMHd2VV9fZGNyZzRPQld4Z2Qzc21PTmZlSk5pVzZncENPdE1tN3ItMU1oNTJKSE1LUUpqU3JmQ3V4d0t2UDBJU2dzQzI2dXdsc3J5Nm1RYWcyX3NIREE?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://www.bloomberg.com",
        "title": "Bloomberg"
      }
    },
    {
      "title": "‘Attention is All You Need’ Author Suggests LLMs ‘Reflect’ in Pre-Training - Analytics India Magazine",
      "description": "‘Attention is All You Need’ Author Suggests LLMs ‘Reflect’ in Pre-Training  Analytics India Magazine",
      "published date": "Mon, 14 Apr 2025 07:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMiuAFBVV95cUxNU0J2TFNQcmROQkJ0NEhld2R2aVFDQS1Pa0UtY1dNYUdJRHFENkNYcl9TcUs2cmVZWF8xT1dITTh0Q2dyNGdXaTdLNkJHWDdQNGZOWUw2QS1pcjVUTkF0Q2xfR2NVZ1JDaTJQcl9DQm9FRzh0akEtRDN6T1dvQUF2allPOVQ0dmxVWVRFS2lmbzFCOHV0cDIyc0RkWHlMc25RaEhIbmtCcEs2THZtVDhFZGtEem9SOFE1?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://analyticsindiamag.com",
        "title": "Analytics India Magazine"
      }
    },
    {
      "title": "You’re Being Alienated From Your Own Attention - The Atlantic",
      "description": "You’re Being Alienated From Your Own Attention  The Atlantic",
      "published date": "Wed, 22 Jan 2025 08:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMijgFBVV95cUxPWnFMblFMaDI5WVNlVHZpU0VBdVF5Q01QVm1MdF9yNzA0VVVrTy1qWnZSNHBqRUh5WjFENWdJSVNIUjlCTmNncEM2TEtweU5md2gyRF94bUpRaEZTXzBmdUtsenNtYVlBWEc5eHJ2eFV6OVptNTBnZjBKQkdaZkNoR2J0andFNVMycmhhdFBB?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://www.theatlantic.com",
        "title": "The Atlantic"
      }
    },
    {
      "title": "What is an attention mechanism? - IBM",
      "description": "What is an attention mechanism?  IBM",
      "published date": "Wed, 11 Dec 2024 16:27:47 GMT",
      "url": "https://news.google.com/rss/articles/CBMiYkFVX3lxTE5TUkJiVWp4UHlmUndLNVdhWXhDVXhTY0xMY1hDdzdzQk45RjNyZ19YZzhZNHdwb1NFaG9nbGZxLTMtNlJhc1owdUFzZllXSWFnM0NfNkxQWm5nY2VzLWplY093?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://www.ibm.com",
        "title": "IBM"
      }
    },
    {
      "title": "Tracing the Transformer in Diagrams - Towards Data Science",
      "description": "Tracing the Transformer in Diagrams  Towards Data Science",
      "published date": "Thu, 07 Nov 2024 08:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMihwFBVV95cUxQWDQtUEM2RWNySjZUellMQ3VFT2o0alBXazZ1R1BBeDB5cFloaUQtcDY1eDIzOG1KTXdodXo0enZET2RpMnRxRWNKRXA3aTNjZFJCbG1QeTc0dEtyQUpGUG5hMlAyVUtrUW9VTFU4NTF2UkpxcWRsWWxRMFZpR3hra0pSSDJpanM?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://towardsdatascience.com",
        "title": "Towards Data Science"
      }
    },
    {
      "title": "What is self-attention? - IBM",
      "description": "What is self-attention?  IBM",
      "published date": "Tue, 18 Feb 2025 08:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMiW0FVX3lxTE9BZW5RbWZPSkdqQXp6a0o2ZzN6RnNBZDNzZkRSa2VpdnZ3ZEVZWDQ2MkpSam1wWDVleGF4YlZhSkRiVldkSnlnSDZtRkVrZGJQcThSMzJtLWdUZ1U?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://www.ibm.com",
        "title": "IBM"
      }
    },
    {
      "title": "Hands-On Attention Mechanism for Time Series Classification, with Python - Towards Data Science",
      "description": "Hands-On Attention Mechanism for Time Series Classification, with Python  Towards Data Science",
      "published date": "Fri, 30 May 2025 07:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMipgFBVV95cUxOeEo3cmRWQ1Uwb2xUSzdGeGphU0ZjSi1FQy1oOEhJRUJhNHBRbEo5eG0tQ2Z1OXlldnNiRjRmVU1IelFhN1dzN3ZydEJ0d0JPRUdjNVd0UkFVUzVNcFM3WFRubTVibUxfR1Z2U2FLSUJHT2o3Y3BubUJSdEpjdmNoLVZ4SFVOMkJQR21Ec0FWR2RiLUY1V1Z3dS1tcU13cW9GQk9zODh3?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://towardsdatascience.com",
        "title": "Towards Data Science"
      }
    },
    {
      "title": "What is grouped query attention (GQA)? - IBM",
      "description": "What is grouped query attention (GQA)?  IBM",
      "published date": "Fri, 06 Dec 2024 08:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMiZ0FVX3lxTE0xS3hBT253RFU0bjljQlZuU3BHVXMzVGhkaTNuTlBoVlJRek1iZElHUXAtdTB1M1NoY2c5WTFFeU5uekhBSGFTVnZoUGtLd0RqM3lUd0l1T2gzSkx5cVVLcjVOMS0zSzA?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://www.ibm.com",
        "title": "IBM"
      }
    },
    {
      "title": "Increasing Transformer Model Efficiency Through Attention Layer Optimization - Towards Data Science",
      "description": "Increasing Transformer Model Efficiency Through Attention Layer Optimization  Towards Data Science",
      "published date": "Mon, 18 Nov 2024 08:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMivgFBVV95cUxNVTE1SzhPaEJlQTg0dURGbnZwWDdUc1U3UU9yaFVfYThMd2oxMVU0ek5ETGRvQk9VMUQ4UE95eFJXN3FoZURVWDAtRzdWaGVzVGxZMnJIbDhEUm1nUWQ1ZUd2c2R1d0lacktfblNnRlROaDRDSTMxQXllOXptY1NCV05lTVYtT0xOaGdMc0hWQ05mNXhaXzJEWExVTEcyX2p4S3BRZGdQTXVtSjNTMkR6b2lUNElRSmtJVE1tWjNn?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://towardsdatascience.com",
        "title": "Towards Data Science"
      }
    },
    {
      "title": "DeepSeek-V3 Explained 1: Multi-head Latent Attention - Towards Data Science",
      "description": "DeepSeek-V3 Explained 1: Multi-head Latent Attention  Towards Data Science",
      "published date": "Fri, 31 Jan 2025 08:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMinAFBVV95cUxOUWc2U2dDN04yc3dqZmx1ZkFmVFRUM2xvT3A0aGFrVGYzcnJkSURlTFVQWVFlclFHUUVhX2FGcmlRX0FNOGFvbnRUOW5hZ3pCbHptS0Z2RlJiN3g4VzgtYkw5MVBaOHRrTlhLMVVIUEZFMVNWQ0pKVkNGQXZTZklqSVpxd3l4QXVWTl9YRkw0V0FWT3pSZUJGTlNFMXE?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://towardsdatascience.com",
        "title": "Towards Data Science"
      }
    },
    {
      "title": "How I Studied LLMs in Two Weeks: A Comprehensive Roadmap - Towards Data Science",
      "description": "How I Studied LLMs in Two Weeks: A Comprehensive Roadmap  Towards Data Science",
      "published date": "Fri, 18 Oct 2024 07:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMiogFBVV95cUxQRG9iUXhrUk1BWWNndFFTcThHc29nMlNVRThmeUZMMGJSY3ZlTVNLVUFYNlEzMnVaVTJQbEZnVERMS29PSUoyVkplMGEzeUtCUDZCS0cwTWQ0MW51ZHRqcVVtNjZwQjNfNG1FMHdkYmhvMWpjOG5Sc1liYUVfSUNyZXRpdktzR1hoQWZtSWJYMi1jYzd3Uy1aTEl5V3B4UG5RRlE?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://towardsdatascience.com",
        "title": "Towards Data Science"
      }
    },
    {
      "title": "“Attention is all you need” - The Ken",
      "description": "“Attention is all you need”  The Ken",
      "published date": "Sun, 24 Aug 2025 07:00:00 GMT",
      "url": "https://news.google.com/rss/articles/CBMif0FVX3lxTE9UdzJObEJHQTRTa3JVZ3Iya2xkUGR5c0JRQ2xiU3Y4QXhVUk9vbWhFNjlqRGk3UWlYdThONGhLaVc3em9XYWpmZkNxcS1jZzlkZWd5OFpHQkN6azIyWDVFYjJsdDN5bUVZaXZocmZaV0RRZlFDYW1HR1AyM1hacjg?oc=5&hl=en-IN&gl=IN&ceid=IN:en",
      "publisher": {
        "href": "https://the-ken.com",
        "title": "The Ken"
      }
    }
  ],
  "knowledge": {
    "topic": "Attention Is All You Need",
    "sources": [
      {
        "id": 1,
        "title": "Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models",
        "authors": [
          "Georgy Tyukin",
          "Gbetondji J-S Dovonon",
          "Jean Kaddour",
          "Pasquale Minervini"
        ],
        "source": "arXiv",
        "url": "https://arxiv.org/abs/2407.15516"
      },
      {
        "id": 2,
        "title": "All the attention you need: Global-local, spatial-channel attention for image retrieval",
        "authors": [
          "Chull Hwan Song",
          "Hye Joo Han",
          "Yannis Avrithis"
        ],
        "source": "arXiv",
        "url": "https://arxiv.org/abs/2107.08000"
      },
      {
        "id": 3,
        "title": "RITA: Group Attention is All You Need for Timeseries Analytics",
        "authors": [
          "Jiaming Liang",
          "Lei Cao",
          "Samuel Madden",
          "Zachary Ives",
          "Guoliang Li"
        ],
        "source": "arXiv",
        "url": "https://arxiv.org/abs/2306.01926"
      }
    ],
    "topics": [
      {
        "id": "t1",
        "title": "Inference-time layer skipping and sublayer removal in Transformer LLMs (Llama-v2 study)",
        "summary_points": [
          "Motivation: inference cost of Transformer-based LLMs scales poorly due to quadratic self-attention cost and deep architectures; layer skipping at inference is explored to reduce compute and latency.",
          "Model variants: construct three skip strategies for a model M with L layers (each layer = Attention_i + MLP_i): (a) Mskip_MLP — remove MLP sublayers in the last k layers, (b) Mskip_Attention — remove attention sublayers in the last k layers, (c) Mskip_Block — remove entire final k Transformer blocks.",
          "Empirical observation on representation redundancy: cosine similarity between consecutive Llama-v2 layers increases with depth (features become more similar), except the final layer which diverges most from the previous one; this motivates dropping deeper components.",
          "Key quantitative result: for Llama-v2 13B, removing ~33% of attention sublayers yields only a ~1.8% drop in average OpenLLM benchmark performance (ARC, HellaSwag, MMLU, TruthfulQA), showing attention sublayers in deeper layers are relatively redundant.",
          "Ablations: dropping MLP (ffwd) sublayers causes larger performance degradation than dropping attention sublayers, indicating deeper MLPs have a larger contribution to final performance than attention in later layers.",
          "Last-layer importance: preserving the last layer when skipping earlier layers affects results—keeping the final block sometimes mitigates degradation, but skipping everything except the last layer can amplify errors because the final feedforward must process perturbed upstream representations.",
          "Compute vs. accuracy trade-off: full-block removal gives the largest runtime improvement, followed by attention sublayer skipping; feedforward sublayer skipping yields smaller runtime gains. Measured token-prediction latency (averaged over 1000 sequences) shows substantial wall-clock improvements for 25–33% skipping.",
          "Practical implications: targeted skipping of deeper attention sublayers is a low-cost inference optimization for LLMs that preserves quality; MLP removal is riskier. Small additional fine-tuning after skipping (continued training) is proposed as a potential remedy for degraded robustness."
        ],
        "subtopics": [
          {
            "id": "t1.1",
            "title": "Layer skipping methodology",
            "summary_points": [
              "Formalization: define skipping variants by replacing the relevant sublayer(s) with ∅ in the last k layers; evaluate performance vs. baseline on downstream benchmarks.",
              "Variants: Mskip_MLP (remove MLP in last k), Mskip_Attention (remove Attention in last k), Mskip_Block (remove whole blocks), with experiments across keeping 66%, 75%, 90%, 100% of original network.",
              "Measurement protocols: report benchmark metrics (ARC, HellaSwag, TruthfulQA, MMLU) and measure per-token inference latency averaged over 1000 sequences for sequence lengths 50 and 100 to quantify time improvements."
            ],
            "references": [
              1
            ]
          },
          {
            "id": "t1.2",
            "title": "Empirical results and benchmarks",
            "summary_points": [
              "Benchmarks used: OpenLLM compilation including ARC, HellaSwag, MMLU, TruthfulQA to measure multi-task LLM performance.",
              "Performance trends: skipping attention sublayers preserves much of accuracy (average falls modestly), while skipping MLP sublayers leads to larger drops; examples: 13B-66% (keeping 66% of network) shows only ~1.8% average drop when removing attention vs larger drops when removing MLP.",
              "Timing results: for Llama-v2-7B, skipping full layers produced the most significant time savings, attention sublayer skipping gave intermediate speedups, feedforward skipping gave the least; tables show time(s) × 10^2 per token and percentage improvements for 50/100-length inputs.",
              "Last-layer inclusion experiments: keeping the final block while skipping earlier components often improves robustness of reduced models, but sometimes extremely reduced configurations still perform poorly (especially for 13B when heavy skipping but last-layer preserved)."
            ],
            "references": [
              1
            ]
          },
          {
            "id": "t1.3",
            "title": "Practical implications, limitations, and future directions",
            "summary_points": [
              "Implication: inference-time sublayer removal (especially attention sublayers in deeper layers) is a cost-effective optimization for serving LLMs with lower latency and modest accuracy loss.",
              "Caveats: MLP components are more sensitive in deeper layers and removing them degrades behaviour more than attention removal; last-layer processing can amplify upstream perturbations making some skip patterns brittle.",
              "Suggested mitigations: small continued training or lightweight adaptation after skipping could recover robustness; dynamic or adaptive skipping (early-exit style) remains an open avenue tied to robustness and calibration studies.",
              "Limitations: experiments focused on Llama-v2 7B/13B and the OpenLLM benchmark; behaviour on other architectures, modalities, or very long-context regimes remains to be validated."
            ],
            "references": [
              1
            ]
          }
        ],
        "references": [
          1
        ]
      },
      {
        "id": "t2",
        "title": "Global-Local Attention Module (GLAM) for image retrieval: spatial × channel × local × global",
        "summary_points": [
          "Core idea: GLAM combines four attention types—local-channel, local-spatial, global-channel, global-spatial—applied jointly to a CNN feature tensor to produce a fused feature tensor used for global pooling and image retrieval.",
          "Architecture overview: input feature tensor F (c × h × w) is processed by two parallel streams: local attention (extracts Al_c: c×1×1, and Al_s: 1×h×w) and global attention (extracts Ag_c: c×c and Ag_s: hw×hw), their outputs are fused with F to produce Fgl before pooling.",
          "Local attention design: local channel attention uses GAP + 1D conv (kernel k controls cross-channel interaction) followed by sigmoid (inspired by ECA-Net); local spatial attention uses multi-scale/dilated convolutions to capture contextual spatial cues.",
          "Global attention design: global channel attention models interactions between channels (produces c×c map) using pairwise computations and a softmax-like normalization; global spatial attention models interactions across spatial positions producing hw×hw maps (non-local style).",
          "Empirical contribution: GLAM is trained and evaluated for instance-level image retrieval, yielding improved state-of-the-art global descriptors on standard benchmarks and providing experimental evidence that combining all four attention forms is beneficial.",
          "Positioning vs prior work: prior methods typically address only one or two attention forms (e.g., channel-only, spatial-only, local or global); GLAM is a systematic, learned, end-to-end-attached module for global descriptor learning."
        ],
        "subtopics": [
          {
            "id": "t2.1",
            "title": "Local attention components and implementation",
            "summary_points": [
              "Local channel attention: obtain c×1×1 via global average pooling then 1D conv (kernel size k) → sigmoid to produce per-channel weights; captures local cross-channel context.",
              "Local spatial attention: reduce channels to c' via 1×1 conv, then apply multi-scale (dilated) convolutions (kernels 3×3 with dilations 1,3,5) and combine via 1×1 conv + sigmoid to obtain 1×h×w spatial map; captures multi-scale local spatial context.",
              "Design role: local modules weigh elements independently based on pooled/contextual cues; they are learned and operate as efficient reweighting mechanisms suited for retrieval embeddings."
            ],
            "references": [
              2
            ]
          },
          {
            "id": "t2.2",
            "title": "Global attention components and fusion",
            "summary_points": [
              "Global channel attention: compute interactions between channels leading to a c×c attention map (pairwise channel interactions) and apply normalization (softmax-like) and gating to reweight channel activations.",
              "Global spatial attention: compute hw×hw affinity to allow every spatial location to attend to every other (non-local mechanism) producing spatial reweighting that captures long-range context beneficial for discriminative global descriptors.",
              "Fusion: outputs of local and global streams (Fl and Fg) are combined with original features F through a learned fusion mechanism to produce a final feature tensor Fgl which is pooled (e.g., GeM or similar) into a compact global descriptor for retrieval."
            ],
            "references": [
              2
            ]
          },
          {
            "id": "t2.3",
            "title": "Empirical findings and comparative positioning",
            "summary_points": [
              "GLAM's empirical results show that combining local/global and spatial/channel attentions improves retrieval performance vs. methods that use only a subset; experiments demonstrated state-of-the-art performance on standard retrieval benchmarks at the time of publication.",
              "The paper provides a taxonomy of prior attention methods (local/global × spatial/channel) and situates GLAM as the first to jointly employ all four forms in a learned module applied to global descriptor learning.",
              "Implication: richer attention modelling (capturing both pairwise interactions and locally pooled cues across both spatial and channel dimensions) strengthens compact global descriptors for instance-level retrieval."
            ],
            "references": [
              2
            ]
          }
        ],
        "references": [
          2
        ]
      },
      {
        "id": "t3",
        "title": "Group attention for timeseries analytics (RITA): scalable approximate self-attention",
        "summary_points": [
          "Problem: canonical Transformer self-attention has O(n^2) time and memory in sequence length n, limiting applicability to long timeseries (e.g., tens of thousands of segments).",
          "Key idea: group attention clusters segments (sequence chunks) dynamically into N groups (segments in same group are similar) and computes attention at the group granularity, yielding a compressed group-level attention matrix and drastically reducing compute/memory.",
          "Embedding quality and correctness: introduce an embedding aggregation strategy and a customized group softmax that allow producing distinct embeddings per original segment while operating on the compressed attention matrix; theoretical results show equivalence to restoring the original attention matrix under the scheme (i.e., no additional embedding error beyond provable bounds).",
          "Adaptive scheduler and batching: RITA adapts the number of groups N per layer and iteration via a scheduler guided by a user-specified approximation error bound; it also models and adapts batch size B as a function of N to maximize GPU utilization (learned mapping from sampled <N,B> pairs).",
          "Implementation details: GPU-friendly parallel grouping algorithm, group-level attention computations, and a dynamic strategy to merge groups to meet approximation bounds while minimizing computation.",
          "Empirical results: across public timeseries benchmarks and a real EEG dataset, RITA achieves speedups between 4× and up to 63× over baseline attention mechanisms while matching or improving predictive accuracy on forecasting, classification and anomaly tasks.",
          "Implication: group attention makes Transformer-style models practical for long-timeseries analytics by trading a tunable approximation (controlled error bound) for large efficiency gains; it generalizes the attention mechanism to clustered computations."
        ],
        "subtopics": [
          {
            "id": "t3.1",
            "title": "Group attention algorithmic components",
            "summary_points": [
              "Clustering: chunk input timeseries into segments and cluster segments with similar embeddings into N groups (dynamic per layer/iteration).",
              "Compressed attention: compute Queries/Keys/Values per segment, aggregate per-group, compute group-level attention matrix A_group (N×N) and use an embedding aggregation strategy to reconstruct segment-level outputs without materializing full n×n attention.",
              "Custom group softmax: replace classical softmax with a grouping-aware normalization enabling correct redistribution of attention weights back to segment-level outputs without expanding to the full matrix."
            ],
            "references": [
              3
            ]
          },
          {
            "id": "t3.2",
            "title": "Adaptive scheduler, error bounds, and batching",
            "summary_points": [
              "Adaptive scheduler: starts with a large N and iteratively merges similar groups guided by a provable approximation error bound, deciding when two groups are mergeable to satisfy user tolerance.",
              "Batch size adaptation: learns a simple predictive model mapping N→B (batch size) using a small set of measured <N,B> pairs to scale batch size as memory is freed when N reduces, improving throughput.",
              "Theoretical guarantees: provide error bounds on attention approximation that drive merging decisions so RITA maintains a user-specified approximation quality while reducing computational load."
            ],
            "references": [
              3
            ]
          },
          {
            "id": "t3.3",
            "title": "Empirical scalability and accuracy outcomes",
            "summary_points": [
              "Benchmarks: experiments on multiple public timeseries datasets and a clinical EEG dataset demonstrate that RITA outperforms state-of-the-art timeseries attention approximations in accuracy while substantially lowering runtime and memory.",
              "Scalability: RITA is shown to scale to very long sequences (≥2000) where standard self-attention becomes infeasible; reported speedups reach up to 63× compared to canonical attention baselines in favorable settings.",
              "Practical note: choosing N trades speed vs approximation error; the scheduler automates this trade-off per layer/iteration to meet constraints while leveraging increased parallelism via larger batch sizes when possible."
            ],
            "references": [
              3
            ]
          }
        ],
        "references": [
          3
        ]
      }
    ],
    "abstract": "This knowledge base synthesizes methods and findings from three recent works that extend, adapt, or economize Transformer-style attention across modalities and deployment constraints. The first paper investigates inference-time layer skipping in Llama-v2 models, formalizing removal of attention sublayers, MLP sublayers, or entire Transformer blocks at the tail of deep networks and measuring impacts on accuracy (OpenLLM benchmarks) and latency. It finds deeper attention sublayers are relatively redundant (e.g., removing ~33% of attention layers in 13B Llama-v2 yields only ~1.8% average drop) while MLP removal is more harmful; full-block dropping achieves largest speedups. The second paper introduces GLAM, a global-local attention module for image retrieval that jointly models local/global and spatial/channel attention (four attention maps: local-channel, local-spatial, global-channel, global-spatial), fusing them into a learned feature tensor that improves global descriptors and advances state-of-the-art retrieval. The third paper presents RITA, a scalable Transformer for long timeseries via group attention: dynamically cluster segments into a small number of groups, compute group-level attention with an embedding-aggregation and group-softmax scheme that preserves segment-level outputs, and use an adaptive scheduler (guided by approximation error bounds) plus batch-size modeling for efficient GPU usage; experiments show substantial speedups (up to 63×) with equal or better accuracy on long-timeseries tasks. Collectively, these works show attention can be restructured—via pruning, richer multi-dimensional attention, or coarse-grained grouping—to meet constraints of latency, memory and task demands while retaining performance.",
    "conclusion": "Across language, vision, and timeseries domains, attention-centric modifications offer complementary routes to practical Transformers: (1) inference-time skipping (preferentially of deeper attention sublayers or whole blocks) provides a low-cost latency reduction with modest accuracy loss and is attractive for serving large LLMs, (2) richer attention factorization (GLAM) that jointly models local/global and spatial/channel interactions yields stronger compact descriptors for image retrieval, and (3) coarse-grained group attention (RITA) enables provably approximate, highly efficient self-attention for very long sequences by clustering and adaptive scheduling. Each approach trades approximation or model capacity for compute/memory savings; combining these ideas (e.g., adaptive skipping informed by representation similarity, richer local/global reweighting inside grouped attention) and validating cross-architecture/generalization remain promising directions. Careful attention to last-layer robustness, approximation error bounds, and small adaptation training can help recover or preserve performance when applying these optimizations in production."
  },
  "report_parts": [
    "## Inference-time layer skipping and sublayer removal in Transformer LLMs (Llama-v2 study)\n\nTransformer-based large language models incur high inference cost because self-attention scales quadratically with sequence length and because modern models are deep, motivating the exploration of inference-time structural reductions to reduce compute and latency [1]. The study constructs and evaluates a set of targeted removal strategies that eliminate components at inference — either sublayers (attention or MLP) or entire Transformer blocks — in the deeper part of the network, with the explicit aim of trading off runtime against task performance on a multi-task benchmark suite [1]. Empirical analysis of internal representations shows increasing similarity between consecutive layer outputs with depth, indicating representational redundancy in deeper layers, while the final layer tends to diverge from its predecessor; this pattern motivates selective dropping of deeper components rather than uniform pruning throughout the network [1].  \n\nQuantitatively, the experiments on Llama-v2 indicate that removing a substantial fraction of deeper attention sublayers can yield large runtime benefits with only modest performance degradation: for the 13B model, removing roughly one-third of attention sublayers produced only about a 1.8% drop in average performance across the OpenLLM compilation (ARC, HellaSwag, MMLU, TruthfulQA) [1]. Ablation results further demonstrate that removing feedforward (MLP) sublayers causes larger performance declines than analogous removal of attention sublayers, implying that deeper MLP components contribute more to final task performance than attention in later layers [1]. Regarding runtime, full-block removal yields the largest wall-clock speedups, attention-sub‑layer skipping provides intermediate gains, and MLP removal gives the smallest time improvements; measured token-prediction latency averaged over 1000 sequences confirms substantial improvements for skip rates in the 25–33% range [1].  \n\nThe study also investigates the role of the final layer: preserving the last block while skipping earlier components sometimes mitigates performance loss, but extreme reductions that leave only the final layer can amplify upstream perturbations and produce brittle behaviour, particularly in larger models under heavy skipping [1]. As a practical recommendation, targeted skipping of deeper attention sublayers emerges as a cost‑effective inference optimization that tends to preserve quality, whereas MLP removal is riskier; the authors propose light continued training or lightweight adaptation after skipping as a potential remedy for degraded robustness and identify dynamic/adaptive skipping as an open direction for further research and validation across architectures and contexts [1].\n\n### Layer skipping methodology\n\nLayer skipping is formalized by replacing specified sublayers with the empty operation ∅ in the last k layers of a model M composed of L Transformer layers (each layer consisting of Attention_i and MLP_i), yielding distinct inference-time variants that retain the original trained weights elsewhere [1]. Three concrete variants are defined and evaluated: Mskip_MLP, in which feedforward (MLP) sublayers are removed in the final k layers; Mskip_Attention, where the attention sublayers are removed in the final k layers; and Mskip_Block, which removes entire final k Transformer blocks (both attention and MLP) [1]. Experiments are reported across configurations that correspond to keeping 66%, 75%, 90%, and 100% of the original network, which maps to the fraction of layers or sublayers retained under each skip strategy [1].  \n\nPerformance under these skip strategies is assessed by comparing downstream benchmark metrics to the baseline intact model and by measuring per-token inference latency to quantify time savings. The benchmark suite comprises ARC, HellaSwag, TruthfulQA, and MMLU (the OpenLLM compilation), and latency is measured as per-token prediction time averaged over 1000 sequences for two sequence lengths (50 and 100 tokens) to capture realistic wall-clock behaviour under different input sizes [1]. These measurement protocols enable a direct comparison of accuracy versus compute trade-offs across the different skip variants and retention rates [1].\n\n### Empirical results and benchmarks\n\nAcross the OpenLLM benchmarks, skipping attention sublayers in deeper layers preserves much of the model’s accuracy, with only modest average declines, whereas removing MLP sublayers produces substantially larger performance drops under comparable retention levels [1]. For example, in the Llama-v2-13B experiments, retaining 66% of the network (i.e., removing about one-third of deeper attention sublayers) corresponded to an average performance decrease of only approximately 1.8% when attention sublayers were removed, while analogous MLP removal produced larger degradations [1]. The representation analysis that motivates these interventions shows increasing cosine similarity between consecutive layers with depth, supporting the notion that deeper attention computations are relatively redundant and thus amenable to removal [1].  \n\nTiming experiments indicate that removing entire Transformer blocks yields the largest runtime improvements, followed by attention-sub‑layer skipping, with feedforward-sublayer removal producing the smallest speedups; reported measurements quantify per-token latency (times reported per sequence length) and show notable percentage improvements for 50- and 100-token inputs when skipping 25–33% of components [1]. Experiments that selectively preserve the last block demonstrate that keeping the final layer while skipping earlier components often improves robustness of the reduced models, although some heavily reduced configurations (particularly for the 13B model) can still perform poorly despite last-layer preservation, suggesting limits to how much upstream computation can be removed without inducing brittle behaviour [1].\n\n### Practical implications, limitations, and future directions\n\nThe study indicates that inference-time sublayer removal, and in particular targeted skipping of deeper attention sublayers, offers a low-cost optimization for serving LLMs that can meaningfully reduce latency while incurring only modest accuracy loss, making it a practical option for latency-sensitive deployment scenarios [1]. However, several caveats apply: deeper MLP components are more sensitive than attention sublayers, so removing feedforward modules degrades downstream behaviour more severely, and extreme reductions that rely heavily on the final layer can amplify perturbations from upstream omission, producing brittle outputs [1]. As potential mitigations, the authors propose light continued training or other lightweight adaptation after skipping to recover robustness, and they identify dynamic or adaptive skipping (analogous to early-exit mechanisms) as an open research avenue that requires study of robustness and calibration [1].  \n\nLimitations of the present evaluation include its focus on Llama-v2 7B/13B models and the OpenLLM benchmark suite; the generality of the observed trends to other architectures, modalities, or very long-context regimes remains to be validated in future work [1].",
    "## Global-Local Attention Module (GLAM) for image retrieval: spatial × channel × local × global\n\nThe Global-Local Attention Module (GLAM) is a learned attention module designed to enrich convolutional feature tensors for instance-level image retrieval by jointly applying four complementary attention types: local-channel, local-spatial, global-channel, and global-spatial. GLAM operates on a CNN feature tensor F of shape c × h × w and produces a fused feature tensor that encodes both locally pooled cues and pairwise long-range interactions across channels and spatial positions; this fused tensor is then used for global pooling to produce compact descriptors for retrieval tasks [2]. The central idea is that combining reweighting based on local pooled context with explicit pairwise modelling of interactions yields more discriminative global descriptors than using any single attention form alone [2].\n\nArchitecturally, the module splits processing into two parallel streams. The local-attention stream extracts a per-channel reweighting Al_c of shape c×1×1 and a per-spatial-location map Al_s of shape 1×h×w, both designed to operate as efficient, learned reweighting mechanisms. The global-attention stream computes higher-order pairwise interactions, producing a channel–channel attention map Ag_c of shape c×c and a spatial affinity map Ag_s of shape hw×hw; these capture long-range dependencies among channels and among spatial positions respectively. The outputs of the local and global streams are fused with the original feature tensor F through a learned fusion mechanism to form the final fused tensor Fgl, which is then pooled (for example using a GeM-like pooling) into a compact global descriptor for image retrieval [2].\n\nGLAM is presented as an end-to-end-attached module for global descriptor learning that systematically covers the four quadrants of the local/global × channel/spatial taxonomy. Empirically, the paper reports that training image retrieval models with GLAM produces improved state-of-the-art global descriptors on standard retrieval benchmarks of the time, and provides experimental evidence that jointly modeling all four attention forms yields superior retrieval performance compared to methods that consider only a subset of these attention types [2]. The module is thus positioned as a general and learnable augmentation to CNN backbones for enhanced instance-level retrieval representations [2].\n\n### Local attention components and implementation\n\nThe local attention stream comprises two complementary components. The local channel attention produces a c×1×1 vector by first applying global average pooling across spatial dimensions and then feeding the pooled vector into a one-dimensional convolution with kernel size k; the output is passed through a sigmoid to produce per-channel weights, thereby capturing local cross-channel context with controlled cross-channel interaction determined by k (this design is inspired by ECA-Net-style mechanisms) [2]. This lightweight channel reweighting allows the network to modulate each channel independently based on pooled contextual cues, which is particularly suitable for creating compact, discriminative embeddings for retrieval [2].\n\nLocal spatial attention is implemented by first reducing the channel dimensionality to c' via a 1×1 convolution and then applying a set of multi-scale dilated convolutions (3×3 kernels with dilations 1, 3, and 5) to capture spatial context at multiple receptive-field sizes. The multi-scale responses are combined through another 1×1 convolution and a sigmoid activation to yield a 1×h×w spatial attention map, which reweights spatial locations according to local contextual cues. Together, the local channel and spatial components act as efficient, learned reweighting modules that emphasize informative channels and positions based on locally pooled context, making them well-suited for enhancing retrieval embeddings [2].\n\n### Global attention components and fusion\n\nThe global attention stream explicitly models pairwise interactions at both the channel and spatial levels. Global channel attention computes pairwise interactions between channels to produce a c×c attention map; this map is normalized with a softmax-like mechanism and subsequently used (often with a gating mechanism) to reweight channel activations, thereby capturing long-range dependencies and correlations across feature channels. This pairwise modelling enables the representation to account for relationships between feature detectors that are not captured by local pooling alone [2].\n\nGlobal spatial attention computes an hw×hw affinity matrix that lets every spatial location attend to every other location in a non-local fashion, producing spatial reweighting that captures long-range spatial context and relationships that are critical for discriminative global descriptors. Such non-local interactions complement the local spatial module by introducing global positional dependencies into the pooled representation [2].\n\nThe outputs of the local and global streams (Fl and Fg) are merged with the original feature tensor F through a learned fusion mechanism to form the final fused tensor Fgl; this fusion integrates locally pooled cues and pairwise long-range interactions across both channel and spatial dimensions. The fused tensor Fgl is then aggregated by a global pooling operation (for example, GeM or a similar pooling strategy) into a compact global descriptor that is used for instance-level image retrieval [2].\n\n### Empirical findings and comparative positioning\n\nEmpirical evaluations reported for GLAM indicate that jointly employing local/global and spatial/channel attentions produces improved retrieval performance compared to methods that use only a subset of these attention forms, yielding state-of-the-art global descriptors on standard benchmarks at the time of the study. The experiments demonstrate that the richer attention modelling afforded by GLAM—combining both pairwise interactions and locally pooled cues across channel and spatial dimensions—strengthens compact global descriptors for instance-level retrieval [2].\n\nThe work also offers a taxonomy of prior attention methods along the local/global × spatial/channel axes and situates GLAM as the first learned, end-to-end module to systematically and jointly employ all four attention types for global descriptor learning. This positioning emphasizes GLAM’s contribution as a unifying, systematic approach that extends prior channel-only or spatial-only attention mechanisms by integrating them into a single module tailored for retrieval tasks [2].",
    "## Group attention for timeseries analytics (RITA): scalable approximate self-attention\n\nTransformer self-attention incurs quadratic time and memory cost in sequence length n, which limits its applicability to long timeseries composed of tens of thousands of segments and similar long-range problems [3]. RITA addresses this scalability bottleneck by replacing elementwise attention with a grouped computation: input timeseries are partitioned into segments and similar segments are dynamically clustered into N groups, so that attention is computed at the group granularity rather than across all n segments. This grouping yields a compressed N×N attention matrix and substantially reduces both compute and memory requirements compared to the canonical n×n attention form [3].\n\nTo preserve per-segment expressivity while operating on compressed group-level attention, RITA combines an embedding aggregation strategy with a customized group-aware softmax. The aggregation strategy computes Queries, Keys, and Values at the segment level, then aggregates these into per-group representations and performs attention on the compact group matrix; a redistribution mechanism reconstructs distinct segment-level outputs from the group results without materializing the full n×n attention matrix. The theoretical formulation supports correctness guarantees showing that, under the proposed scheme, the original attention matrix can be effectively restored within provable bounds, so no uncontrolled embedding error is introduced beyond these bounds [3].\n\nRITA further introduces an adaptive scheduler and batching strategy to maximize runtime efficiency while enforcing a user-specified approximation tolerance. The scheduler adaptively selects the number of groups N per layer and iteration, iteratively merging groups until a provable approximation error bound is met; concurrently, RITA models and adapts the attainable batch size B as a function of N to improve GPU utilization by learning a mapping from sampled <N,B> pairs. Implementation-level choices—including a GPU-friendly parallel grouping algorithm, group-level attention kernels, and dynamic group-merging logic—enable practical deployment. Empirically, across multiple public timeseries benchmarks and a clinical EEG dataset, RITA achieves substantial speedups (reported between 4× and as high as 63×) over baseline attention mechanisms while matching or improving predictive accuracy on forecasting, classification, and anomaly-detection tasks, demonstrating that clustered attention with a tunable approximation bound makes Transformer-style models practical for long-timeseries analytics [3].\n\n### Group attention algorithmic components\n\nRITA first chunks the input timeseries into segments and clusters segments with similar embeddings into N groups; this clustering is dynamic and may vary by layer and iteration, allowing the grouping to adapt to evolving representations within the model. The method computes Queries, Keys, and Values at the original segment granularity but aggregates these into per-group representations so that subsequent attention operates on a compressed group-level attention matrix A_group of size N×N rather than the full n×n matrix, thereby reducing computational and memory overhead [3].\n\nA core component is the embedding aggregation and reconstruction pipeline: after computing group-level attention and group outputs, RITA applies an aggregation strategy to redistribute group outputs back to individual segments so that each original segment receives a distinct embedding without requiring expansion to the full attention matrix. To enable correct redistribution, RITA replaces the classical softmax with a grouping-aware normalization (\"group softmax\") that preserves the necessary normalization properties across grouped keys and queries; this grouping-aware softmax ensures attention weights are correctly redistributed to segment-level outputs while operating only on the compressed representation [3].\n\n### Adaptive scheduler, error bounds, and batching\n\nRITA uses an adaptive scheduler that begins with a comparatively large number of groups N and iteratively merges similar groups while monitoring a provable approximation error bound; merging decisions are made when two groups are determined mergeable under the user-specified tolerance, so the scheduler balances approximation quality against computational savings. These error bounds are formalized to guide merging decisions, enabling the system to maintain a user-specified approximation quality while progressively reducing computational load [3].\n\nConcurrently, RITA models how available memory and compute change as groups are merged and leverages a learned, simple predictive mapping from N to an effective batch size B. By sampling a small set of <N,B> pairs and learning this mapping offline or during warm-up, the system increases batch sizes opportunistically as N is reduced, improving throughput and GPU utilization without violating the approximation constraints enforced by the scheduler [3].\n\n### Empirical scalability and accuracy outcomes\n\nEmpirical evaluation on multiple public timeseries datasets and a clinical EEG dataset shows that RITA outperforms existing timeseries attention approximations in predictive accuracy while substantially lowering runtime and memory footprints, demonstrating both efficiency and effectiveness on forecasting, classification, and anomaly-detection tasks [3]. The method scales to very long sequences (empirically in the regime of ≥2000 segments), a region where standard self-attention becomes infeasible, and reports speedups up to 63× over canonical attention baselines in favorable settings [3].\n\nIn practice, choosing the number of groups N controls the trade-off between computational speed and approximation error; RITA’s scheduler automates this trade-off on a per-layer and per-iteration basis, meeting user-specified error bounds while exploiting increased parallelism through larger batch sizes when memory is freed. These empirical and practical characteristics suggest that group attention extends Transformer-style architectures to long-timeseries analytics by offering a tunable approximation mechanism that yields large efficiency gains with formally guided error control [3]."
  ],
  "criticism": {
    "0": "PASS",
    "1": "PASS",
    "2": "PASS"
  },
  "is_criticized": true
}