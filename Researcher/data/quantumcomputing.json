{
  "topic": "Quantum Computing",
  "source": "both",
  "wikipedia_docs": "Index: 1\nTitle: Quantum computing\nSource: https://en.wikipedia.org/wiki/Quantum_computing\nContent: A quantum computer is a (real or theoretical) computer that uses quantum mechanical phenomena in an essential way: it exploits superposed and entangled states, and the intrinsically non-deterministic outcomes of quantum measurements, as features of its computation. Quantum computers can be viewed as sampling from quantum systems that evolve in ways classically described as operating on an enormous number of possibilities simultaneously, though still subject to strict computational constraints. By contrast, ordinary (\"classical\") computers operate according to deterministic rules. Any classical computer can, in principle, be replicated by a (classical) mechanical device such as a Turing machine, with only polynomial overhead in time. Quantum computers, on the other hand are believed to require exponentially more resources to simulate classically. It is widely believed that a scalable quantum computer could perform some calculations exponentially faster than any classical computer. Theoretically, a large-scale quantum computer could break some widely used public-key cryptographic schemes and aid physicists in performing physical simulations. However, current hardware implementations of quantum computation are largely experimental and only suitable for specialized tasks.\nThe basic unit of information in quantum computing, the qubit (or \"quantum bit\"), serves the same function as the bit in ordinary or \"classical\" computing. However, unlike a classical bit, which can be in one of two states (a binary), a qubit can exist in a linear combination of two states known as a quantum superposition. The result of measuring a qubit is one of the two states given by a probabilistic rule. If a quantum computer manipulates the qubit in a particular way, wave interference effects amplify probability of the desired measurement result. The design of quantum algorithms involves creating procedures that allow a quantum computer to perform this amplification.\nQuantum computers are not yet practical for real-world applications. Physically engineering high-quality qubits has proven to be challenging. If a physical qubit is not sufficiently isolated from its environment, it suffers from quantum decoherence, introducing noise into calculations. National governments have invested heavily in experimental research aimed at developing scalable qubits with longer coherence times and lower error rates. Example implementations include superconductors (which isolate an electrical current by eliminating electrical resistance) and ion traps (which confine a single atomic particle using electromagnetic fields). Researchers have claimed, and are widely believed to be correct, that certain quantum devices can outperform classical computers on narrowly defined tasks, a milestone referred to as quantum advantage or quantum supremacy. These tasks are not necessarily useful for real-world applications.\n\n\n== History ==\n\nFor many years, the fields of quantum mechanics and computer science formed distinct academic communities. Modern quantum theory developed in the 1920s to explain perplexing physical phenomena observed at atomic scales, and digital computers emerged in the following decades to replace human computers for tedious calculations. Both disciplines had practical applications during World War II; computers played a major role in wartime cryptography, and quantum physics was essential for nuclear physics used in the Manhattan Project.\nAs physicists applied quantum mechanical models to computational problems and swapped digital bits for qubits, the fields of quantum mechanics and computer science began to converge. In 1980, Paul Benioff introduced the quantum Turing machine, which uses quantum theory to describe a simplified computer.\nWhen digital computers became faster, physicists faced an exponential increase in overhead when simulating quantum dynamics, prompting Yuri Manin and Richard Feynman to independently suggest that hardware based on quantum phenomena might be more efficient for computer simulation.\nIn a 1984 paper, Charles Bennett and Gilles Brassard applied quantum theory to cryptography protocols and demonstrated that quantum key distribution could enhance information security.\nQuantum algorithms then emerged for solving oracle problems, such as Deutsch's algorithm in 1985, the Bernstein–Vazirani algorithm in 1993, and Simon's algorithm in 1994.\nThese algorithms did not solve practical problems, but demonstrated mathematically that one could gain more information by querying a black box with a quantum state in superposition, sometimes referred to as quantum parallelism.\n\nPeter Shor built on these results with his 1994 algorithm for breaking the widely used RSA and Diffie–Hellman encryption protocols, which drew significant attention to the field of quantum computing. In 1996, Grover's algorithm established a quantum speedup for the widely applicable unstructured search problem. The same year, Seth Lloyd proved that quantum computers could simulate quantum systems without the exponential overhead present in classical simulations, validating Feynman's 1982 conjecture.\nOver the years, experimentalists have constructed small-scale quantum computers using trapped ions and superconductors.\nIn 1998, a two-qubit quantum computer demonstrated the feasibility of the technology, and subsequent experiments have increased the number of qubits and reduced error rates.\nIn 2019, Google AI and NASA announced that they had achieved quantum supremacy with a 54-qubit machine, performing a computation that is impossible for any classical computer.\nThis announcement was met with a rebuttal from Google's direct competitor, IBM. IBM contended that the calculation Google claimed would take 10,000 years could be performed in just 2.5 days on its own Summit supercomputer if its architecture were optimized, sparking a debate over the precise threshold for \"quantum supremacy\".\n\n\n== Quantum information processing ==\nComputer engineers typically describe a modern computer's operation in terms of classical electrodynamics.\nWithin these \"classical\" computers, some components (such as semiconductors and random number generators) may rely on quantum behavior, but these components are not isolated from their environment, so any quantum information quickly decoheres.\nWhile programmers may depend on probability theory when designing a randomized algorithm, quantum mechanical notions like superposition and interference are largely irrelevant for program analysis.\nQuantum programs, in contrast, rely on precise control of coherent quantum systems. Physicists describe these systems mathematically using linear algebra. Complex numbers model probability amplitudes, vectors model quantum states, and matrices model the operations that can be performed on these states. Programming a quantum computer is then a matter of composing operations in such a way that the resulting program computes a useful result in theory and is implementable in practice.\nAs physicist Charlie Bennett describes the relationship between quantum and classical computers,\n\nA classical computer is a quantum computer ... so we shouldn't be asking about \"where do quantum speedups come from?\" We should say, \"well, all computers are quantum. ... Where do classical slowdowns come from?\"\n\n\n=== Quantum information ===\nJust as the bit is the basic concept of classical information theory, the qubit is the fundamental unit of quantum information. The same term qubit is used to refer to an abstract mathematical model and to any physical system that is represented by that model. A classical bit, by definition, exists in either of two physical states, which can be denoted 0 and 1. A qubit is also described by a state, and two states often written \n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n      \n    \n    {\\displaystyle |0\\rangle }\n  \n and \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle |1\\rangle }\n  \n serve as the quantum counterparts of the classical states 0 and 1. However, the quantum states \n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n      \n    \n    {\\displaystyle |0\\rangle }\n  \n and \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle |1\\rangle }\n  \n belong to a vector space, meaning that they can be multiplied by constants and added together, and the result is again a valid quantum state. Such a combination is known as a superposition of \n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n      \n    \n    {\\displaystyle |0\\rangle }\n  \n and \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle |1\\rangle }\n  \n.\nA two-dimensional vector mathematically represents a qubit state. Physicists typically use Dirac notation for quantum mechanical linear algebra, writing \n  \n    \n      \n        \n          |\n        \n        ψ\n        ⟩\n      \n    \n    {\\displaystyle |\\psi \\rangle }\n  \n 'ket psi' for a vector labeled \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n . Because a qubit is a two-state system, any qubit state takes the form \n  \n    \n      \n        α\n        \n          |\n        \n        0\n        ⟩\n        +\n        β\n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle \\alpha |0\\rangle +\\beta |1\\rangle }\n  \n , where \n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n      \n    \n    {\\displaystyle |0\\rangle }\n  \n and \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle |1\\rangle }\n  \n are the standard basis states, and \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n and \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n are the probability amplitudes, which are in general complex numbers. If either \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n or \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n is zero, the qubit is effectively a classical bit; when both are nonzero, the qubit is in superposition. Such a quantum state vector acts similarly to a (classical) probability vector, with one key difference: unlike probabilities, probability amplitudes are not necessarily positive numbers. Negative amplitudes allow for destructive wave interference.\nWhen a qubit is measured in the standard basis, the result is a classical bit. The Born rule describes the norm-squared correspondence between amplitudes and probabilities—when measuring a qubit \n  \n    \n      \n        α\n        \n          |\n        \n        0\n        ⟩\n        +\n        β\n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle \\alpha |0\\rangle +\\beta |1\\rangle }\n  \n, the state collapses to \n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n      \n    \n    {\\displaystyle |0\\rangle }\n  \n with probability \n  \n    \n      \n        \n          |\n        \n        α\n        \n          \n            |\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle |\\alpha |^{2}}\n  \n, or to \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle |1\\rangle }\n  \n with probability \n  \n    \n      \n        \n          |\n        \n        β\n        \n          \n            |\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle |\\beta |^{2}}\n  \n.\nAny valid qubit state has coefficients \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n and \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n such that \n  \n    \n      \n        \n          |\n        \n        α\n        \n          \n            |\n          \n          \n            2\n          \n        \n        +\n        \n          |\n        \n        β\n        \n          \n            |\n          \n          \n            2\n          \n        \n        =\n\n---\n\nIndex: 2\nTitle: Timeline of quantum computing and communication\nSource: https://en.wikipedia.org/wiki/Timeline_of_quantum_computing_and_communication\nContent: This is a timeline of quantum computing and communication.\n\n\n== 1960s ==\n\n\n=== 1968/69/70 ===\nStephen Wiesner invents conjugate coding.\n\n\n=== 1969 ===\n13 June – James L. Park (Washington State University, Pullman)'s paper is received by Foundations of Physics, in which he describes the non possibility of disturbance in a quantum transition state in the context of a disproof of quantum jumps in the concept of the atom described by Bohr.\n\n\n== 1970s ==\n\n\n=== 1973 ===\nAlexander Holevo's paper is published. The Holevo bound describes a limit of the quantity of classical information which is possible to quanta encode.\nCharles H. Bennett shows that computation can be done reversibly.\n\n\n=== 1975 ===\nR. P. Poplavskii publishes \"Thermodynamical models of information processing\" (in Russian) which shows the computational infeasibility of simulating quantum systems on classical computers, due to the superposition principle.\nRoman Stanisław Ingarden, a Polish mathematical physicist, submits the paper \"Quantum Information Theory\" in Reports on Mathematical Physics, vol. 10, pp. 43–72, published 1976. It is one of the first attempts at creating a quantum information theory, showing that Shannon information theory cannot directly be generalized to the quantum case, but rather that it is possible to construct a quantum information theory, which is a generalization of Shannon's theory, within the formalism of a generalized quantum mechanics of open systems and a generalized concept of observables (the so-called semi-observables).\n\n\n== 1980s ==\n\n\n=== 1980 ===\nPaul Benioff describes the first quantum mechanical model of a computer. In this work, Benioff showed that a computer could operate under the laws of quantum mechanics by describing a Schrödinger equation description of Turing machines, laying a foundation for further work in quantum computing. The paper was submitted in June 1979 and published in April 1980.\nYuri Manin briefly motivates the idea of quantum computing.\nTommaso Toffoli introduces the reversible Toffoli gate, which (together with initialized ancilla bits) is functionally complete for reversible classical computation.\n\n\n=== 1981 ===\nAt the first Conference on the Physics of Computation, held at the Massachusetts Institute of Technology (MIT) in May, Paul Benioff and Richard Feynman give talks on quantum computing. Benioff's talk built on his earlier 1980 work showing that a computer can operate under the laws of quantum mechanics. The talk was titled \"Quantum mechanical Hamiltonian models of discrete processes that erase their own histories: application to Turing machines\". In Feynman's talk, he observed that it appeared to be impossible to efficiently simulate the evolution of a quantum nature system on a classical computer, and he proposed a basic model for a quantum computer. Feynman's conjecture on a quantum simulating computer, published 1982, understood as – the reality of quantum mechanics expressed as an effective quantum system necessitates quantum computers, is conventionally accepted as a beginning of quantum computing.\n\n\n=== 1982 ===\nPaul Benioff further develops his original model of a quantum mechanical Turing machine.\nWilliam Wootters and Wojciech H. Zurek, and independently Dennis Dieks rediscover the no-cloning theorem of James L. Park.\n\n\n=== 1984 ===\nCharles Bennett and Gilles Brassard employ Wiesner's conjugate coding for distribution of cryptographic keys.\n\n\n=== 1985 ===\nDavid Deutsch, at the University of Oxford, England, describes the first universal quantum computer. Just as a Universal Turing machine can simulate any other Turing machine efficiently (Church–Turing thesis), so the universal quantum computer is able to simulate any other quantum computer with at most a polynomial slowdown.\nAsher Peres points out the need for quantum error correction schemes and discusses a repetition code for amplitude errors.\n\n\n=== 1988 ===\nYoshihisa Yamamoto and K. Igeta propose the first physical realization of a quantum computer, including Feynman's CNOT gate. Their approach uses atoms and photons and is the progenitor of modern quantum computing and networking protocols using photons to transmit qubits and atoms to perform two-qubit operations.\n\n\n=== 1989 ===\nGerard J. Milburn proposes a quantum-optical realization of a Fredkin gate.\nBikas Chakrabarti & collaborators from Saha Institute of Nuclear Physics, Kolkata, India, propose that quantum fluctuations could help explore rugged energy landscapes by escaping from local minima of glassy systems having tall but thin barriers by tunneling (instead of climbing over using thermal excitations), suggesting the effectiveness of quantum annealing over classical simulated annealing.\n\n\n== 1990s ==\n\n\n=== 1991 ===\nArtur Ekert at the University of Oxford, proposes entanglement-based secure communication.\n\n\n=== 1992 ===\nDavid Deutsch and Richard Jozsa propose a computational problem that can be solved efficiently with the deterministic Deutsch–Jozsa algorithm on a quantum computer, but for which no deterministic classical algorithm is possible. This was perhaps the earliest result in the computational complexity of quantum computers, proving that they were capable of performing some well-defined computation more efficiently than any classical computer.\nEthan Bernstein and Umesh Vazirani propose the Bernstein–Vazirani algorithm. It is a restricted version of the Deutsch–Jozsa algorithm where instead of distinguishing between two different classes of functions, it tries to learn a string encoded in a function. The Bernstein–Vazirani algorithm was designed to prove an oracle separation between complexity classes BQP and BPP.\nResearch groups at Max Planck Institute of Quantum Optics (Garching) and shortly after at NIST (Boulder) experimentally realize the first crystallized strings of laser-cooled ions. Linear ion crystals constitute the qubit basis for most quantum computing and simulation experiments with trapped ions.\n\n\n=== 1993 ===\nDaniel R. Simon, at Université de Montréal, Quebec, Canada, invent an oracle problem, Simon's problem, for which a quantum computer would be exponentially faster than a conventional computer. This algorithm introduces the main ideas which were then developed in Peter Shor's factorization algorithm.\n\n\n=== 1994 ===\nPeter Shor, at AT&T's Bell Labs in New Jersey, publishes Shor's algorithm. It would allow a quantum computer to factor large integers quickly. It solves both the factoring problem and the discrete log problem. The algorithm can theoretically break many of the cryptosystems in use today. Its invention sparked tremendous interest in quantum computers.\nThe first United States Government workshop on quantum computing is organized by NIST in Gaithersburg, Maryland, in autumn.\nIsaac Chuang and Yoshihisa Yamamoto propose a quantum-optical realization of a quantum computer to implement Deutsch's algorithm. Their work introduced dual-rail encoding for photonic qubits.\nIn December, Ignacio Cirac, at University of Castilla–La Mancha at Ciudad Real, and Peter Zoller at the University of Innsbruck propose an experimental realization of the controlled NOT gate with cold trapped ions.\n\n\n=== 1995 ===\nThe first United States Department of Defense workshop on quantum computing and quantum cryptography is organized by United States Army physicists Charles M. Bowden, Jonathan Dowling, and Henry O. Everitt; it took place in February at the University of Arizona in Tucson.\nPeter Shor proposes the first schemes for quantum error correction.\nChristopher Monroe and David J. Wineland at NIST (Boulder, Colorado) experimentally realize the first quantum logic gate – the controlled NOT gate – with trapped ions, following the Cirac-Zoller proposal.\nIndependently, Subhash Kak and Ronald Chrisley propose the first quantum neural network.\n\n\n=== 1996 ===\nLov Grover, at Bell Labs, invents the quantum database search algorithm. The quadratic speedup is not as dramatic as the speedup for factoring, discrete logs, or physics simulations. However, the algorithm can be applied to a much wider variety of problems. Any problem that can be solved by random, brute-force search, may take advantage of this quadratic speedup in the number of search queries.\nThe United States Government, particularly in a joint partnership of the Army Research Office (now part of the Army Research Laboratory) and the National Security Agency, issues the first public call for research proposals in quantum information processing.\nAndrew Steane designs Steane code for error correction.\nDavid DiVincenzo, of IBM, proposes a list of minimal requirements for creating a quantum computer, now called DiVincenzo's criteria.\nSeth Lloyd proves Feynman's conjecture on quantum simulation.\n\n\n=== 1997 ===\nDavid G. Cory, Amr Fahmy and Timothy Havel, and at the same time Neil Gershenfeld and Isaac Chuang at MIT publish the first papers realizing gates for quantum computers based on bulk nuclear spin resonance, or thermal ensembles. The technology is based on a nuclear magnetic resonance (NMR) machine, which is similar to the medical magnetic resonance imaging machine.\nAlexei Kitaev describes the principles of topological quantum computation as a method for dealing with the problem of decoherence.\nDaniel Loss and David DiVincenzo propose the Loss-DiVincenzo quantum computer, using as qubits the intrinsic spin-1/2 degree of freedom of individual electrons confined to quantum dots.\n\n\n=== 1998 ===\nThe first experimental demonstration of a quantum algorithm is reported. A working 2-qubit NMR quantum computer was used to solve Deutsch's problem by Jonathan A. Jones and Michele Mosca at Oxford University and shortly after by Isaac L. Chuang at IBM's Almaden Research Center, in California, and Mark Kubinec and the University of California, Berkeley together with coworkers at Stanford University in California and MIT in Massachusetts.\nThe first working 3-qubit NMR computer is reported.\nBruce Kane proposes a silicon-based nuclear spin quantum computer, using nuclear spins of individual phosphorus atoms in silicon as the qubits and donor electrons to mediate the coupling between qubits.\nThe first execution of Grover's algorithm on an NMR computer is reported.\nHidetoshi Nishimori & colleagues from Tokyo Institute of Technology show that a quantum annealing algorithm can perform better than classical simulated annealing under certain conditions.\nDaniel Gottesman and Emanuel Knill independently prove that a certain subclass of quantum computations can be efficiently emulated with classical resources (Gottesman–Knill theorem).\n\n\n=== 1999 ===\nSamuel L. Braunstein and collaborators show that none of the bulk NMR experiments performed to date contain any entanglement; the quantum states being too strongly mixed. This is seen as evidence that NMR computers would likely not yield a benefit over classical computers. It remains an open question, however, whether entanglement is necessary for quantum computational speedup.\nGabriel Aeppli, Thomas Rosenbaum and colleagues demonstrate experimentally the basic concepts of quantum annealing in a condensed matter system.\nYasunobu Nakamura and Jaw-Shen Tsai demonstrate that a superconducting circuit can be used as a qubit.\n\n\n== 2000s ==\n\n\n=== 2000 ===\nArun K. Pati and Samuel L. Braunstein prove the quantum no-deleting theorem. This is dual to the no-cloning theorem which shows that one cannot delete a copy of an unknown qubit. Together with the stronger no-cloning theorem, the no-deleting theorem has the implication that quantum information can neither be created nor be destroyed.\nThe first working 5-qubit NMR computer is demonstrated at the Technical University of Munich, Germany.\nThe first execution of order finding (part of Shor's algorithm) at IBM's Almaden Research Center and Stanford University is demonstrated.\nThe first working 7-qubit NMR computer is demonstrated at the Los Alamos National Laboratory in New Mexico.\nThe textbook, Quantum Comp\n\n---\n\nIndex: 3\nTitle: Superconducting quantum computing\nSource: https://en.wikipedia.org/wiki/Superconducting_quantum_computing\nContent: Superconducting quantum computing is a branch of solid state  physics and quantum computing that implements superconducting electronic circuits using superconducting qubits as artificial atoms, or quantum dots. For superconducting qubits, the two logic states are the ground state and the excited state, denoted \n  \n    \n      \n        \n          |\n        \n        g\n        ⟩\n        \n           and \n        \n        \n          |\n        \n        e\n        ⟩\n      \n    \n    {\\displaystyle |g\\rangle {\\text{ and }}|e\\rangle }\n  \n respectively. Research in superconducting quantum computing is conducted by companies such as Google, IBM, IMEC, BBN Technologies, Rigetti, and Intel.  Many recently developed QPUs (quantum processing units, or quantum chips) use superconducting architecture.\nAs of May 2016, up to 9 fully controllable qubits are demonstrated in the 1D array, and up to 16 in 2D architecture. In October 2019, the Martinis group, partnered with Google, published an article demonstrating novel quantum supremacy, using a chip composed of 53 superconducting qubits.\n\n\n== Background ==\nClassical computation models rely on physical implementations consistent with the laws of classical mechanics. Classical descriptions are accurate only for specific systems consisting of a relatively large number of atoms. A more general description of nature is given by quantum mechanics. Quantum computation studies quantum phenomena applications beyond the scope of classical approximation, with the purpose of performing quantum information processing and communication. Various models of quantum computation exist, but the most popular models incorporate concepts of qubits and quantum gates (or gate-based superconducting quantum computing).\nSuperconductors are implemented due to the fact that at low temperatures they have infinite conductivity and zero resistance. Each qubit is built using semiconductor circuits with an LC circuit: a capacitor and an inductor.\nSuperconducting capacitors and inductors are used to produce a resonant circuit that dissipates almost no energy, as heat  can disrupt quantum information. The superconducting resonant circuits are a class of artificial atoms that can be used as qubits. Theoretical and physical implementations of quantum circuits are widely different. Implementing a quantum circuit had its own set of challenges and must abide by DiVincenzo's criteria, conditions proposed by theoretical physicist David P DiVincenzo, which is set of criteria for the physical implementation of superconducting quantum computing, where the initial five criteria ensure that the quantum computer is in line with the postulates of quantum mechanics and the remaining two pertaining to the relaying of this information over a network.\nWe map the ground and excited states of these atoms to the 0 and 1 state as these are discrete and distinct energy values and therefore it is in line with the postulates of quantum mechanics. In such a construction however an electron can jump to multiple other energy states and not be confined to our excited state; therefore, it is imperative that the system be limited to be affected only by photons with energy difference required to jump from the ground state to the excited state. However, this leaves one major issue, we require uneven spacing between our energy levels to prevent photons with the same energy from causing transitions between neighboring pairs of states. Josephson junctions are superconducting elements with a nonlinear inductance, which is critically important for qubit implementation. The use of this nonlinear element in the resonant superconducting circuit produces uneven spacings between the energy levels.\n\n\n=== Qubits ===\nA qubit is a generalization of a bit (a system with two possible states) capable of occupying a quantum superposition of both states. A quantum gate, on the other hand, is a generalization of a logic gate describing the transformation of one or more qubits once a gate is applied given their initial state. Physical implementation of qubits and gates is challenging for the same reason that quantum phenomena are difficult to observe in everyday life given the minute scale on which they occur. One approach to achieving quantum computers is by implementing superconductors whereby quantum effects are macroscopically observable, though at the price of extremely low operation temperatures.\n\n\n=== Superconductors ===\nUnlike typical conductors, superconductors possess a critical temperature at which resistivity plummets to zero and conductivity is drastically increased. In superconductors, the basic charge carriers are pairs of electrons (known as Cooper pairs), rather than single fermions as found in typical conductors.  Cooper pairs are loosely bound and have an energy state lower than that of Fermi energy. Electrons forming Cooper pairs possess equal and opposite momentum and spin so that the total spin of the Cooper pair is an integer spin. Hence, Cooper pairs are bosons. Two such superconductors which have been used in superconducting qubit models are niobium and tantalum, both d-band superconductors.\n\n\n==== Bose–Einstein condensates ====\nOnce cooled to nearly absolute zero, a collection of bosons collapse into their lowest energy quantum state (the ground state) to form a state of matter known as Bose–Einstein condensate. Unlike fermions, bosons may occupy the same quantum energy level (or quantum state) and do not obey the Pauli exclusion principle. Classically, Bose-Einstein Condensate can be conceptualized as multiple particles occupying the same position in space and having equal momentum. Because interactive forces between bosons are minimized, Bose-Einstein Condensates effectively act as a superconductor. Thus, superconductors are implemented in quantum computing because they possess both near infinite conductivity and near zero resistance. The advantages of a superconductor over a typical conductor, then, are twofold in that superconductors can, in theory, transmit signals nearly instantaneously and run infinitely with no energy loss. The prospect of actualizing superconducting quantum computers becomes all the more promising considering NASA's recent development of the Cold Atom Lab in outer space where Bose-Einstein Condensates are more readily achieved and sustained (without rapid dissipation) for longer periods of time without the constraints of gravity.\n\n\n=== Electrical circuits ===\nAt each point of a superconducting electronic circuit (a network of electrical elements), the condensate wave function describing charge flow is well-defined by some complex probability amplitude. In typical conductor electrical circuits, this same description is true for individual charge carriers except that the various wave functions are averaged in macroscopic analysis,  making it impossible to observe quantum effects. The condensate wave function becomes useful in allowing design and measurement of macroscopic quantum effects. Similar to the discrete atomic energy levels in the Bohr model, only discrete numbers of magnetic flux quanta can penetrate a superconducting loop. In both cases, quantization results from complex amplitude continuity. Differing from microscopic implementations of quantum computers (such as atoms or photons), parameters of superconducting circuits are designed by setting (classical) values to the electrical elements composing them such as by adjusting capacitance or inductance.\nTo obtain a quantum mechanical description of an electrical circuit, a few steps are required. Firstly, all electrical elements must be described by the condensate wave function amplitude and phase rather than by closely related macroscopic current and voltage descriptions used for classical circuits. For instance, the square of the wave function amplitude at any arbitrary point in space corresponds to the probability of finding a charge carrier there. Therefore, the squared amplitude corresponds to a classical charge distribution. The second requirement to obtain a quantum mechanical description of an electrical circuit is that generalized Kirchhoff's circuit laws are applied at every node of the circuit network to obtain the system's equations of motion. Finally, these equations of motion must be reformulated to Lagrangian mechanics such that a quantum Hamiltonian is derived describing the total energy of the system.\n\n\n== Technology ==\n\n\n=== Manufacturing ===\nSuperconducting quantum computing devices are typically designed in the radio-frequency spectrum, cooled in dilution refrigerators below 15 mK and addressed with conventional electronic instruments, e.g. frequency synthesizers and spectrum analyzers. Typical dimensions fall on the range of micrometers, with sub-micrometer resolution, allowing for the convenient design of a Hamiltonian system with well-established integrated circuit technology. Manufacturing superconducting qubits follows a process involving lithography, depositing of metal, etching, and controlled oxidation as described in. Manufacturers continue to improve the lifetime of superconducting qubits and have made significant improvements since the early 2000s.\n\n\n=== Josephson junctions ===\n\nOne distinguishable attribute of superconducting quantum circuits is the use of Josephson junctions. Josephson junctions are an electrical element which does not exist in normal conductors. Recall that a junction is a weak connection between two leads of wire (in this case a superconductive wire) on either side of a thin layer of insulator material only a few atoms thick, usually implemented using shadow evaporation technique. The resulting Josephson junction device exhibits the Josephson Effect whereby the junction produces a supercurrent. An image of a single Josephson junction is shown to the right. The condensate wave function on the two sides of the junction are weakly correlated, meaning that they are allowed to have different superconducting phases. This distinction of nonlinearity contrasts continuous superconducting wire for which the wave function across the junction must be continuous. Current flow through the junction occurs by quantum tunneling, seeming to instantaneously \"tunnel\" from one side of the junction to the other. This tunneling phenomenon is unique to quantum systems. Thus, quantum tunneling is used to create nonlinear inductance, essential for qubit design as it allows a design of anharmonic oscillators for which energy levels are discretized (or quantized) with nonuniform spacing between energy levels, denoted \n  \n    \n      \n        Δ\n        E\n      \n    \n    {\\displaystyle \\Delta E}\n  \n. In contrast, the quantum harmonic oscillator cannot be used as a qubit as there is no way to address only two of its states, given that the spacing between every energy level and the next is exactly the same.\n\n\n== Qubit archetypes ==\nThe three primary superconducting qubit archetypes are the phase, charge and flux qubit. Many hybridizations of these archetypes exist including the fluxonium, transmon, Xmon, and quantronium. For any qubit implementation the logical quantum states \n  \n    \n      \n        {\n        \n          |\n        \n        0\n        ⟩\n        ,\n        \n          |\n        \n        1\n        ⟩\n        }\n      \n    \n    {\\displaystyle \\{|0\\rangle ,|1\\rangle \\}}\n  \n are mapped to different states of the physical system (typically to discrete energy levels or their quantum superpositions). Each of the three archetypes possess a distinct range of Josephson energy to charging energy ratio. Josephson energy refers to the energy stored in Josephson junctions when current passes through, and charging energy is the energy required for one Cooper pair to charge the junction's total capacitance. Josephson energy can be written as \n\n  \n    \n      \n        \n          U\n          \n            j\n          \n        \n        =\n        −\n        \n          \n            \n              \n                I\n\n---\n",
  "arxiv_docs": "Index: 1\nTitle: The Rise of Quantum Internet Computing\nPublished: 2022-08-01\nAuthors: Seng W. Loke\nSource: Arxiv research paper\nContent: arXiv:2208.00733v1  [cs.ET]  1 Aug 2022\nIEEE IOT MAGAZINE, VOL. XX, NO. X, X 2022\n1\nThe Rise of Quantum Internet Computing\nSeng W. Loke, Member, IEEE\nAbstract—This article highlights quantum Internet computing as referring to distributed quantum computing over the quantum Internet,\nanalogous to (classical) Internet computing involving (classical) distributed computing over the (classical) Internet. Relevant to\nquantum Internet computing would be areas of study such as quantum protocols for distributed nodes using quantum information for\ncomputations, quantum cloud computing, delegated veriﬁable blind or private computing, non-local gates, and distributed quantum\napplications, over Internet-scale distances.\nIndex Terms—quantum Internet computing, quantum Internet, distributed quantum computing, Internet computing, distributed\nsystems, Internet\n”This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this\nversion may no longer be accessible.”\n✦\n1\nINTRODUCTION\nT\nHERE have been tremendous developments in quantum\ncomputing, quantum cryptography, quantum commu-\nnications and the quantum Internet, and we have seen\nincreased investments and intensive research in quantum\ncomputing in recent years [1], [2]. The quantum Internet will\nnot necessarily replace the (classical) Internet we know and\nuse today, at least not in the near future, but can complement\nthe current Internet. The quantum Internet aims to enable\nrobust quantum teleportation (or transmission) of qubits,1\nand entanglement among qubits,2 over long Internet-scale\ndistances, which are key to many of the quantum protocols\nincluding quantum key distribution, quantum voting, and\nothers, as well as for non-local control of quantum gates.\nThere have been efforts to build quantum computers,\nand it remains to see if any one paradigm becomes the\ndominant or best way of building such quantum comput-\ners. At the same time, even as researchers develop more\npowerful quantum computers (supporting more qubits for\noperations, and at lower error rates), there is an opportunity\nfor connecting multiple quantum computers from differ-\nent sites to achieve much more complex quantum com-\nputations, i.e., inter-linking multiple quantum computers\non different sites to perform distributed computing with\na distributed system of quantum computers (or quantum\nprocessing units (QPUs) at different nodes), arriving at the\nnotion of distributed quantum computing, e.g., [3].\nWhile distributed quantum computing can involve mul-\ntiple QPUs next to each other or at the same site, with the\nquantum Internet, one can envision distributed quantum\n•\nSeng W. Loke is with the School of Information Technology, Deakin\nUniversity, Melbourne, Australia.\nE-mail: see https://www.deakin.edu.au/about-deakin/people/seng-loke.\nManuscript received X XX, 20XX; revised X XX, 20XX.\n1. A qubit is the basic unit of quantum information, and can be\nthought of as a two-state, or two-levelled, quantum-mechanical system,\nsuch as an electron’s spin, where the two levels are spin up and spin\ndown, or a photon’s polarization, where the two states are the vertical\npolarization and the horizontal polarization.\n2. Multiple qubits at different sites can share an entangled state, a\nsuperpositon of “specially correlated” states, to be used in distributed\nalgorithms.\ncomputing over nodes geographically far apart. As noted\nin [4], the idea is the quantum Internet as the “underly-\ning infrastructure of the Distributed Quantum Computing\necosystem.”\nThis article highlights the emerging area of distributed\nquantum computing over the quantum Internet, which we\nrefer to as quantum Internet computing, i.e., the idea of com-\nputing using quantumly connected distributed quantum\ncomputers over Internet-scale distances. Hence, quantum\nInternet computing is not a new concept in itself but a\nproposed “umbrella term” used here for the collection of\ntopics (listed below), from an analogy to (classical) Internet\ncomputing.\nInternet computing, where one does distributed comput-\ning but over Internet-scale distances and distributed sys-\ntems involve nodes connected via the Internet, is at the inter-\nsection of work in (classical) distributed computing and the\n(classical) Internet. Analogous to Internet computing, one\ncould ask the question of what would be at the intersection\nof work in distributed quantum computing and work on the\nquantum Internet, which brings us to the notion of quantum\nInternet computing.\nAlso, while the quantum Internet and distributed quan-\ntum computing are still nascent research areas, there are at\nleast three key topics which can be considered as relevant to\nquantum Internet computing:\n•\ndistributed quantum computing, including quantum\nprotocols from theoretical perspectives involving\ncommunication complexity studies, and distributed\nquantum computing via non-local or distributed\nquantum gates,\n•\nquantum cloud computing with a focus on delegat-\ning quantum computations, blind quantum comput-\ning, and verifying delegated quantum computations,\nand\n•\ncomputations and algorithms for the quantum Inter-\nnet including key ideas such as quantum entangle-\nment distillation, entanglement swapping, quantum\nIEEE IOT MAGAZINE, VOL. XX, NO. X, X 2022\n2\nrepeaters, and quantum Internet standards.3\nWe brieﬂy discuss the above topics in the following sections.\n2\nDISTRIBUTED QUANTUM COMPUTING\nDistributed quantum computing problems and quantum\nprotocols have been well-studied for over two decades,\nfrom a theoretical computer science perspective,4 many of\nwhich have their inspiration from classical distributed com-\nputing research. Quantum versions of classical distributed\ncomputing problems and protocols, and new forms of dis-\ntributed computing using quantum information, have been\nexplored, e.g., the distributed three-party product problem,\nthe distributed Deutsch-Jozsa promise problem and the\ndistributed intersection problem, demonstrating how, for\nsome problems, quantum information can enable fewer\nbits of communication to be used for a solution, and how\ncertain distributed computation problems can be solved\nwith quantum information, but cannot be solved classically.\nMany quantum protocols, including quantum coin ﬂipping,\nquantum leader election, quantum anonymous broadcast-\ning, quantum voting, quantum Byzantine Generals, quan-\ntum secret sharing, and quantum oblivious transfer, can\nbe viewed as “quantum versions” of classical distributed\ncomputing problems, and have been studied extensively.\nAnother area of study, which has also been considered\nas distributed quantum computing, is non-local gates, or\nthe non-local control of quantum gates, including early\nwork nearly over two decades ago.5 Key to performing\nsuch non-local control of quantum gates is the use of en-\ntanglement, which can be viewed as a resource for such\nnon-local computations. More recent work has looked at\nhow to partition the computations of distributed quantum\ncircuits over multiple QPUs, e.g., [3] as we mentioned earlier\n- with considerations including distributing computations\nin such a way as to optimize performance and to reduce the\nrequirements on entanglement, since if the entanglements\nrequired are generated at too low a rate, this will hold up\ncomputations. The key motivation here is to inter-link a\nset of quantum computers to form effectively a much more\npowerful quantum computer.\n3\nQUANTUM CLOUD COMPUTING AND DELEGAT-\nING QUANTUM COMPUTATIONS\nWe have seen big tech companies and startups offering\nquantum computing as a service similar to accessing other\ncloud service offerings, which is a fantastic resource for\nexperimentation and studies.\nMore formally, studies into delegating quantum com-\nputation from a client (which can be either classical, or\nalmost classical, i.e., with minimal capability to perform\n3. For example, see https://www.ietf.org/archive/id/draft-irtf-qirg-principles-10.html\n[last accessed: 1/8/2022]\n4. For example, see Buhrman and R¨ohrig’s paper dating back to\n2003: https://link.springer.com/chapter/10.1007/978-3-540-45138-9 1\n[last accessed: 1/8/2022]\n5. For example, see the work by Yimsiriwattana and Lomonaco\nJr.\nin\nhttps://arxiv.org/pdf/quant-ph/0402148.pdf\nand\na\ndistributed\nversion\nof\nShor’s\nfamous\nfactorization\nalgorithm\nhttps://arxiv.org/abs/2207.05976 [last accessed: 1/8/2022]\ncomputations such as teleporting qubits, applying simple\nPauli quantum operations, and doing basic measurements)\nwhich is much more restricted than the server (assumed\nto be a universal quantum computer) have been studied,\ncalled delegated quantum computing. And when the server\nis prevented from knowing the client’s inputs but still can\nperform delegated computations, by a technique such as\nthe quantum one-time pad (where the client applies Pauli\noperations to add uncertainty from the server’s perspective,\nthereby effectively encrypting the quantum inputs it sends\nto the server, and keeps track of operations it later needs\nto decrypt the outputs from the server), this is called blind\nquantum computing.\nIn order to be sure that the server does indeed perform\nthe required quantum operations delegated to it by the\nclient, the client can embed tests (or test runs) into the\ndelegated computations, so that the server (not being able\nto distinguish between tests and normal computations) can\nbe caught out if it did not perform the required compu-\ntations properly. That is, the client can verify if the server\nperformed the required quantum computations.6 Further\nabstractions for delegating quantum computations with\nsupporting cloud services continues to be investigated.\n4\nTHE QUANTUM INTERNET\nAs we mentioned earlier, work on the quantum Internet\nfocuses on how to efﬁciently enable robust entanglement\nshared among qubits over long geographical distances. If\ntwo nodes in different continents share entangled states,\nthen, this can be a resource to do non-local gates, i.e.,\nto perform distributed quantum computations, and enable\nquantum protocols over Internet-scale distances.\nThere have been the use of satellites to enable long dis-\ntance entanglement, as well as the use of optical ﬁbre cables\nto demonstrate entanglement. Key to the quantum Internet\nare ideas such as entanglement swapping and quantum\nrepeaters, including ideas such quantum distillation, to\nachieve high ﬁdelity distributed entangled states over long\ndistances, and quantum error correction - this continues to\nbe a research endeavour as mentioned earlier [2].\nThere are other interesting distributed quantum appli-\ncations to be considered including quantum cryptography,\nquantum sensing, and quantum positioning systems.\n5\nDISTRIBUTED QUANTUM COMPUTING OVER THE\nQUANTUM INTERNET: QUANTUM INTERNET COM-\nPUTING AND THE QUANTUM IOT?\nApart from the many quantum computers available over\nthe cloud by big tech and startups which work at very\nlow temperatures, room temperature quantum computers\nhave also started to emerge.7 This could pave the way\nfor quantum computers at the fog and at the edge, not\njust in remote clouds, and perhaps even mobile quantum\n6. An\nexcellent\nexample\nis\nthe\nwork\nby\nBroadbent\nat\nhttps://theoryofcomputing.org/articles/v014a011/\n[last\naccessed:\n1/8/2022]\n7. See https://spectrum.ieee.org/nitrogen-vacancy-diamond-quantum-computer-\nand also https://otd.harvard.edu/explore-innovation/technologies/scalable-room-t\n[last accessed: 1/8/2022]\nIEEE IOT MAGAZINE, VOL. XX, NO. X, X 2022\n3\ncomputers, or quantum computers embedded into every-\nday devices and objects, if ever! Will we then have the\nquantum Internet of Things (IoT)? The answer remains to\nbe seen, and “quantum entangled things across the world”\nwill likely complement the classical IoT. Future applications\nand potential of quantum Internet computing remains to\nbe investigated. Meanwhile, others have begun to look at\nthe connection between 6G networking and the quantum\nInternet [5].\nREFERENCES\n[1] W.\nKozlowski\nand\nS.\nWehner,\n“Towards\nlarge-scale\nq\n\n---\n\nIndex: 2\nTitle: Unconventional Quantum Computing Devices\nPublished: 2000-03-31\nAuthors: Seth Lloyd\nSource: Arxiv research paper\nContent: arXiv:quant-ph/0003151v1  31 Mar 2000\nUnconventional Quantum Computing Devices\nSeth Lloyd\nMechanical Engineering\nMIT 3-160\nCambridge, Mass. 02139\nAbstract: This paper investigates a variety of unconventional quantum computation de-\nvices, including fermionic quantum computers and computers that exploit nonlinear quan-\ntum mechanics. It is shown that unconventional quantum computing devices can in prin-\nciple compute some quantities more rapidly than ‘conventional’ quantum computers.\nComputers are physical: what they can and cannot do is determined by the laws\nof physics. When scientiﬁc progress augments or revises those laws, our picture of what\ncomputers can do changes. Currently, quantum mechanics is generally accepted as the\nfundamental dynamical theory of how physical systems behave. Quantum computers can\nin principle exploit quantum coherence to perform computational tasks that classical com-\nputers cannot [1-21]. If someday quantum mechanics should turn out to be incomplete\nor faulty, then our picture of what computers can do will change. In addition, the set\nof known quantum phenomena is constantly increasing: essentially any coherent quantum\nphenomenon involving nonlinear interactions between quantum degrees of freedom can\nin principle be exploited to perform quantum logic. This paper discusses how the revi-\nsion of fundamental laws and the discovery of new quantum phenomena can lead to new\ntechnologies and algorithms for quantum computers.\nSince new quantum eﬀects are discovered seemingly every day, let’s ﬁrst discuss two\nbasic tests that a phenomenon must pass to be able to function as a basis for quantum\ncomputation. These are 1) The phenomenon must be nonlinear, and 2) It must be coherent.\nTo support quantum logic, the phenomenon must involve some form of nonlinearity, e.g.,\na nonlinear interaction between quantum degrees of freedom. Without such a nonlinearity\nquantum devices, like linear classical devices, cannot perform even so simple a nonlinear\noperation as an AND gate.\nQuantum coherence is a prerequisite for performing tasks\nsuch as factoring using Shor’s algorithm [10], quantum simulation a la Feynman [11] and\nLloyd [12], or Grover’s data-base search algorithm [13], all of which require extended\nmanipulations of coherent quantum superpositions.\n1\nThe requirements of nonlinearity and coherence are not only necessary for a phe-\nnomenon to support quantum computation, they are also in principle suﬃcient. As shown\nin [14-15], essentially any nonlinear interaction between quantum degrees of freedom suf-\nﬁces to construct universal quantum logic gates that can be assembled into a quantum\ncomputer. In addition, the work of Preskill et al. [18] on robust quantum computation\nshows that an error rate of no more than 10−4 per quantum logic operation allows one to\nperform arbitrarily long quantum computations in principle.\nIn practice, of course, few if any quantum phenomena are likely to prove suﬃciently\ncontrollable to provide extended quantum computation. Promising devices under current\nexperimental investigation include ion traps [5,7], high ﬁnesse cavities for manipulating\nlight and atoms using quantum electrodynamics [6], and molecular systems that can be\nmade to compute using nuclear magnetic resonance [8-9]. Such devices store quantum\ninformation on the states of quantum systems such as photons, atoms, or nuclei, and\naccomplish quantum logic by manipulating the interactions between the systems via the\napplication of semiclassical potentials such as microwave or laser ﬁelds. We will call such\ndevices ‘conventional’ quantum computers, if only because such devices have actually been\nconstructed.\nThere is another sense in which such computers are conventional: although the de-\nvices described above have already been used to explore new regimes in physics and to\ncreate and investigate the properties of new and exotic quantum states of matter, they\nfunction according to well established and well understood laws of physics. Perhaps the\nmost striking examples of the ‘conventionality’ of current quantum logic devices are NMR\nquantum microprocessors that are operated using techniques that have been reﬁned for\nalmost half a century. Ion-trap and quantum electrodynamic quantum computers, though\ncertainly cutting edge devices, operate in a quantum electrodynamic regime where the\nfundamental physics has been understood for decades (that is not to say that new and\nunexpected physics does not arise frequently in this regime, rather that there is general\nagreement on how to model the dynamics of such devices).\nMake no mistake about it: a conventional quantum logic device is the best kind of\nquantum logic device to have around. It is exactly because the physics of nuclear magnetic\nresonance and quantum electrodynamics are well understood that devices based on this\nphysics can be used systematically to construct and manipulate the exotic quantum states\nthat form the basis for quantum computation.\nWith that recognition, let us turn to\n2\n‘unconventional’ quantum computers.\nPerhaps the most obvious basis for an unconventional quantum computer is the use\nof particles with non-Boltzmann statistics in a reﬁme where these statistics play a key role\nin the dynamics of the device. For example, Lloyd [16] has proposed the use of fermions\nas the fundamental carriers of quantum information, so that a site or state occupied by a\nfermion represents a 1 and an unoccupied site or state represents a 0. It is straightforward\nto design a universal quantum computer using a conditional hopping dynamics on an array\nof sites, in which a fermion hops from one site to another if only if other sites are occupied.\nIf the array is one-dimensional, then such a fermionic quantum computer is equivalent\nto a conventional quantum computer via the well-known technique of bosonization. If the\narray is two or more dimensional, however, a local operation involving fermions on the\nlattice cannot be mocked up by a local operation on a conventional quantum computer,\nwhich must explicitly keep track of the phases induced by Fermi statistics. As a result,\nsuch a fermionic computer can perform certain operations more rapidly than a conventional\nquantum computer. An obvious example of a problem that can be solved more rapidly on\na fermionic quantum computer is the problem of simulating a lattice fermionic system in\ntwo or more dimensions. To get the antisymmetrization right in second quantized form,\na conventional ‘Boltzmann’ quantum computer takes time proportional to Tℓd−1 where T\nis the time over which the simulation is to take place, ℓis the length of the lattice and\nd is the dimension, while a fermionic quantum computer takes time proportional to T.\n(Here we assume that the computations for both conventional and Fermionic quantum\ncomputers can take advantage of the intrinsic parallelizability of such simulations: if the\ncomputations are performed serially an additional factro of ℓd is required for both types\nof computer to update each site sequentially.)\nAs the lattice size ℓand the dimension d grow large, the diﬀerence between the two\ntypes of computer also grows large. Indeed, the problem of simulating fermions hopping\non a hypercube of dimension d as d →∞is evidently exponentially harder on a con-\nventional quantum computer than a Fermionic quantum computer.\nSince a variety of\ndiﬃcult problems such as the travelling-salesman problem and data-base search problem\ncan be mapped to particles hopping on a hypercube, it is interesting to speculate whether\nfermionic computers might provide an exponential speed-up on problems of interest in ad-\ndition to quantum simulation. No such problems are currently known, however. Fermionic\ncomputers could be realized in principle by manipulating the ways in which electrons and\n3\nholes hop from site to site on a semiconductor lattice (though problems of decoherence are\nlikely to be relatively severe for such systems).\nIt might also be possible to construct bosonic computers using photons, phonons, or\natoms in a Bose-Einstein condensate. Such systems can be highly coherent and support\nnonlinear interactions: phonons and photons can interact in a nonlinear fshion via their\ncommon nonlinear interaction with matter, and atoms in a Bose condensate can be made\nto interact bia quantum electrodynamics (by introduction of a cavity) or by collisions. So\nfar, however, the feature of Bose condensates that makes them so interesting from the point\nof view of physics — all particles in the same state — makes them less interesting from the\npoint of view of quantum computation. Many particles in the same state, which can be\nmanipulated coherently by a variety of techniques, explore the same volume of Hilbert space\nas a single particle in that state. As a result, it is unclear how such a bosonic system could\nprovide a speed-up over conventional quantum computation. More promising than Bose\ncondensates from the perspective of quantum computation and quantum communications,\nis the use of cavity quantum electrodynamics to ‘dial up’ or synthesize arbitrary states\nof the cavity ﬁeld. Such a use of bosonic states is important for the ﬁeld of quantum\ncommunications, which requires the ability to create and manipulate entangled states of\nthe electromagnetic ﬁeld.\nA third unconventional design for a quantum computer relies on ‘exotic’ statistics\nthat are neither fermionic nor bosonic. Kitaev has recently proposed a quantum computer\narchitecture based on ‘anyons,’ particles that when exchanged acquuire an arbitrary phase.\nExamples of anyons include two-dimensional topological defects in lattice systems of spins\nwith various symmetries. Kitaev noted that such anyons could perform quantum logic via\nAharonov-Bohm type interactions [19]. Preskill et al. have shown explicitly how anyonic\nsystems could compute in principle [20], and Lloyd et al.\nhave proposed methods of\nrealizing anyons using superconducting circuits (they could also in principle be constructed\nusing NMR quantum computers to mock up the anyonic dynamics in an eﬀectively two-\ndimensional space of spins) [21]. The advantage of using anyons for quantum computation\nis that their nonlocal topological nature can make them intrinsically error-correcting and\nvirtually immune to the eﬀects of noise and interference.\nAs the technologies of the microscale become better developed, more and more po-\ntential designs for quantum computers, both conventional and unconventional, are likely\nto arise. Additional technologies that could prove useful for the construction of quantum\n4\nlogic devices include photonic crystals, optical hole-burning techniques, electron spin res-\nonance, quantum dots, superconducting circuits in the quantum regime, etc. Since every\nquantum degree of freedom can in principle participate in a computation one cannot a\npriori rule out the possibility of using currently hard to control degrees of freedom such as\nquark and gluon in complex nuclei to process information. Needless to say, most if not all\nof the designs inspired by these technologies are likely to fail. There is room for optimism\nthat some such quantum computer designs will prove practicable, however.\nThe preceding unconventional designs for quantum computers were based on existing,\nexperimentally conﬁrmed physical phenomena (except in the case of non-abelian anyons).\nLet us now turn to designs based on speculative, hypothetical, and not yet veriﬁed phenom-\nena. (One of the most interesting of these phenomena is large-scale quantum computation\nitself: can we create and systematically transform entangled states involving hundreds or\nthousands of quantum variables?) A particularly powerful hypothesis from the point of\nview of quantum computation is that of nonlinear quantum mechanics.\nThe conventional picture of quantum mechanics is that it is linear in the sense that the\nsuperposition principle is obeyed exactly. (Of course, quantum systems can still exhibit\nnonlinear interaction\n\n---\n\nIndex: 3\nTitle: Geometrical perspective on quantum states and quantum computation\nPublished: 2013-11-20\nAuthors: Zeqian Chen\nSource: Arxiv research paper\nContent: arXiv:1311.4939v1  [quant-ph]  20 Nov 2013\nGeometrical perspective on quantum states and quantum computation\nZeqian Chen\nState Key Laboratory of Resonances and Atomic and Molecular Physics,\nWuhan Institute of Physics and Mathematics, Chinese Academy of Sciences,\n30 West District, Xiao-Hong-Shan, Wuhan 430071, China\nWe interpret quantum computing as a geometric evolution process by reformulating ﬁnite quantum\nsystems via Connes’ noncommutative geometry. In this formulation, quantum states are represented\nas noncommutative connections, while gauge transformations on the connections play a role of\nunitary quantum operations. Thereby, a geometrical model for quantum computation is presented,\nwhich is equivalent to the quantum circuit model. This result shows a geometric way of realizing\nquantum computing and as such, provides an alternative proposal of building a quantum computer.\nPACS numbers: 03.67.Lx, 03.65.Aa\nQuantum computation has the advantage of solving\neﬃciently some problems that are considered intractable\nby using conventional classical computation [1]. In this\ncontext, there are two remarkable algorithms found:\nShor’s factoring algorithm [2] and Grove’s search algo-\nrithm [3].\nBut it remains a challenge to ﬁnd eﬃcient\nquantum circuits that can perform these complicated\ntasks in practice, due to quantum decoherence. A cru-\ncial step in the theory of quantum computer has been\nthe discovery of error-correcting quantum codes [4] and\nfault-tolerant quantum computation [5, 6], which estab-\nlished a threshold theorem that proves that quantum de-\ncoherence can be corrected as long as the decoherence is\nsuﬃciently weak. To tackle this barrier, a revolutionary\nstrategy, topological quantum computation (see [7] and\nreferences therein), is to make the system immune to the\nusual sources of quantum decoherence, by involving the\nglobally robust topological nature of the computation.\nRecently, substantial progress in this ﬁeld has been made\non both theoretical and experimental fronts [8].\nIn this paper, we provide an alternative approach to\nquantum computation from a geometrical view of point.\nTo this end, we need to reformulate quantum mechanics\nvia Connes’ noncommutative geometry [9]. In this for-\nmulation, quantum states are represented as noncommu-\ntative connections, while gauge transformations on the\nconnections play a role of unitary quantum operations.\nIn this way, we present a geometrical model for quan-\ntum computation, which is equivalent to the quantum\ncircuit model. In this computational model, information\nis encoded in gauge states instead of quantum states and\nimplementing on gauge states is played by gauge transfor-\nmations. Therefore, our scheme shows a geometric way\nof realizing quantum computing and as such, provides an\nalternative proposal of building a quantum computer.\nLet H be a N dimensional Hilbert space associated\nwith a ﬁnite quantum system. Let A be the algebra of\nall (bounded) linear operators on H, and let U(A) = {u ∈\nA : uu∗= u∗u = I} with I being the unit operator on\nH. Given a selfadjoint operator D on H, (A, H, D) is a\nspectral triple in the sense of noncommutative geometry\n[9, 10]. A (noncommutative) connection on (A, H, D) is\ndeﬁned to be a selfadjoint operator V on H of the form\nthat follows\nV =\nX\nj\naj[D, bj]\n(1)\nwhere aj, bj ∈A and [a, b] = ab −ba. A gauge transform\non a connection V under u ∈U(A) is deﬁned as\nV 7−→Gu(V ) = uV u∗+ u[D, u∗].\n(2)\nFor avoiding triviality, we always assume that D ̸= 0 or\nI in what follows.\nFor any (pure) quantum state |ψ⟩⟨ψ| with ψ being a\nunit vector in H, we have\n|ψ⟩⟨ψ| = |ψ⟩⟨ϕ|i[D, b]|ϕ⟩⟨ψ|\nwhere i = √−1 and, b is a selfjoint operator on H such\nthat i[D, b] has eigenvalue 1 at |ϕ⟩. Such a selfjoint oper-\nator b always exists because D ̸= 0 or I. In this case,\n|ψ⟩⟨ψ| = ia∗[D, ba] −ia∗b[D, a]\n(3)\nwith a = |ϕ⟩⟨ψ|. Thus, every quantum state |ψ⟩⟨ψ| can\nbe represented as a connection, denoted by Vψ, i.e.,\nVψ = ia∗[D, ba] −ia∗b[D, a].\n(4)\nLet GD(H) be the set of all connections V which can\nbe written as V = Vψ + uDu∗−D with ψ being a unit\nvector in H and u ∈U(A). An element in GD(H) is said\nto be a gauge state on (A, H, D). Any quantum state is\nnecessarily a gauge state, but a gauge state need not to\nbe a quantum state. However, any gauge state V can be\nobtained from a quantum state by performing a gauge\ntransform. Indeed, if V = Vψ + uDu∗−D then V =\nGu(Vu∗ψ). Moreover, for any gauge state V on (A, H, D)\nwe have (see [11])\n• for any u ∈U(A), Gu(V ) is again a gauge state;\n• Guv(V ) = Gu(Gv(V )) for all u, v ∈U(A).\nTherefore, a gauge transform preserves gauge states.\nLet V be a gauge state which is prepared from a quan-\ntum state |ψ⟩⟨ψ| by operating a gauge transform Gu, i.e.,\n2\nV = Gu(Vψ). For any event E, the probability of E oc-\ncurring on V is\n⟨E⟩V = ⟨ψ|u∗Eu|ψ⟩.\n(5)\nNote that a gauge state may be prepared in several ways.\nHence, the probability of a event E occurring on a gauge\nstate V depends on the quantum state from which V is\nprepared.\nLet H be a selfadjoint operator on H. Assuming ut =\neitH for t ∈R, we have that the gauge transforms Vt =\nGt(V ) on a ﬁxed gauge state V under ut form a group\n(see [11]), that is,\nGt+s(V ) = Gt(Gs(V )).\n(6)\nThis yields a dynamical equation governed by the Hamil-\ntonian H for gauge states on (A, H, D) as follows [12]\nidVt\ndt = [Vt, H] + [D, H]\n(7)\nwith V0 = V. In particular, for a unit vector ψ we have\nVt = Gt(Vψ) = Vutψ + utDu∗\nt −D.\n(8)\nWe now turn to product of two spectral triples. Sup-\npose (Ai, Hi, Di), i = 1, 2, are two spectral triple associ-\nated with ﬁnite quantum systems. Put\nD = D1 ⊗I2 + I1 ⊗D2\n(9)\nwith Ii being the unit operator on Hi (i = 1, 2). Then\nD is a selfjoint operator on H1 ⊗H2. The spectral triple\n(A1⊗A2, H1⊗H2, D) is called the product of two spectral\ntriples (Ai, Hi, Di), i = 1, 2.\nNow we illustrate our scheme by using a qubit. Let\nH = C2 and\nσx =\n\u0014\n0 1\n1 0\n\u0015\n, σy =\n\u0014\n0 −i\ni\n0\n\u0015\n, σz =\n\u0014\n1\n0\n0 −1\n\u0015\n.\n(10)\nThen (M2, C2, D) is a spectral triple with D = σx, where\nM2 is the set of all 2×2 complex matrices. For |0⟩=\n\u0014\n1\n0\n\u0015\n,\nwe have\nV|0⟩=\n\u0014\n1 0\n0 0\n\u0015\n,\nGσx(V|0⟩) =\n\u0014\n0 0\n0 1\n\u0015\n,\nand\nGσy(V|0⟩) =\n\u0014\n0\n−2\n−2\n1\n\u0015\n,\nGσz(V|0⟩) =\n\u0014\n1\n−2\n−2\n0\n\u0015\n.\nFor |1⟩=\n\u0014\n0\n1\n\u0015\n, we have\nV|1⟩=\n\u0014\n0 0\n0 1\n\u0015\nand\nGσy(V|1⟩) =\n\u0014\n1\n−2\n−2\n0\n\u0015\n.\nHence Gσy(V|1⟩) = Gσz(V|0⟩) and so, the gauge state\nV =\n\u0014\n1\n−2\n−2\n0\n\u0015\ncan be prepared in two diﬀerent ways.\nWe are now ready to interpret quantum computation\nfrom a geometrical view of point.\nBut let us take a\nstep backward and discuss the standard quantum circuit\nmodel for computation [13]. Let H = (C2)⊗n, the tensor\nproduct of n copies of C2. A quantum circuit model on\nn qubits consists of\n• a initial state |ψ⟩, represented by a unit vector ψ ∈\nH;\n• a quantum circuit Γ = UNUN−1 · · · U1, where quan-\ntum “gates” Uk 1 ≤k ≤N, are unitary transfor-\nmations on either C2\ni or C2\ni ⊗C2\nj, 1 ≤i, j ≤n, the\nidentity on all remaining factors;\n• reading the output of the circuit Γ|ψ⟩by measuring\nthe ﬁrst qubit; the probability of observing |1⟩is\nP(Γ) = ⟨ψ|Γ∗Π1Γ|ψ⟩, where Π1 = |1⟩⟨1| ⊗I · · · ⊗I\nis the projection to |1⟩in the ﬁrst qubit.\nLet A = M2n. Put\nD =\nn\nX\ni=1\nI ⊗· · · ⊗I\n|\n{z\n}\ni−1\n⊗σx ⊗I · · · ⊗I\nwhere I is the identity on C2. A computational model\nbased on the spectral triple (A, H, D) is as follows:\n• Initialization of a gauge state Vψ in the spectral\ntriple (A, H, D), where ψ is a unit vector in H;\n• Gauge implementation of the computational pro-\ngram\nG(Γ) = GUN GUN−1 · · · GU1\nwhere “gates” GUk, 1 ≤k ≤N, are gauge transfor-\nmations induced by Uk;\n• Application of the projection operator Π1 for read-\ning the output of the computation G(Γ)(Vψ);\nthe probability of observing |1⟩is P(GΓ)\n=\n⟨ψ|Γ∗Π1Γ|ψ⟩because G(Γ)(Vψ) = GΓ(Vψ) (see\n[11]), i.e., G(Γ)(Vψ) = Γ|ψ⟩⟨ψ|Γ∗+ ΓDΓ∗−D.\nThus, we obtain a geometrical model on n qubits for\nquantum computation, which is evidently equivalent to\nthe quantum circuit model as described above. Due to\nthe essential role of gauge transformations played in this\ncomputational model, we call this scheme gauge quantum\ncomputation.\nAs illustration, we give the Deutsch-Jozsa algorithm\n[14] in gauge quantum computation. Let f : {0, 1}n 7→\n{0, 1} be a function that takes an n-bit into a bit. We\ncall f balanced if f(x) = 1 for exactly half of all possible\n3\nx and f(x) = 0 for the other half.\nGiven a function\nf that is either constant or balanced, we want to ﬁnd\nout which it is with certainty. More precisely, we select\none x ∈{0, 1}n and calculate f(x) with the result being\neither 0 or 1. What is the fewest number of queries that\nwe can make to determine whether or not f is constant?\nIn the classical case, at worst we will need to calculate f\n2n−1 + 1 times, because we may ﬁrst obtain 2n−1 zeros\nand will need one more query to decide.\nHowever, in\nthe setting of quantum computation we could achieve the\ngoal in just one query using the Deutsch-Jozsa algorithm.\nIn the sequel, we give a realization of the Deutsch-Jozsa\nalgorithm in gauge quantum computation.\nLet H = (C2)⊗(n+1) and A = M2n+1. Given a selfad-\njoint operator D on H that is not 0 or I, we get the desired\nspectral triple (A, H, D). For a given f, we deﬁne the as-\nsociated operator Uf on H as Uf|x, y⟩= |x, y ⊕f(x)⟩for\nx ∈{0, 1}n and y ∈{0, 1}. Recall that the Hadamard\noperator H on C2 is\nH =\n1\n√\n2\nX\nx,y∈{0,1}\n(−1)x·y|x⟩⟨y|\nwhere x·y signiﬁes ordinary multiplication. The following\nis the Deutsch-Jozsa algorithm in the setting of gauge\nquantum computation:\n• Initialization of a gauge state Vψ with ψ = |0⟩⊗n ⊗\n|1⟩;\n• Gauge implementation of the computational pro-\ngram G(Γ) = GH⊗n⊗IGUf GH⊗(n+1);\n• Application of the projection operator Π|0⟩⊗n for\nreading the output of the computation G(Γ)(Vψ),\nwhere Π|0⟩⊗n is the projection to |0⟩⊗n in the ﬁrst\nn qubits.\nThe ﬁnal gauge state is V = VΓψ + ΓDΓ∗−D with Γ =\n(H⊗n ⊗I)UfH⊗(n+1), where\nΓψ =\nX\nx,y∈{0,1}n\n(−1)x·y+f(x)\n2n\n|y⟩⊗|0⟩−|1⟩\n√\n2\n.\nSince the amplitude for the state |0⟩⊗n in the ﬁrst n\nqubits is P\nx(−1)f(x)/2n, the probability of observing 0\nis 1 if f is constant, or 0 if f is balanced. Thus we have\ntwo possibilities of obtaining the outcome zero or the\noutcome nonzero. In the ﬁrst case, f is certainly constant\nand in the second case f must be balanced. Therefore,\nwe only need to perform three times gauge transforms for\ndetermining whether or not f is constant.\nIn conclusion, we present a geometrical description of\nquantum computation via noncommutative geometry. In\nthis geometrical model, information is encoded in gauge\nstates and computational operation is implemented by\ngauge transforms instead of unitary transforms. In prin-\nciple, gauge transforms are easier to perform than uni-\ntary quantum operation [15]. Therefore, gauge quantum\ncomputation should be more accessible than the usual\nquantum circuit computation and as such, this provides\nan alternative proposal of building a quantum computer.\nThis work was supported in part by the NSFC under\nGrant No. 11171338 and National Basic Research Pro-\ngram of China under Grant No. 2012CB922102.\n[1] M. A. Nielsen, I. L. Chuang, Quantum Computation\nand Quantum Information (Cambridge University Press,\nCambridge, 2000).\n[2] P. Shor, Algorithms for quantum computation, discrete\nlogarithms and factoring, Proc. 35th Annual Symposium\non Foundations of Computer Science (IEEE Computer\nSociety Press, Los Alamitos, CA, 1994, 124-134).\n[3] L. Grover, Quantum mechanics helps in search for a nee-\ndle in a haystack, Phys. Rev. Lett. 79 (1997), 325-328.\n[4] P. Shor, Scheme for reducing decoherence in quantum\ncomputer memory, Phys. Rev. A 52 (1995), 2493-2496.\n[5] J. Preskill, Fault-tolerant quantum computation, arXiv:\nquant-ph/9712048, 1997.\n[6] P. Shor, Fault-tolerant quantum computation, Proc. 37th\nAnnual Symposium on Foundations of Computer Science\n(IEEE Computer Society Press, Los Alamitos, CA, 1996,\n56-65).\n[7] C. Nayak, S. H. Simon, A. Stern, M. Freedman, S. Das\nSarma, Non-Abelian anyons and topological quantum\ncomputation, Rev. Mod. Phys. 80 (2008), 1083-1159.\n[8] A.\nStern,\nN.\nH.\nLindner,\nTopological\n\n---\n",
  "knowledge": {
    "topic": "Quantum Computing",
    "sources": [
      {
        "id": 1,
        "title": "Quantum computing",
        "authors": [
          "Wikipedia contributors"
        ],
        "source": "Wikipedia",
        "url": "https://en.wikipedia.org/wiki/Quantum_computing"
      },
      {
        "id": 2,
        "title": "Timeline of quantum computing and communication",
        "authors": [
          "Wikipedia contributors"
        ],
        "source": "Wikipedia",
        "url": "https://en.wikipedia.org/wiki/Timeline_of_quantum_computing_and_communication"
      },
      {
        "id": 3,
        "title": "Superconducting quantum computing",
        "authors": [
          "Wikipedia contributors"
        ],
        "source": "Wikipedia",
        "url": "https://en.wikipedia.org/wiki/Superconducting_quantum_computing"
      },
      {
        "id": 4,
        "title": "The Rise of Quantum Internet Computing",
        "authors": [
          "Seng W. Loke"
        ],
        "source": "arXiv",
        "url": "https://arxiv.org/abs/2208.00733"
      },
      {
        "id": 5,
        "title": "Unconventional Quantum Computing Devices",
        "authors": [
          "Seth Lloyd"
        ],
        "source": "arXiv",
        "url": "https://arxiv.org/abs/quant-ph/0003151"
      },
      {
        "id": 6,
        "title": "Geometrical perspective on quantum states and quantum computation",
        "authors": [
          "Zeqian Chen"
        ],
        "source": "arXiv",
        "url": "https://arxiv.org/abs/1311.4939"
      }
    ],
    "topics": [
      {
        "id": "t1",
        "title": "Fundamental concepts",
        "summary_points": [
          "Quantum computing exploits quantum-mechanical phenomena—superposition, entanglement, and probabilistic measurement outcomes—to process information in ways not available to classical machines [1].",
          "The basic information unit is the qubit: a two-level quantum system state |ψ⟩ = α|0⟩ + β|1⟩ with complex amplitudes α, β and normalization |α|^2 + |β|^2 = 1; measurement yields classical outcomes with probabilities |α|^2 and |β|^2 (Born rule) [1].",
          "Quantum gates are unitary operations acting on qubits; algorithms engineer interference to amplify desired measurement outcomes. Quantum circuits are modeled via linear algebra (vectors for states, matrices for operations) [1,6].",
          "Quantum computation can be seen as sampling or evolving a high-dimensional quantum state; classical simulation typically incurs exponential overhead in general, motivating quantum hardware for specific speedups [1,5].",
          "Key theoretical constraints and resources include no-cloning and no-deleting theorems, requirement of coherence, and need for nonlinear interactions (for universality) together with controllable quantum dynamics [1,5]."
        ],
        "subtopics": [
          {
            "id": "t1.1",
            "title": "Qubits, superposition, and measurement",
            "summary_points": [
              "A qubit is a vector in a 2D Hilbert space expressed in Dirac notation; superposition allows simultaneous representation of basis states and enables interference-based algorithms [1].",
              "Measurement collapses the state probabilistically (Born rule); amplitudes (complex numbers) allow constructive and destructive interference—central to quantum algorithm design [1]."
            ],
            "references": [
              1
            ]
          },
          {
            "id": "t1.2",
            "title": "Entanglement and nonlocality",
            "summary_points": [
              "Entanglement creates correlations between qubits that cannot be explained classically and serves as a resource for protocols including teleportation, non-local gates, quantum cryptography, and distributed computing [1,4].",
              "The quantum Internet and distributed protocols rely on establishing high-fidelity entanglement across nodes for computation and cryptographic tasks [4]."
            ],
            "references": [
              1,
              4
            ]
          }
        ],
        "references": [
          1,
          5
        ]
      },
      {
        "id": "t2",
        "title": "History and milestones",
        "summary_points": [
          "Quantum computing emerged from converging advances in quantum physics and computer science (Benioff's quantum Turing machine, Feynman's quantum simulation idea), with early formal and experimental work through the 1980s and 1990s [2].",
          "Foundational theoretical milestones: Benioff (1980), Feynman (1981/82) on quantum simulation, Deutsch (1985) universal quantum computer, no-cloning theorem rediscovered (1982), Bennett & Brassard (1984) quantum key distribution, Shor (1994) factoring algorithm, Grover (1996) search algorithm, and Lloyd (1996) on quantum simulation efficiency [2,1].",
          "Experimental progress increased qubit counts and gate demonstrations (1998 NMR two- and three-qubit experiments, trapped ions, superconducting qubits). In 2019 Google reported quantum supremacy with a 53/54-qubit device; the claim provoked debate (IBM rebuttal) about classical simulability and the supremacy threshold [2,1]."
        ],
        "subtopics": [
          {
            "id": "t2.1",
            "title": "Decadal timeline highlights",
            "summary_points": [
              "1960s–1970s: foundational theory precursors (conjugate coding, Holevo bound, reversible computation).",
              "1980s: quantum Turing machine (Benioff), Feynman's proposal for quantum simulators, Deutsch's universal quantum computer and early ideas of quantum error correction.",
              "1990s: Simon's oracle separation (1993), Shor's algorithm (1994), experimental two-qubit gates (1995–1998), Grover's algorithm (1996) and early proposals for solid-state and topological implementations.",
              "2000s–2010s: scaling experiments, DiVincenzo criteria formalized, demonstrations of various qubit platforms; 2019 claimed quantum supremacy milestone."
            ],
            "references": [
              2,
              1
            ]
          }
        ],
        "references": [
          2,
          1
        ]
      },
      {
        "id": "t3",
        "title": "Algorithms, complexity, and applications",
        "summary_points": [
          "Quantum algorithms exploit superposition and interference to achieve asymptotic speedups for specific problems; prominent algorithms include Shor's factoring (exponential speedup for integer factorization and discrete log) and Grover's search (quadratic speedup for unstructured search) [1].",
          "Oracle and promise algorithms (Deutsch-Jozsa, Bernstein–Vazirani, Simon) provided early formal separations between quantum and classical query complexity, laying groundwork for later practical algorithms [2].",
          "Quantum computers are expected to offer advantages in cryptanalysis (breaking RSA/DH if sufficiently large and low-error machines exist), quantum simulation (efficient simulation of quantum many-body systems), optimization heuristics (quantum annealing), and specialized sampling tasks used to demonstrate quantum advantage/ supremacy [1,5].",
          "Complexity class BQP captures problems efficiently solvable on quantum computers; some subclasses of quantum computations (e.g., stabilizer circuits) can be classically simulated efficiently (Gottesman–Knill theorem) [2]."
        ],
        "subtopics": [
          {
            "id": "t3.1",
            "title": "Cryptography implications",
            "summary_points": [
              "Shor's algorithm threatens widely used public-key cryptosystems (RSA, Diffie–Hellman) by enabling polynomial-time factoring and discrete logarithm computation on an ideal quantum computer; this motivates post-quantum cryptography research [1].",
              "Quantum key distribution (Bennett–Brassard) uses quantum properties to provide information-theoretic secure key exchange rather than computational assumptions [2]."
            ],
            "references": [
              1,
              2
            ]
          },
          {
            "id": "t3.2",
            "title": "Quantum simulation and optimization",
            "summary_points": [
              "Quantum computers can efficiently simulate quantum systems (Feynman conjecture proven by Lloyd), enabling potential advances in chemistry and materials science [2,5].",
              "Quantum annealing and adiabatic methods seek to exploit tunneling and quantum fluctuations to escape local minima; evidence shows advantages in specific regimes but results are problem-dependent [2,5]."
            ],
            "references": [
              2,
              5
            ]
          }
        ],
        "references": [
          1,
          2,
          5
        ]
      },
      {
        "id": "t4",
        "title": "Physical implementations and engineering",
        "summary_points": [
          "Multiple hardware paradigms exist: superconducting circuits, trapped ions, photonics, NMR, spin qubits in semiconductors, topological anyonic systems, and experimental room-temperature platforms (e.g., NV centers) are under active development; each trades off coherence, controllability, connectivity, and scalability [1,3,5].",
          "Superconducting qubits are a leading solid-state approach: they use macroscopic Josephson-junction-based circuits acting as artificial atoms (nonlinear inductance provides anharmonicity), are manufactured by lithography, cooled in dilution refrigerators, and controlled with microwave electronics; variants include charge, flux, and phase qubits and hybrids such as transmon, Xmon, fluxonium [3].",
          "Engineering challenges include decoherence (coupling to environment), gate fidelity, crosstalk, scalable interconnects, and cryogenic control. Manufacturing advances (materials, fabrication, packaging) continue to improve coherence times and error rates [3]."
        ],
        "subtopics": [
          {
            "id": "t4.1",
            "title": "Superconducting quantum computing details",
            "summary_points": [
              "Superconducting qubits map logical states to ground and excited energy levels of engineered circuits; Josephson junctions introduce necessary nonlinearity to separate energy levels and enable two-level addressing [3].",
              "Design and analysis use circuit quantization (condensate wave function, Lagrangian → Hamiltonian) to obtain the quantum description; Josephson energy vs. charging energy ratio determines archetype (charge/flux/phase) [3].",
              "Devices typically operate at millikelvin temperatures; fabrication relies on integrated-circuit techniques enabling micrometer-scale features and Josephson junction formation via controlled oxidation and shadow evaporation [3]."
            ],
            "references": [
              3
            ]
          },
          {
            "id": "t4.2",
            "title": "Other platforms and topological approaches",
            "summary_points": [
              "Trapped ions and photonic systems have shown high-fidelity gates and long coherence; NMR demonstrated early small-scale algorithms but lacked entanglement in bulk implementations [2].",
              "Topological quantum computing (anyons) aims to encode information non-locally in topological degrees of freedom to intrinsically suppress decoherence; proposals exist though scalable non-abelian anyons remain experimentally challenging [5]."
            ],
            "references": [
              2,
              5
            ]
          }
        ],
        "references": [
          3,
          2,
          5
        ]
      },
      {
        "id": "t5",
        "title": "Error correction, decoherence, and fault tolerance",
        "summary_points": [
          "Decoherence (loss of coherence due to environment) and gate errors are central obstacles to scaling; quantum error correction (QEC) and fault-tolerant constructions enable arbitrarily long quantum computation provided physical error rates are below threshold values (e.g., ~10^-4 per operation in some models) [1,5].",
          "QEC encodes logical qubits into entangled states of multiple physical qubits (e.g., Steane code, surface codes) and requires repeated syndrome measurement and correction operations. DiVincenzo's criteria outline minimal physical requirements for a qubit implementation (scalable qubits, initialization, universal gates, measurement, long coherence, interconvertibility and transmission) [2,3].",
          "Topological codes and topologically protected qubits aim to raise effective thresholds by storing information in global properties immune to local noise (motivates anyon-based schemes) [5]."
        ],
        "subtopics": [
          {
            "id": "t5.1",
            "title": "Thresholds and fault-tolerant architectures",
            "summary_points": [
              "Fault tolerance requires logical gate constructions that prevent errors from proliferating; threshold theorems guarantee scalability if physical error rates are below certain values dependent on code and architecture [5].",
              "Practical implementations balance overhead (number of physical qubits per logical qubit) and achievable gate fidelities; superconducting and ion-trap platforms are actively pursuing architectures compatible with surface codes and other QEC schemes [3]."
            ],
            "references": [
              5,
              3
            ]
          }
        ],
        "references": [
          1,
          5,
          3
        ]
      },
      {
        "id": "t6",
        "title": "Quantum Internet and distributed quantum computing",
        "summary_points": [
          "The quantum Internet aims to distribute qubits and entanglement over long distances enabling quantum key distribution, delegated and blind quantum computing, non-local gates, distributed algorithms, and quantum sensing/positioning applications [4].",
          "Key technical primitives include high-fidelity entanglement generation, entanglement swapping, quantum repeaters, entanglement distillation, teleportation, and network protocols adapted to quantum constraints; satellites and fiber links are being used to demonstrate long-range entanglement [4].",
          "Distributed quantum computing envisions connecting multiple QPUs (quantum processing units) to form larger logical systems; entanglement is a consumable resource for non-local control and must be generated at sufficient rates to avoid bottlenecks in partitioned circuit execution [4]."
        ],
        "subtopics": [
          {
            "id": "t6.1",
            "title": "Quantum cloud, delegation, and verification",
            "summary_points": [
              "Quantum cloud services provide remote access to QPUs. Delegated quantum computing studies protocols where a (possibly classical or limited) client outsources computation to a quantum server, with blind quantum computing techniques (quantum one-time pad) hiding inputs and verification protocols embedding traps/tests to ensure honest computation [4].",
              "Verifiable delegation protocols allow clients to detect incorrect outputs by indistinguishable tests; these are important for trustworthy quantum cloud services and for integrating QPUs over the quantum Internet [4]."
            ],
            "references": [
              4
            ]
          },
          {
            "id": "t6.2",
            "title": "Quantum IoT and edge considerations",
            "summary_points": [
              "Emerging room-temperature quantum devices (e.g., NV centers) could enable quantum computation at the edge or fog, suggesting a potential 'quantum IoT' where quantum-enabled devices participate in networked quantum tasks, though challenges remain in coherence, connectivity, and standards [4].",
              "Standardization efforts and research into network layering, routing and entanglement management are ongoing to enable Internet-scale distributed quantum applications [4]."
            ],
            "references": [
              4
            ]
          }
        ],
        "references": [
          4
        ]
      },
      {
        "id": "t7",
        "title": "Unconventional, theoretical and geometric models",
        "summary_points": [
          "Beyond conventional platforms, unconventional device proposals include fermionic and bosonic computers, anyon/topological schemes, and speculative architectures that exploit different quantum statistics or nonlinear quantum mechanics; such devices can in principle change computational power or efficiency for specific tasks (e.g., fermionic systems for fermion simulations) [5].",
          "Seth Lloyd emphasizes two core requirements for a computational phenomenon: nonlinearity (for logical nonlinearity) and coherence (for extended superposition manipulation); many unconventional ideas are assessed against these criteria [5].",
          "Geometric and algebraic reformulations (e.g., via noncommutative geometry and spectral triples) offer alternative computational models equivalent to the circuit model; gauge-state representations and gauge transformations can recast unitary circuits as geometric gauge dynamics with potential insights into implementation and abstraction [6]."
        ],
        "subtopics": [
          {
            "id": "t7.1",
            "title": "Fermionic, bosonic, and anyonic devices",
            "summary_points": [
              "Fermionic quantum computers naturally encode occupancy of fermionic modes and can simulate lattice fermion systems more directly than conventional bosonic-qubit encodings, potentially yielding asymptotic speed advantages for certain simulations [5].",
              "Anyons (non-abelian statistics) underpin topological quantum computing proposals where computation is effected by braiding operations; topological protection can provide intrinsic error suppression but requires realizing non-abelian anyons experimentally [5]."
            ],
            "references": [
              5
            ]
          },
          {
            "id": "t7.2",
            "title": "Geometric reformulation of quantum computation",
            "summary_points": [
              "Quantum states can be represented as noncommutative connections and computational operations as gauge transformations within Connes' noncommutative geometry; such formulations are provably equivalent to the quantum circuit model and provide alternative conceptual and mathematical frameworks for computation and possibly for device realization [6].",
              "The gauge-state picture maps unitary gates to gauge transforms and can express algorithms (e.g., Deutsch-Jozsa) within geometric evolution, offering different perspectives on initialization, dynamics, and measurement [6]."
            ],
            "references": [
              6
            ]
          }
        ],
        "references": [
          5,
          6
        ]
      }
    ],
    "abstract": "Quantum computing studies computation harnessing quantum mechanical phenomena—superposition, entanglement, and measurement—to perform tasks beyond classical capabilities. This knowledge base synthesizes foundational theory (qubits, gates, Born rule), major historical milestones (Benioff, Feynman, Deutsch, Shor, Grover), algorithmic consequences (Shor's exponential factoring speedup, Grover's quadratic search, oracle separations), and practical implications (threats to current public-key cryptography, opportunities in quantum simulation and optimization). It summarizes leading hardware approaches with engineering details for superconducting qubits (Josephson junctions, circuit quantization, fabrication, cryogenic operation), as well as trapped ions, photonics, NMR, topological proposals, and unconventional devices (fermionic, anyonic, bosonic paradigms). Error correction, decoherence, and fault tolerance (threshold theorems, surface and topological codes) are highlighted as central to scalable quantum computation. The emerging quantum Internet and distributed quantum computing are covered, including entanglement distribution, quantum repeaters, delegated and blind quantum computation, and implications for quantum cloud services and potential quantum-IoT applications. The review also presents alternative conceptual frameworks—unconventional device potentials and geometric/gauge reformulations of quantum circuits—indicating both practical engineering challenges and fertile theoretical directions. Sources include encyclopedic surveys, a timeline of milestones, focused reviews of superconducting technology, and arXiv treatments of distributed quantum Internet computing, unconventional architectures, and geometric models.",
    "conclusion": "Quantum computing combines deep theoretical insights with demanding experimental engineering. Foundational algorithms demonstrate provable quantum advantages for important problems, but realizing large-scale, fault-tolerant quantum machines requires overcoming decoherence and error-correction overheads. Superconducting and trapped-ion platforms lead current hardware progress while topological and unconventional approaches offer promising routes to intrinsic robustness. The quantum Internet and distributed quantum computing extend capabilities by networking QPUs, enabling new protocols for delegation, verification, and non-local computation. Complementary theoretical models (fermionic encodings, geometric formulations) expand the conceptual toolbox. Continued advances in materials, fabrication, architectures, standards, and networking—paired with algorithmic and complexity-theoretic development—will determine which applications become practical and when quantum computing achieves broad real-world impact."
  },
  "report_parts": [
    "## Fundamental concepts\n\nQuantum computing exploits uniquely quantum-mechanical phenomena—most notably superposition, entanglement, and probabilistic measurement outcomes—to process information in ways that are not available to classical machines [1]. The elementary information carrier in this paradigm is the qubit, a two-level quantum system whose pure state can be written in Dirac notation as |ψ⟩ = α|0⟩ + β|1⟩ with complex amplitudes α and β satisfying the normalization condition |α|^2 + |β|^2 = 1; measurement of the qubit yields classical outcomes with probabilities given by these squared amplitudes in accordance with the Born rule [1]. Quantum gates implement unitary transformations on qubits, and quantum algorithms are designed to engineer interference among amplitude components so as to amplify desired measurement outcomes; the mathematical framework for these constructions is linear algebra, with state vectors and matrices representing states and operations respectively [1].\n\nFrom an operational perspective, a quantum computation can be viewed as the controlled evolution or sampling of a high-dimensional quantum state, where the exponential dimension of the combined state space (with respect to the number of qubits) underlies both the power and the simulation difficulty of quantum systems. In general, classical simulation of such quantum dynamics incurs exponential overhead in the system size, which motivates the development of quantum hardware for tasks that admit provable or empirical speedups [1][5]. At the same time, theoretical constraints and practical resource requirements shape what can be achieved: fundamental theorems such as no-cloning and no-deleting restrict state replication and erasure, coherence must be maintained to preserve quantum information during computation, and universality and certain computational capabilities depend on the availability of appropriate (including nonlinear) interactions together with precisely controllable quantum dynamics [1][5]. These constraints govern both algorithm design and the engineering of quantum devices.\n\n### Qubits, superposition, and measurement\n\nA qubit is formally represented as a vector in a two-dimensional Hilbert space and is commonly expressed using Dirac (bra–ket) notation; this representation compactly captures the possibility that the system occupies a superposition of the computational basis states |0⟩ and |1⟩ simultaneously, with complex amplitudes specifying the relative weights and phases [1]. Superposition is therefore central to quantum information processing because it enables interference between amplitude components: by applying suitable unitary operations, algorithm designers arrange constructive interference for desirable outcomes and destructive interference for undesirable ones [1].\n\nMeasurement of a qubit projects the quantum state onto one of the basis outcomes in a fundamentally probabilistic manner. The probabilities of obtaining each classical outcome are given by the squared magnitudes of the corresponding amplitudes (the Born rule), and this collapse of the state upon measurement is a key operational feature that algorithms must account for when translating quantum amplitudes into useful classical results [1]. The complex nature of amplitudes—encompassing both magnitude and phase—underpins the interference effects that make many quantum algorithms effective [1].\n\n### Entanglement and nonlocality\n\nEntanglement produces correlations between parts of a quantum system that cannot be explained by classical statistics and thus serves as a distinct resource for a range of quantum information protocols [1]. Entangled states enable primitives such as quantum teleportation, the implementation of non-local gates, advantages in quantum cryptography, and capabilities for distributed quantum computation; in each of these applications, entanglement supplies correlations or connectivity that classical systems cannot replicate [1].\n\nBuilding distributed quantum systems and the envisioned quantum Internet relies fundamentally on the ability to establish and maintain high-fidelity entanglement between spatially separated nodes. Distributed protocols for computation and cryptographic tasks depend on reliably generating, distributing, and preserving entangled resources across network links so that they can be consumed by higher-level algorithms and security protocols [4].",
    "## History and milestones\n\nQuantum computing as a distinct field emerged from the convergence of advances in quantum physics and computer science, with early formal proposals connecting physical quantum systems to computational models. Pioneering theoretical ideas such as Benioff's quantum Turing machine and Feynman's proposal that quantum systems could be simulated efficiently only by other quantum systems set the stage for later formalization and experimental efforts through the 1980s and 1990s [2]. This foundational phase established the central premise that quantum mechanical principles could be harnessed for information processing and inspired a parallel expansion of experimental work to realize small-scale quantum devices [2].\n\nA sequence of foundational theoretical milestones marked the discipline's rapid intellectual development: Benioff's work in 1980, Feynman's quantum simulation proposals in 1981–82, Deutsch's 1985 formulation of a universal quantum computer, the rediscovery of the no-cloning theorem in 1982, Bennett and Brassard's 1984 proposal of quantum key distribution, Shor's 1994 factoring algorithm, Grover's 1996 search algorithm, and Lloyd's 1996 analysis of quantum simulation efficiency. Collectively, these results defined problem classes, capabilities, and limits for quantum computation and cryptography, and they guided both theoretical exploration and experimental priorities [2][1].\n\nExperimental progress over subsequent decades steadily increased qubit counts and demonstrated elementary multi-qubit gates across multiple platforms. Notable early demonstrations included nuclear magnetic resonance (NMR) experiments implementing two- and three-qubit systems in the late 1990s, alongside parallel progress in trapped-ion and superconducting-qubit technologies, among others [2][1]. A focal point in modern discourse was the 2019 claim of quantum supremacy by a superconducting device with a reported 53/54 qubits; that claim generated significant debate, including an IBM rebuttal challenging aspects of classical simulability and the criteria for establishing supremacy, underscoring the continuing interplay between experimental demonstration and benchmarks for quantum advantage [2][1].\n\n### Decadal timeline highlights\n\nThe 1960s–1970s produced several conceptual precursors that later proved essential for quantum information theory, such as ideas related to conjugate coding, limits on quantum information transmission exemplified by the Holevo bound, and the principles of reversible computation that would underpin quantum gate models [2][1]. These theoretical constructs provided a backdrop against which explicit quantum computational proposals could later be formulated.\n\nThe 1980s crystallized the formal theoretical framework: Benioff articulated a quantum analogue of the Turing machine, Feynman proposed the use of quantum simulators to model quantum systems, and Deutsch introduced the notion of a universal quantum computer; this decade also saw the emergence of early ideas about quantum error correction that would become critical for scalable devices [2][1]. These developments collectively transitioned quantum computation from philosophical possibility to a program of concrete theoretical research.\n\nThe 1990s witnessed both striking algorithmic discoveries and the first experimental demonstrations of multi-qubit operations. Simon's oracle separation in 1993 and Shor's factoring algorithm in 1994 highlighted separations between classical and quantum complexity; Grover's search algorithm appeared in 1996, and practical demonstrations of two-qubit gates and small-scale devices occurred through the mid-to-late 1990s, alongside proposals for solid-state and topological implementations [2][1]. The 2000s and 2010s emphasized scaling experiments, the formalization of implementation criteria such as the DiVincenzo conditions, and demonstrations across multiple qubit platforms, culminating in the contested 2019 claimed milestone of quantum supremacy [2][1].",
    "## Algorithms, complexity, and applications\n\nQuantum algorithms leverage quantum mechanical principles such as superposition and interference to change the asymptotic scaling of resource requirements for particular computational tasks, producing provable separations from classical algorithms in specific instances [1]. Notable exemplars are Shor's algorithm, which affords an exponential speedup for integer factorization and discrete logarithm problems relative to the best known classical algorithms, and Grover's algorithm, which achieves a quadratic speedup for unstructured search by amplitude amplification [1]. These algorithms illustrate how phase interference and amplitude manipulation can be harnessed to alter query and time complexity for targeted problems [1].\n\nFoundational work on oracle and promise problems established formal separations between quantum and classical query complexity and thereby provided conceptual groundwork for later, more practically oriented quantum algorithms; canonical examples include the Deutsch–Jozsa, Bernstein–Vazirani, and Simon algorithms, which solve specified promise problems with fewer quantum queries than classical queries [2]. At the level of complexity theory, the class BQP (bounded-error quantum polynomial time) formalizes the set of decision problems efficiently solvable by quantum computers, while certain restricted families of quantum computation—most prominently stabilizer circuits—are amenable to efficient classical simulation as captured by the Gottesman–Knill theorem [2].\n\nProjected application domains for quantum algorithms include cryptanalysis, quantum simulation, optimization heuristics, and specialized sampling tasks designed to demonstrate quantum advantage or supremacy in regimes that are intractable for classical simulation [1][5]. The prospective advantages in these domains are generally problem- and regime-dependent: for example, quantum annealing and adiabatic methods pursue an alternative heuristic based on quantum fluctuations and tunneling for optimization problems, but empirical and theoretical evidence indicates that any advantage is contingent on problem structure and parameter choices [2][5].\n\n### Cryptography implications\n\nShor's algorithm directly threatens widely used public-key cryptosystems by providing polynomial-time algorithms for integer factoring and discrete logarithm problems when executed on a sufficiently large and low-error quantum computer; this vulnerability to quantum cryptanalysis motivates research into post-quantum cryptographic primitives that are believed to resist quantum attacks [1]. The existence of such an algorithm has therefore been a central factor in research programs that seek cryptographic constructions based on problems not known to be solvable efficiently by quantum machines [1].\n\nComplementing the threat to classical public-key schemes, quantum cryptography offers primitives grounded in the physical properties of quantum information rather than computational hardness. In particular, quantum key distribution protocols such as the Bennett–Brassard scheme provide information-theoretically secure key exchange mechanisms whose security derives from quantum measurement principles and the no-cloning property rather than from assumptions about computational intractability [2]. This contrast between quantum-capable cryptanalysis and quantum-native secure protocols motivates continued research into both post-quantum cryptography and quantum-based cryptographic primitives [1][2].\n\n### Quantum simulation and optimization\n\nQuantum computers are expected to efficiently simulate quantum many-body systems, a capability that follows the line of reasoning originating with Feynman's proposal and has been formalized in subsequent work; such simulation capability is anticipated to have potential impact on domains such as chemistry and materials science by enabling exploration of quantum phenomena that are difficult for classical simulation to capture [2][5]. These prospects have motivated algorithmic and hardware efforts aimed at realizing quantum simulation of physically relevant models and dynamics [2][5].\n\nQuantum annealing and adiabatic quantum computation constitute algorithmic strategies that attempt to exploit quantum tunneling and quantum fluctuations to navigate complex energy landscapes and escape local optima in combinatorial optimization problems. Empirical and theoretical studies indicate that any practical advantage from these approaches is instance- and regime-dependent: advantages have been observed for particular problem classes or parameter settings, but the effectiveness relative to classical heuristics varies and remains an active subject of investigation [2][5].",
    "## Physical implementations and engineering\n\nMultiple hardware paradigms are under active development for quantum information processing, including superconducting circuits, trapped ions, photonic systems, nuclear magnetic resonance (NMR), spin qubits in semiconductors, topological anyonic systems, and experimental room‑temperature platforms such as nitrogen‑vacancy (NV) centers. Each platform embodies distinct tradeoffs among coherence, controllability, connectivity, and prospects for scalability, leading research groups to pursue diverse engineering pathways tailored to these competing constraints [3][5].  \n\nWithin the solid‑state domain, superconducting qubits have emerged as a leading approach, employing macroscopic circuits that incorporate Josephson junctions to act as artificial atoms: the nonlinear inductance of the junction provides the anharmonicity necessary to resolve and selectively address qubit transitions. These circuits are fabricated by lithographic techniques, operated at millikelvin temperatures in dilution refrigerators, and controlled with microwave electronics; several device families and hybrids exist, including charge, flux, and phase qubits as well as transmon, Xmon, and fluxonium variants [3].  \n\nDespite substantial progress, engineering challenges remain central to all platforms. Principal issues include decoherence arising from coupling to the environment, the need to raise gate fidelity while minimizing crosstalk, and the development of scalable interconnects and cryogenic control systems for large processor arrays. Continued advances in materials, fabrication, and packaging have progressively improved coherence times and reduced error rates, but integration of these improvements into manufacturable, large‑scale architectures remains an active area of engineering research [3].\n\n### Superconducting quantum computing details\n\nSuperconducting qubits encode logical |0⟩ and |1⟩ states in the ground and first excited energy levels of engineered superconducting circuits. The inclusion of Josephson junctions introduces the necessary nonlinearity that separates the energy spectrum, enabling selective two‑level addressing within an otherwise harmonic circuit spectrum [3].  \n\nThe theoretical description and design of these devices rely on circuit quantization techniques: starting from a description of the superconducting condensate and a Lagrangian formulation of the circuit, one derives a Hamiltonian that captures the relevant quantum degrees of freedom. The device archetype—whether charge, flux, or phase dominated—depends on the ratio of Josephson energy to charging energy, which in turn guides choices in circuit layout and parameter regimes during design [3].  \n\nFabrication of superconducting devices leverages integrated‑circuit methods to define micrometer‑scale features and to form Josephson junctions, commonly using controlled oxidation and shadow‑evaporation processes to realize the tunnel barriers. Operation at millikelvin temperatures is required to suppress thermal excitations and realize the coherence properties necessary for quantum control and readout [3].\n\n### Other platforms and topological approaches\n\nTrapped ions and photonic systems have demonstrated exceptionally high‑fidelity gates and long coherence times, making them strong contenders for applications that prioritize gate quality and coherence over dense integration. Nuclear magnetic resonance played an important historical role by demonstrating small‑scale quantum algorithms, although bulk NMR implementations did not exhibit scalable entanglement in the same manner as other platforms [2].  \n\nTopological quantum computing proposes to encode quantum information non‑locally in topological degrees of freedom—specifically anyonic excitations—so that logical states are intrinsically protected from certain local sources of decoherence. While theoretical proposals outline how non‑abelian anyons could enable fault‑tolerant operations through braiding, the experimental realization of scalable non‑abelian anyonic systems remains challenging and is an ongoing area of fundamental and applied research [5].",
    "## Error correction, decoherence, and fault tolerance\n\nDecoherence, understood as the loss of quantum coherence due to interactions with the environment, together with imperfect quantum gates, constitutes a central obstacle to scaling quantum information processors [1][5]. Quantum error correction (QEC) and fault-tolerant constructions provide the principled framework to address these obstacles: threshold theorems establish that, if physical error rates can be reduced below certain code- and architecture-dependent values, arbitrarily long quantum computation is achievable; in some theoretical models these threshold values are on the order of 10^-4 per operation [1][5]. These results formalize the requirement that physical error rates must be driven below specified thresholds for error-corrected quantum computation to scale [1][5].\n\nQEC achieves protection by encoding logical qubits into entangled states of multiple physical qubits and by performing repeated syndrome measurement and active correction to remove errors while preserving logical information [1][5]. Representative examples of QEC codes include the Steane code and various surface codes; these codes differ in encoding structure and operational requirements but share the necessity of continual syndrome extraction and recovery operations [1][5]. Complementary to code design, DiVincenzo’s criteria enumerate minimal physical capabilities required of a qubit implementation to support scalable quantum computation, including scalable qubit arrays, reliable initialization, a universal gate set, high-fidelity measurement, sufficiently long coherence times, and the ability to interconvert and transmit quantum information [3].\n\nTopological codes and proposals for topologically protected qubits pursue an alternative route to improving effective error thresholds by encoding information in global, nonlocal degrees of freedom that are less susceptible to certain types of local perturbations; these approaches motivate anyon-based and related schemes [5]. The aim of topological protection is to raise effective thresholds and to reduce the burden on active error-correction procedures, although the degree of protection and the practical gains depend on specific code constructions and physical implementations [5].\n\n### Thresholds and fault-tolerant architectures\n\nFault tolerance requires explicit logical-gate constructions and syndrome-extraction protocols that prevent errors on a limited number of physical components from proliferating into uncorrectable logical faults; threshold theorems formalize this by guaranteeing scalability when physical error rates lie below code- and architecture-dependent thresholds [5]. These theorems place constraints on how errors may be propagated and combined within fault-tolerant protocols and thereby inform the design of logical gate sequences and syndrome-extraction circuits [5].\n\nPractical fault-tolerant architectures must balance resource overhead—most prominently the number of physical qubits required per logical qubit—and the achievable gate fidelities of the underlying hardware, since this trade-off strongly influences which QEC codes and architectural strategies are viable in a given platform [5][3]. Superconducting-qubit and trapped-ion platforms are actively pursuing architectures designed to be compatible with surface codes and other leading QEC schemes, reflecting distinct engineering choices about connectivity, native gate sets, and error rates that affect both overhead and threshold considerations [3][5]. Quantitative threshold values and the resulting resource overheads remain dependent on the chosen code and implementation strategy, underscoring the need to match error-correction protocols to realistic hardware characteristics [1][5].",
    "## Quantum Internet and distributed quantum computing\n\nThe quantum Internet is conceived as an architecture for distributing qubits and entanglement across long distances to enable a range of quantum-enabled services, including quantum key distribution, delegated and blind quantum computing, implementation of non-local gates, execution of distributed quantum algorithms, and enhanced quantum sensing and positioning applications [4]. These application domains rely fundamentally on the capacity to create, maintain, and utilize entanglement between remote nodes, making long-range quantum connectivity a central objective of the quantum Internet research agenda [4].  \n\nRealizing such a network requires a suite of technical primitives tailored to quantum information: high-fidelity entanglement generation between nodes, entanglement swapping to extend entangled links, quantum repeaters to mitigate loss and decoherence over long distances, and entanglement distillation to improve fidelity when noise is present [4]. Teleportation protocols and network-layer protocols adapted to quantum constraints are also essential to move quantum states and orchestrate multi-node operations, and both satellite-based links and optical fiber are being experimentally employed to demonstrate and extend long-range entanglement distribution [4].  \n\nDistributed quantum computing envisions connecting multiple quantum processing units (QPUs) into larger logical systems so that quantum resources can be pooled to perform computations that a single QPU cannot efficiently realize alone [4]. In this model entanglement functions as a consumable resource that enables non-local control and gate operations across partitioned circuits, and therefore must be generated at sufficient rates and fidelities to avoid becoming a throughput bottleneck during distributed execution [4]. Consideration of resource generation rates, fidelity management, and protocol scheduling is therefore intrinsic to the design and evaluation of distributed quantum computing architectures over the quantum Internet [4].  \n\n### Quantum cloud, delegation, and verification\n\nQuantum cloud services provide remote access to QPUs, allowing clients to run quantum programs without local quantum hardware [4]. Delegated quantum computing formalizes protocols in which a client—potentially classical or possessing only limited quantum capabilities—outsources quantum computations to a quantum server while maintaining desired security or privacy properties [4]. Blind quantum computing techniques, such as the quantum one-time pad, protect client inputs and computation details from the server, while verification protocols embed indistinguishable tests or traps that enable clients to detect incorrect or malicious server behavior [4]. Verifiable delegation schemes are therefore critical for establishing trustworthy quantum cloud services and for enabling reliable integration of remote QPUs within the broader quantum Internet [4].  \n\n### Quantum IoT and edge considerations\n\nEmerging room-temperature quantum devices, for example those based on NV centers, offer the possibility of quantum functionality at the network edge or fog, suggesting a future \"quantum IoT\" in which quantum-enabled devices participate in distributed quantum tasks and sensing applications [4]. However, such edge deployments face persistent challenges in maintaining coherence, ensuring reliable connectivity, and establishing interoperable standards that permit heterogeneous devices to function collectively in a networked quantum system [4]. To address these challenges, standardization efforts and research into network layering, routing, and entanglement management are ongoing, as these protocol-level developments are necessary to scale quantum networking to Internet-scale distributed applications [4].",
    "## Unconventional, theoretical and geometric models\n\nBeyond conventional qubit and bosonic-photonic platforms, the literature surveys a range of unconventional device proposals that exploit alternative quantum statistics or modified dynamical principles. Examples include fermionic and bosonic computers, anyon- and topological-based schemes, and more speculative architectures that invoke different quantum statistics or even nonlinear variants of quantum mechanics; such proposals are considered because, in principle, they can alter computational power or efficiency for particular tasks [5]. A concrete instance of task-specific advantage is the suggestion that fermionic devices can more directly encode and simulate lattice fermion systems than encodings based on bosonic qubits, potentially yielding asymptotic speedups for those simulation problems [5].\n\nAssessment of unconventional computational phenomena is guided by foundational criteria that distinguish mere physical novelties from useful computational resources. Seth Lloyd, for example, emphasizes two core requirements for a phenomenon to serve as a computational primitive: nonlinearity, insofar as it furnishes logical nonlinearity, and coherence, insofar as it permits manipulation of extended superposition states; many unconventional proposals are evaluated against these requirements to determine whether they can support logical operations and coherent quantum information processing [5]. This evaluative framework helps to separate proposals that may be physically interesting from those that provide genuine computational enhancement for specific algorithmic tasks [5].\n\nIn parallel with hardware-divergent proposals, geometric and algebraic reformulations of quantum computation provide alternative conceptual and mathematical frameworks equivalent to the standard circuit model. Reformulations using tools such as Connes' noncommutative geometry and spectral triples represent quantum states and operations in geometric-algebraic terms and are provably equivalent to the conventional circuit description, offering different perspectives on abstraction and potential routes to implementation [6]. Relatedly, gauge-state representations recast unitary circuits as gauge dynamics, a viewpoint that can illuminate aspects of initialization, dynamical evolution, and measurement by interpreting gates as geometric gauge transformations [6].\n\n### Fermionic, bosonic, and anyonic devices\n\nFermionic quantum computers are distinguished by their natural encoding of occupation numbers for fermionic modes, which aligns directly with the structure of lattice fermion problems. This natural encoding can simplify the representation of fermionic Hamiltonians and evolution compared to conventional encodings that translate fermionic degrees of freedom into bosonic qubits, and it has been argued that such direct encodings can yield asymptotic speed advantages for certain fermionic simulation tasks [5]. Bosonic-device proposals also appear among unconventional architectures as alternative ways to exploit quantum statistics for computation, with the broader class of proposals motivated by leveraging the distinct physical behavior of bosons or fermions to match particular computational problems [5].\n\nAnyons, and in particular non-abelian anyons, form the conceptual basis for topological quantum computing proposals in which logical operations are effected by braiding worldlines of anyons. The nonlocal, topologically encoded information in such schemes offers intrinsic error suppression—topological protection—because logical degrees of freedom are stored in global braiding properties rather than local observables, but these advantages rely on the experimental realization of non-abelian anyons, which remains a central challenge for the approach [5].\n\n### Geometric reformulation of quantum computation\n\nQuantum computational processes can be reformulated in geometric and algebraic language by representing quantum states as noncommutative connections and computational operations as gauge transformations within frameworks such as Connes' noncommutative geometry; these formulations have been shown to be equivalent to the quantum circuit model and thus provide alternative but formally consistent descriptions of quantum computation [6]. Framing computation in terms of spectral triples and noncommutative connections supplies a different mathematical viewpoint that can clarify structural aspects of algorithms and may suggest alternative abstraction layers for device design and control [6].\n\nThe gauge-state picture, in particular, establishes a direct mapping between unitary gates and gauge transformations, enabling standard quantum algorithms to be expressed as geometric evolutions. Algorithms that are ordinarily presented in circuit form, including canonical examples such as the Deutsch–Jozsa algorithm, can be reinterpreted within this geometric evolution framework, which offers distinct perspectives on initialization procedures, dynamical pathways, and measurement processes while remaining operationally equivalent to the circuit description [6]."
  ],
  "criticism": {
    "0": "PASS",
    "1": "PASS",
    "2": "The section is largely accurate and consistent with the INPUT JSON, but there are a few unsupported or slightly overreaching statements. Below are the issues, why they are problematic, and how to fix them.\n\n1. Unsupported claim about transition planning and standardization\n   - Problematic statement: \"The potential for practical cryptanalytic capability on quantum hardware underlies substantial efforts in cryptographic transition planning and standardization.\"\n   - Why problematic: The INPUT JSON (t3.1) states that Shor's algorithm motivates research into post-quantum cryptography, but it does not mention \"substantial efforts in cryptographic transition planning and standardization.\" That is an extrapolation beyond the provided summaries.\n   - Where it conflicts with the JSON: topic t3, subtopic t3.1 (summary_points).\n   - Suggested fix: Remove or soften the claim to match the JSON wording (e.g., limit to \"motivating research into post-quantum cryptography\") or add an explicit source if you intend to assert ongoing standardization efforts.\n\n2. Broad, unsupported framing claim about \"cryptographic futures\"\n   - Problematic statement: \"This contrast between quantum-enabled attacks on classical schemes and quantum-native secure protocols frames much of the contemporary discussion on cryptographic futures.\"\n   - Why problematic: The JSON does not make this broad meta-level claim about the scope of contemporary discussion; it only contrasts threats from quantum algorithms with QKD as an alternative (t3.1). The sentence generalizes beyond what the source summaries assert.\n   - Where it conflicts with the JSON: topic t3, subtopic t3.1 (summary_points).\n   - Suggested fix: Replace with a narrower statement tied to the JSON, e.g., \"This contrast is discussed in the literature and motivates research into both post-quantum cryptography and quantum-native secure protocols.\"\n\n3. Minor specificity not present in the JSON: \"electronic structure problems\"\n   - Problematic statement: \"targeted at simulating Hamiltonian dynamics and electronic structure problems.\"\n   - Why problematic: The JSON mentions \"chemistry and materials science\" (t3.2) but does not explicitly list \"electronic structure problems.\" While electronic structure is a common example of quantum simulation applications, it is not explicitly present in the provided summaries.\n   - Where it conflicts with the JSON: topic t3, subtopic t3.2 (summary_points).\n   - Suggested fix: Either revert to the exact phrasing in the JSON (\"chemistry and materials science\") or add a citation/source supporting the specific mention of electronic-structure simulation.\n\nEverything else in the writer output aligns with the INPUT JSON (references to Shor, Grover, oracle/promise problems, BQP, Gottesman–Knill, quantum simulation, and the conditional/instance-dependent nature of annealing advantages).",
    "3": "PASS",
    "4": "- Unsupported/extra claim: The sentence asserting arbitrarily long computation is achievable \"with polylogarithmic overhead\" is not supported by the INPUT JSON. The JSON (t5 and t5.1) states that threshold theorems guarantee scalability if physical error rates are below code- and architecture-dependent thresholds and that practical overheads must be balanced, but it does not state a polylogarithmic-overhead guarantee. Flag this sentence for revision or removal (see t5, t5.1).\n\n- Minor reference inconsistency: The paragraph names the Steane code and surface codes and cites [3]. In the INPUT JSON, QEC examples (Steane, surface codes) and related discussion are listed under t5 with references [1,5]; reference [3] in the JSON is associated with superconducting-qubit engineering (t4). Either align the citation to the references given for QEC in the JSON (t5: [1,5]) or omit the specific bracketed citation here.\n\n- Suggestion for precision: Several statements (e.g., \"intrinsically immune to many forms of local noise\", \"reducing the demand on active error correction\") are consistent in spirit with the INPUT (t5), but they are slightly stronger phrasing than the JSON's \"aim to raise effective thresholds\" and \"motivate anyon-based schemes\". Consider softening or explicitly noting these are goals/aims rather than proven universal outcomes (refer to t5).\n\nEverything else in the WRITER OUTPUT matches the topics and summary points in the INPUT JSON (see t5 and t5.1).",
    "5": "PASS",
    "6": "PASS"
  },
  "is_criticized": true
}