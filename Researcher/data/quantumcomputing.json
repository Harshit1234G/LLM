{
  "topic": "Quantum Computing",
  "source": "both",
  "wikipedia_docs": "Index: 1\nTitle: Quantum computing\nSource: https://en.wikipedia.org/wiki/Quantum_computing\nContent: A quantum computer is a (real or theoretical) computer that uses quantum mechanical phenomena in an essential way: it exploits superposed and entangled states, and the intrinsically non-deterministic outcomes of quantum measurements, as features of its computation. Quantum computers can be viewed as sampling from quantum systems that evolve in ways classically described as operating on an enormous number of possibilities simultaneously, though still subject to strict computational constraints. By contrast, ordinary (\"classical\") computers operate according to deterministic rules. Any classical computer can, in principle, be replicated by a (classical) mechanical device such as a Turing machine, with only polynomial overhead in time. Quantum computers, on the other hand are believed to require exponentially more resources to simulate classically. It is widely believed that a scalable quantum computer could perform some calculations exponentially faster than any classical computer. Theoretically, a large-scale quantum computer could break some widely used public-key cryptographic schemes and aid physicists in performing physical simulations. However, current hardware implementations of quantum computation are largely experimental and only suitable for specialized tasks.\nThe basic unit of information in quantum computing, the qubit (or \"quantum bit\"), serves the same function as the bit in ordinary or \"classical\" computing. However, unlike a classical bit, which can be in one of two states (a binary), a qubit can exist in a linear combination of two states known as a quantum superposition. The result of measuring a qubit is one of the two states given by a probabilistic rule. If a quantum computer manipulates the qubit in a particular way, wave interference effects amplify probability of the desired measurement result. The design of quantum algorithms involves creating procedures that allow a quantum computer to perform this amplification.\nQuantum computers are not yet practical for real-world applications. Physically engineering high-quality qubits has proven to be challenging. If a physical qubit is not sufficiently isolated from its environment, it suffers from quantum decoherence, introducing noise into calculations. National governments have invested heavily in experimental research aimed at developing scalable qubits with longer coherence times and lower error rates. Example implementations include superconductors (which isolate an electrical current by eliminating electrical resistance) and ion traps (which confine a single atomic particle using electromagnetic fields). Researchers have claimed, and are widely believed to be correct, that certain quantum devices can outperform classical computers on narrowly defined tasks, a milestone referred to as quantum advantage or quantum supremacy. These tasks are not necessarily useful for real-world applications.\n\n\n== History ==\n\nFor many years, the fields of quantum mechanics and computer science formed distinct academic communities. Modern quantum theory developed in the 1920s to explain perplexing physical phenomena observed at atomic scales, and digital computers emerged in the following decades to replace human computers for tedious calculations. Both disciplines had practical applications during World War II; computers played a major role in wartime cryptography, and quantum physics was essential for nuclear physics used in the Manhattan Project.\nAs physicists applied quantum mechanical models to computational problems and swapped digital bits for qubits, the fields of quantum mechanics and computer science began to converge. In 1980, Paul Benioff introduced the quantum Turing machine, which uses quantum theory to describe a simplified computer.\nWhen digital computers became faster, physicists faced an exponential increase in overhead when simulating quantum dynamics, prompting Yuri Manin and Richard Feynman to independently suggest that hardware based on quantum phenomena might be more efficient for computer simulation.\nIn a 1984 paper, Charles Bennett and Gilles Brassard applied quantum theory to cryptography protocols and demonstrated that quantum key distribution could enhance information security.\nQuantum algorithms then emerged for solving oracle problems, such as Deutsch's algorithm in 1985, the Bernstein–Vazirani algorithm in 1993, and Simon's algorithm in 1994.\nThese algorithms did not solve practical problems, but demonstrated mathematically that one could gain more information by querying a black box with a quantum state in superposition, sometimes referred to as quantum parallelism.\n\nPeter Shor built on these results with his 1994 algorithm for breaking the widely used RSA and Diffie–Hellman encryption protocols, which drew significant attention to the field of quantum computing. In 1996, Grover's algorithm established a quantum speedup for the widely applicable unstructured search problem. The same year, Seth Lloyd proved that quantum computers could simulate quantum systems without the exponential overhead present in classical simulations, validating Feynman's 1982 conjecture.\nOver the years, experimentalists have constructed small-scale quantum computers using trapped ions and superconductors.\nIn 1998, a two-qubit quantum computer demonstrated the feasibility of the technology, and subsequent experiments have increased the number of qubits and reduced error rates.\nIn 2019, Google AI and NASA announced that they had achieved quantum supremacy with a 54-qubit machine, performing a computation that is impossible for any classical computer.\nThis announcement was met with a rebuttal from Google's direct competitor, IBM. IBM contended that the calculation Google claimed would take 10,000 years could be performed in just 2.5 days on its own Summit supercomputer if its architecture were optimized, sparking a debate over the precise threshold for \"quantum supremacy\".\n\n\n== Quantum information processing ==\nComputer engineers typically describe a modern computer's operation in terms of classical electrodynamics.\nWithin these \"classical\" computers, some components (such as semiconductors and random number generators) may rely on quantum behavior, but these components are not isolated from their environment, so any quantum information quickly decoheres.\nWhile programmers may depend on probability theory when designing a randomized algorithm, quantum mechanical notions like superposition and interference are largely irrelevant for program analysis.\nQuantum programs, in contrast, rely on precise control of coherent quantum systems. Physicists describe these systems mathematically using linear algebra. Complex numbers model probability amplitudes, vectors model quantum states, and matrices model the operations that can be performed on these states. Programming a quantum computer is then a matter of composing operations in such a way that the resulting program computes a useful result in theory and is implementable in practice.\nAs physicist Charlie Bennett describes the relationship between quantum and classical computers,\n\nA classical computer is a quantum computer ... so we shouldn't be asking about \"where do quantum speedups come from?\" We should say, \"well, all computers are quantum. ... Where do classical slowdowns come from?\"\n\n\n=== Quantum information ===\nJust as the bit is the basic concept of classical information theory, the qubit is the fundamental unit of quantum information. The same term qubit is used to refer to an abstract mathematical model and to any physical system that is represented by that model. A classical bit, by definition, exists in either of two physical states, which can be denoted 0 and 1. A qubit is also described by a state, and two states often written \n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n      \n    \n    {\\displaystyle |0\\rangle }\n  \n and \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle |1\\rangle }\n  \n serve as the quantum counterparts of the classical states 0 and 1. However, the quantum states \n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n      \n    \n    {\\displaystyle |0\\rangle }\n  \n and \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle |1\\rangle }\n  \n belong to a vector space, meaning that they can be multiplied by constants and added together, and the result is again a valid quantum state. Such a combination is known as a superposition of \n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n      \n    \n    {\\displaystyle |0\\rangle }\n  \n and \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle |1\\rangle }\n  \n.\nA two-dimensional vector mathematically represents a qubit state. Physicists typically use Dirac notation for quantum mechanical linear algebra, writing \n  \n    \n      \n        \n          |\n        \n        ψ\n        ⟩\n      \n    \n    {\\displaystyle |\\psi \\rangle }\n  \n 'ket psi' for a vector labeled \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n . Because a qubit is a two-state system, any qubit state takes the form \n  \n    \n      \n        α\n        \n          |\n        \n        0\n        ⟩\n        +\n        β\n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle \\alpha |0\\rangle +\\beta |1\\rangle }\n  \n , where \n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n      \n    \n    {\\displaystyle |0\\rangle }\n  \n and \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle |1\\rangle }\n  \n are the standard basis states, and \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n and \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n are the probability amplitudes, which are in general complex numbers. If either \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n or \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n is zero, the qubit is effectively a classical bit; when both are nonzero, the qubit is in superposition. Such a quantum state vector acts similarly to a (classical) probability vector, with one key difference: unlike probabilities, probability amplitudes are not necessarily positive numbers. Negative amplitudes allow for destructive wave interference.\nWhen a qubit is measured in the standard basis, the result is a classical bit. The Born rule describes the norm-squared correspondence between amplitudes and probabilities—when measuring a qubit \n  \n    \n      \n        α\n        \n          |\n        \n        0\n        ⟩\n        +\n        β\n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle \\alpha |0\\rangle +\\beta |1\\rangle }\n  \n, the state collapses to \n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n      \n    \n    {\\displaystyle |0\\rangle }\n  \n with probability \n  \n    \n      \n        \n          |\n        \n        α\n        \n          \n            |\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle |\\alpha |^{2}}\n  \n, or to \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle |1\\rangle }\n  \n with probability \n  \n    \n      \n        \n          |\n        \n        β\n        \n          \n            |\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle |\\beta |^{2}}\n  \n.\nAny valid qubit state has coefficients \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n and \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n such that \n  \n    \n      \n        \n          |\n        \n        α\n        \n          \n            |\n          \n          \n            2\n          \n        \n        +\n        \n          |\n        \n        β\n        \n          \n            |\n          \n          \n            2\n          \n        \n        =\n\n---\n\nIndex: 2\nTitle: Timeline of quantum computing and communication\nSource: https://en.wikipedia.org/wiki/Timeline_of_quantum_computing_and_communication\nContent: This is a timeline of quantum computing and communication.\n\n\n== 1960s ==\n\n\n=== 1968/69/70 ===\nStephen Wiesner invents conjugate coding.\n\n\n=== 1969 ===\n13 June – James L. Park (Washington State University, Pullman)'s paper is received by Foundations of Physics, in which he describes the non possibility of disturbance in a quantum transition state in the context of a disproof of quantum jumps in the concept of the atom described by Bohr.\n\n\n== 1970s ==\n\n\n=== 1973 ===\nAlexander Holevo's paper is published. The Holevo bound describes a limit of the quantity of classical information which is possible to quanta encode.\nCharles H. Bennett shows that computation can be done reversibly.\n\n\n=== 1975 ===\nR. P. Poplavskii publishes \"Thermodynamical models of information processing\" (in Russian) which shows the computational infeasibility of simulating quantum systems on classical computers, due to the superposition principle.\nRoman Stanisław Ingarden, a Polish mathematical physicist, submits the paper \"Quantum Information Theory\" in Reports on Mathematical Physics, vol. 10, pp. 43–72, published 1976. It is one of the first attempts at creating a quantum information theory, showing that Shannon information theory cannot directly be generalized to the quantum case, but rather that it is possible to construct a quantum information theory, which is a generalization of Shannon's theory, within the formalism of a generalized quantum mechanics of open systems and a generalized concept of observables (the so-called semi-observables).\n\n\n== 1980s ==\n\n\n=== 1980 ===\nPaul Benioff describes the first quantum mechanical model of a computer. In this work, Benioff showed that a computer could operate under the laws of quantum mechanics by describing a Schrödinger equation description of Turing machines, laying a foundation for further work in quantum computing. The paper was submitted in June 1979 and published in April 1980.\nYuri Manin briefly motivates the idea of quantum computing.\nTommaso Toffoli introduces the reversible Toffoli gate, which (together with initialized ancilla bits) is functionally complete for reversible classical computation.\n\n\n=== 1981 ===\nAt the first Conference on the Physics of Computation, held at the Massachusetts Institute of Technology (MIT) in May, Paul Benioff and Richard Feynman give talks on quantum computing. Benioff's talk built on his earlier 1980 work showing that a computer can operate under the laws of quantum mechanics. The talk was titled \"Quantum mechanical Hamiltonian models of discrete processes that erase their own histories: application to Turing machines\". In Feynman's talk, he observed that it appeared to be impossible to efficiently simulate the evolution of a quantum nature system on a classical computer, and he proposed a basic model for a quantum computer. Feynman's conjecture on a quantum simulating computer, published 1982, understood as – the reality of quantum mechanics expressed as an effective quantum system necessitates quantum computers, is conventionally accepted as a beginning of quantum computing.\n\n\n=== 1982 ===\nPaul Benioff further develops his original model of a quantum mechanical Turing machine.\nWilliam Wootters and Wojciech H. Zurek, and independently Dennis Dieks rediscover the no-cloning theorem of James L. Park.\n\n\n=== 1984 ===\nCharles Bennett and Gilles Brassard employ Wiesner's conjugate coding for distribution of cryptographic keys.\n\n\n=== 1985 ===\nDavid Deutsch, at the University of Oxford, England, describes the first universal quantum computer. Just as a Universal Turing machine can simulate any other Turing machine efficiently (Church–Turing thesis), so the universal quantum computer is able to simulate any other quantum computer with at most a polynomial slowdown.\nAsher Peres points out the need for quantum error correction schemes and discusses a repetition code for amplitude errors.\n\n\n=== 1988 ===\nYoshihisa Yamamoto and K. Igeta propose the first physical realization of a quantum computer, including Feynman's CNOT gate. Their approach uses atoms and photons and is the progenitor of modern quantum computing and networking protocols using photons to transmit qubits and atoms to perform two-qubit operations.\n\n\n=== 1989 ===\nGerard J. Milburn proposes a quantum-optical realization of a Fredkin gate.\nBikas Chakrabarti & collaborators from Saha Institute of Nuclear Physics, Kolkata, India, propose that quantum fluctuations could help explore rugged energy landscapes by escaping from local minima of glassy systems having tall but thin barriers by tunneling (instead of climbing over using thermal excitations), suggesting the effectiveness of quantum annealing over classical simulated annealing.\n\n\n== 1990s ==\n\n\n=== 1991 ===\nArtur Ekert at the University of Oxford, proposes entanglement-based secure communication.\n\n\n=== 1992 ===\nDavid Deutsch and Richard Jozsa propose a computational problem that can be solved efficiently with the deterministic Deutsch–Jozsa algorithm on a quantum computer, but for which no deterministic classical algorithm is possible. This was perhaps the earliest result in the computational complexity of quantum computers, proving that they were capable of performing some well-defined computation more efficiently than any classical computer.\nEthan Bernstein and Umesh Vazirani propose the Bernstein–Vazirani algorithm. It is a restricted version of the Deutsch–Jozsa algorithm where instead of distinguishing between two different classes of functions, it tries to learn a string encoded in a function. The Bernstein–Vazirani algorithm was designed to prove an oracle separation between complexity classes BQP and BPP.\nResearch groups at Max Planck Institute of Quantum Optics (Garching) and shortly after at NIST (Boulder) experimentally realize the first crystallized strings of laser-cooled ions. Linear ion crystals constitute the qubit basis for most quantum computing and simulation experiments with trapped ions.\n\n\n=== 1993 ===\nDaniel R. Simon, at Université de Montréal, Quebec, Canada, invent an oracle problem, Simon's problem, for which a quantum computer would be exponentially faster than a conventional computer. This algorithm introduces the main ideas which were then developed in Peter Shor's factorization algorithm.\n\n\n=== 1994 ===\nPeter Shor, at AT&T's Bell Labs in New Jersey, publishes Shor's algorithm. It would allow a quantum computer to factor large integers quickly. It solves both the factoring problem and the discrete log problem. The algorithm can theoretically break many of the cryptosystems in use today. Its invention sparked tremendous interest in quantum computers.\nThe first United States Government workshop on quantum computing is organized by NIST in Gaithersburg, Maryland, in autumn.\nIsaac Chuang and Yoshihisa Yamamoto propose a quantum-optical realization of a quantum computer to implement Deutsch's algorithm. Their work introduced dual-rail encoding for photonic qubits.\nIn December, Ignacio Cirac, at University of Castilla–La Mancha at Ciudad Real, and Peter Zoller at the University of Innsbruck propose an experimental realization of the controlled NOT gate with cold trapped ions.\n\n\n=== 1995 ===\nThe first United States Department of Defense workshop on quantum computing and quantum cryptography is organized by United States Army physicists Charles M. Bowden, Jonathan Dowling, and Henry O. Everitt; it took place in February at the University of Arizona in Tucson.\nPeter Shor proposes the first schemes for quantum error correction.\nChristopher Monroe and David J. Wineland at NIST (Boulder, Colorado) experimentally realize the first quantum logic gate – the controlled NOT gate – with trapped ions, following the Cirac-Zoller proposal.\nIndependently, Subhash Kak and Ronald Chrisley propose the first quantum neural network.\n\n\n=== 1996 ===\nLov Grover, at Bell Labs, invents the quantum database search algorithm. The quadratic speedup is not as dramatic as the speedup for factoring, discrete logs, or physics simulations. However, the algorithm can be applied to a much wider variety of problems. Any problem that can be solved by random, brute-force search, may take advantage of this quadratic speedup in the number of search queries.\nThe United States Government, particularly in a joint partnership of the Army Research Office (now part of the Army Research Laboratory) and the National Security Agency, issues the first public call for research proposals in quantum information processing.\nAndrew Steane designs Steane code for error correction.\nDavid DiVincenzo, of IBM, proposes a list of minimal requirements for creating a quantum computer, now called DiVincenzo's criteria.\nSeth Lloyd proves Feynman's conjecture on quantum simulation.\n\n\n=== 1997 ===\nDavid G. Cory, Amr Fahmy and Timothy Havel, and at the same time Neil Gershenfeld and Isaac Chuang at MIT publish the first papers realizing gates for quantum computers based on bulk nuclear spin resonance, or thermal ensembles. The technology is based on a nuclear magnetic resonance (NMR) machine, which is similar to the medical magnetic resonance imaging machine.\nAlexei Kitaev describes the principles of topological quantum computation as a method for dealing with the problem of decoherence.\nDaniel Loss and David DiVincenzo propose the Loss-DiVincenzo quantum computer, using as qubits the intrinsic spin-1/2 degree of freedom of individual electrons confined to quantum dots.\n\n\n=== 1998 ===\nThe first experimental demonstration of a quantum algorithm is reported. A working 2-qubit NMR quantum computer was used to solve Deutsch's problem by Jonathan A. Jones and Michele Mosca at Oxford University and shortly after by Isaac L. Chuang at IBM's Almaden Research Center, in California, and Mark Kubinec and the University of California, Berkeley together with coworkers at Stanford University in California and MIT in Massachusetts.\nThe first working 3-qubit NMR computer is reported.\nBruce Kane proposes a silicon-based nuclear spin quantum computer, using nuclear spins of individual phosphorus atoms in silicon as the qubits and donor electrons to mediate the coupling between qubits.\nThe first execution of Grover's algorithm on an NMR computer is reported.\nHidetoshi Nishimori & colleagues from Tokyo Institute of Technology show that a quantum annealing algorithm can perform better than classical simulated annealing under certain conditions.\nDaniel Gottesman and Emanuel Knill independently prove that a certain subclass of quantum computations can be efficiently emulated with classical resources (Gottesman–Knill theorem).\n\n\n=== 1999 ===\nSamuel L. Braunstein and collaborators show that none of the bulk NMR experiments performed to date contain any entanglement; the quantum states being too strongly mixed. This is seen as evidence that NMR computers would likely not yield a benefit over classical computers. It remains an open question, however, whether entanglement is necessary for quantum computational speedup.\nGabriel Aeppli, Thomas Rosenbaum and colleagues demonstrate experimentally the basic concepts of quantum annealing in a condensed matter system.\nYasunobu Nakamura and Jaw-Shen Tsai demonstrate that a superconducting circuit can be used as a qubit.\n\n\n== 2000s ==\n\n\n=== 2000 ===\nArun K. Pati and Samuel L. Braunstein prove the quantum no-deleting theorem. This is dual to the no-cloning theorem which shows that one cannot delete a copy of an unknown qubit. Together with the stronger no-cloning theorem, the no-deleting theorem has the implication that quantum information can neither be created nor be destroyed.\nThe first working 5-qubit NMR computer is demonstrated at the Technical University of Munich, Germany.\nThe first execution of order finding (part of Shor's algorithm) at IBM's Almaden Research Center and Stanford University is demonstrated.\nThe first working 7-qubit NMR computer is demonstrated at the Los Alamos National Laboratory in New Mexico.\nThe textbook, Quantum Comp\n\n---\n\nIndex: 3\nTitle: Superconducting quantum computing\nSource: https://en.wikipedia.org/wiki/Superconducting_quantum_computing\nContent: Superconducting quantum computing is a branch of solid state  physics and quantum computing that implements superconducting electronic circuits using superconducting qubits as artificial atoms, or quantum dots. For superconducting qubits, the two logic states are the ground state and the excited state, denoted \n  \n    \n      \n        \n          |\n        \n        g\n        ⟩\n        \n           and \n        \n        \n          |\n        \n        e\n        ⟩\n      \n    \n    {\\displaystyle |g\\rangle {\\text{ and }}|e\\rangle }\n  \n respectively. Research in superconducting quantum computing is conducted by companies such as Google, IBM, IMEC, BBN Technologies, Rigetti, and Intel.  Many recently developed QPUs (quantum processing units, or quantum chips) use superconducting architecture.\nAs of May 2016, up to 9 fully controllable qubits are demonstrated in the 1D array, and up to 16 in 2D architecture. In October 2019, the Martinis group, partnered with Google, published an article demonstrating novel quantum supremacy, using a chip composed of 53 superconducting qubits.\n\n\n== Background ==\nClassical computation models rely on physical implementations consistent with the laws of classical mechanics. Classical descriptions are accurate only for specific systems consisting of a relatively large number of atoms. A more general description of nature is given by quantum mechanics. Quantum computation studies quantum phenomena applications beyond the scope of classical approximation, with the purpose of performing quantum information processing and communication. Various models of quantum computation exist, but the most popular models incorporate concepts of qubits and quantum gates (or gate-based superconducting quantum computing).\nSuperconductors are implemented due to the fact that at low temperatures they have infinite conductivity and zero resistance. Each qubit is built using semiconductor circuits with an LC circuit: a capacitor and an inductor.\nSuperconducting capacitors and inductors are used to produce a resonant circuit that dissipates almost no energy, as heat  can disrupt quantum information. The superconducting resonant circuits are a class of artificial atoms that can be used as qubits. Theoretical and physical implementations of quantum circuits are widely different. Implementing a quantum circuit had its own set of challenges and must abide by DiVincenzo's criteria, conditions proposed by theoretical physicist David P DiVincenzo, which is set of criteria for the physical implementation of superconducting quantum computing, where the initial five criteria ensure that the quantum computer is in line with the postulates of quantum mechanics and the remaining two pertaining to the relaying of this information over a network.\nWe map the ground and excited states of these atoms to the 0 and 1 state as these are discrete and distinct energy values and therefore it is in line with the postulates of quantum mechanics. In such a construction however an electron can jump to multiple other energy states and not be confined to our excited state; therefore, it is imperative that the system be limited to be affected only by photons with energy difference required to jump from the ground state to the excited state. However, this leaves one major issue, we require uneven spacing between our energy levels to prevent photons with the same energy from causing transitions between neighboring pairs of states. Josephson junctions are superconducting elements with a nonlinear inductance, which is critically important for qubit implementation. The use of this nonlinear element in the resonant superconducting circuit produces uneven spacings between the energy levels.\n\n\n=== Qubits ===\nA qubit is a generalization of a bit (a system with two possible states) capable of occupying a quantum superposition of both states. A quantum gate, on the other hand, is a generalization of a logic gate describing the transformation of one or more qubits once a gate is applied given their initial state. Physical implementation of qubits and gates is challenging for the same reason that quantum phenomena are difficult to observe in everyday life given the minute scale on which they occur. One approach to achieving quantum computers is by implementing superconductors whereby quantum effects are macroscopically observable, though at the price of extremely low operation temperatures.\n\n\n=== Superconductors ===\nUnlike typical conductors, superconductors possess a critical temperature at which resistivity plummets to zero and conductivity is drastically increased. In superconductors, the basic charge carriers are pairs of electrons (known as Cooper pairs), rather than single fermions as found in typical conductors.  Cooper pairs are loosely bound and have an energy state lower than that of Fermi energy. Electrons forming Cooper pairs possess equal and opposite momentum and spin so that the total spin of the Cooper pair is an integer spin. Hence, Cooper pairs are bosons. Two such superconductors which have been used in superconducting qubit models are niobium and tantalum, both d-band superconductors.\n\n\n==== Bose–Einstein condensates ====\nOnce cooled to nearly absolute zero, a collection of bosons collapse into their lowest energy quantum state (the ground state) to form a state of matter known as Bose–Einstein condensate. Unlike fermions, bosons may occupy the same quantum energy level (or quantum state) and do not obey the Pauli exclusion principle. Classically, Bose-Einstein Condensate can be conceptualized as multiple particles occupying the same position in space and having equal momentum. Because interactive forces between bosons are minimized, Bose-Einstein Condensates effectively act as a superconductor. Thus, superconductors are implemented in quantum computing because they possess both near infinite conductivity and near zero resistance. The advantages of a superconductor over a typical conductor, then, are twofold in that superconductors can, in theory, transmit signals nearly instantaneously and run infinitely with no energy loss. The prospect of actualizing superconducting quantum computers becomes all the more promising considering NASA's recent development of the Cold Atom Lab in outer space where Bose-Einstein Condensates are more readily achieved and sustained (without rapid dissipation) for longer periods of time without the constraints of gravity.\n\n\n=== Electrical circuits ===\nAt each point of a superconducting electronic circuit (a network of electrical elements), the condensate wave function describing charge flow is well-defined by some complex probability amplitude. In typical conductor electrical circuits, this same description is true for individual charge carriers except that the various wave functions are averaged in macroscopic analysis,  making it impossible to observe quantum effects. The condensate wave function becomes useful in allowing design and measurement of macroscopic quantum effects. Similar to the discrete atomic energy levels in the Bohr model, only discrete numbers of magnetic flux quanta can penetrate a superconducting loop. In both cases, quantization results from complex amplitude continuity. Differing from microscopic implementations of quantum computers (such as atoms or photons), parameters of superconducting circuits are designed by setting (classical) values to the electrical elements composing them such as by adjusting capacitance or inductance.\nTo obtain a quantum mechanical description of an electrical circuit, a few steps are required. Firstly, all electrical elements must be described by the condensate wave function amplitude and phase rather than by closely related macroscopic current and voltage descriptions used for classical circuits. For instance, the square of the wave function amplitude at any arbitrary point in space corresponds to the probability of finding a charge carrier there. Therefore, the squared amplitude corresponds to a classical charge distribution. The second requirement to obtain a quantum mechanical description of an electrical circuit is that generalized Kirchhoff's circuit laws are applied at every node of the circuit network to obtain the system's equations of motion. Finally, these equations of motion must be reformulated to Lagrangian mechanics such that a quantum Hamiltonian is derived describing the total energy of the system.\n\n\n== Technology ==\n\n\n=== Manufacturing ===\nSuperconducting quantum computing devices are typically designed in the radio-frequency spectrum, cooled in dilution refrigerators below 15 mK and addressed with conventional electronic instruments, e.g. frequency synthesizers and spectrum analyzers. Typical dimensions fall on the range of micrometers, with sub-micrometer resolution, allowing for the convenient design of a Hamiltonian system with well-established integrated circuit technology. Manufacturing superconducting qubits follows a process involving lithography, depositing of metal, etching, and controlled oxidation as described in. Manufacturers continue to improve the lifetime of superconducting qubits and have made significant improvements since the early 2000s.\n\n\n=== Josephson junctions ===\n\nOne distinguishable attribute of superconducting quantum circuits is the use of Josephson junctions. Josephson junctions are an electrical element which does not exist in normal conductors. Recall that a junction is a weak connection between two leads of wire (in this case a superconductive wire) on either side of a thin layer of insulator material only a few atoms thick, usually implemented using shadow evaporation technique. The resulting Josephson junction device exhibits the Josephson Effect whereby the junction produces a supercurrent. An image of a single Josephson junction is shown to the right. The condensate wave function on the two sides of the junction are weakly correlated, meaning that they are allowed to have different superconducting phases. This distinction of nonlinearity contrasts continuous superconducting wire for which the wave function across the junction must be continuous. Current flow through the junction occurs by quantum tunneling, seeming to instantaneously \"tunnel\" from one side of the junction to the other. This tunneling phenomenon is unique to quantum systems. Thus, quantum tunneling is used to create nonlinear inductance, essential for qubit design as it allows a design of anharmonic oscillators for which energy levels are discretized (or quantized) with nonuniform spacing between energy levels, denoted \n  \n    \n      \n        Δ\n        E\n      \n    \n    {\\displaystyle \\Delta E}\n  \n. In contrast, the quantum harmonic oscillator cannot be used as a qubit as there is no way to address only two of its states, given that the spacing between every energy level and the next is exactly the same.\n\n\n== Qubit archetypes ==\nThe three primary superconducting qubit archetypes are the phase, charge and flux qubit. Many hybridizations of these archetypes exist including the fluxonium, transmon, Xmon, and quantronium. For any qubit implementation the logical quantum states \n  \n    \n      \n        {\n        \n          |\n        \n        0\n        ⟩\n        ,\n        \n          |\n        \n        1\n        ⟩\n        }\n      \n    \n    {\\displaystyle \\{|0\\rangle ,|1\\rangle \\}}\n  \n are mapped to different states of the physical system (typically to discrete energy levels or their quantum superpositions). Each of the three archetypes possess a distinct range of Josephson energy to charging energy ratio. Josephson energy refers to the energy stored in Josephson junctions when current passes through, and charging energy is the energy required for one Cooper pair to charge the junction's total capacitance. Josephson energy can be written as \n\n  \n    \n      \n        \n          U\n          \n            j\n          \n        \n        =\n        −\n        \n          \n            \n              \n                I\n\n---\n",
  "arxiv_docs": "Index: 1\nTitle: The Rise of Quantum Internet Computing\nPublished: 2022-08-01\nAuthors: Seng W. Loke\nSource: Arxiv research paper\nContent: arXiv:2208.00733v1  [cs.ET]  1 Aug 2022\nIEEE IOT MAGAZINE, VOL. XX, NO. X, X 2022\n1\nThe Rise of Quantum Internet Computing\nSeng W. Loke, Member, IEEE\nAbstract—This article highlights quantum Internet computing as referring to distributed quantum computing over the quantum Internet,\nanalogous to (classical) Internet computing involving (classical) distributed computing over the (classical) Internet. Relevant to\nquantum Internet computing would be areas of study such as quantum protocols for distributed nodes using quantum information for\ncomputations, quantum cloud computing, delegated veriﬁable blind or private computing, non-local gates, and distributed quantum\napplications, over Internet-scale distances.\nIndex Terms—quantum Internet computing, quantum Internet, distributed quantum computing, Internet computing, distributed\nsystems, Internet\n”This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this\nversion may no longer be accessible.”\n✦\n1\nINTRODUCTION\nT\nHERE have been tremendous developments in quantum\ncomputing, quantum cryptography, quantum commu-\nnications and the quantum Internet, and we have seen\nincreased investments and intensive research in quantum\ncomputing in recent years [1], [2]. The quantum Internet will\nnot necessarily replace the (classical) Internet we know and\nuse today, at least not in the near future, but can complement\nthe current Internet. The quantum Internet aims to enable\nrobust quantum teleportation (or transmission) of qubits,1\nand entanglement among qubits,2 over long Internet-scale\ndistances, which are key to many of the quantum protocols\nincluding quantum key distribution, quantum voting, and\nothers, as well as for non-local control of quantum gates.\nThere have been efforts to build quantum computers,\nand it remains to see if any one paradigm becomes the\ndominant or best way of building such quantum comput-\ners. At the same time, even as researchers develop more\npowerful quantum computers (supporting more qubits for\noperations, and at lower error rates), there is an opportunity\nfor connecting multiple quantum computers from differ-\nent sites to achieve much more complex quantum com-\nputations, i.e., inter-linking multiple quantum computers\non different sites to perform distributed computing with\na distributed system of quantum computers (or quantum\nprocessing units (QPUs) at different nodes), arriving at the\nnotion of distributed quantum computing, e.g., [3].\nWhile distributed quantum computing can involve mul-\ntiple QPUs next to each other or at the same site, with the\nquantum Internet, one can envision distributed quantum\n•\nSeng W. Loke is with the School of Information Technology, Deakin\nUniversity, Melbourne, Australia.\nE-mail: see https://www.deakin.edu.au/about-deakin/people/seng-loke.\nManuscript received X XX, 20XX; revised X XX, 20XX.\n1. A qubit is the basic unit of quantum information, and can be\nthought of as a two-state, or two-levelled, quantum-mechanical system,\nsuch as an electron’s spin, where the two levels are spin up and spin\ndown, or a photon’s polarization, where the two states are the vertical\npolarization and the horizontal polarization.\n2. Multiple qubits at different sites can share an entangled state, a\nsuperpositon of “specially correlated” states, to be used in distributed\nalgorithms.\ncomputing over nodes geographically far apart. As noted\nin [4], the idea is the quantum Internet as the “underly-\ning infrastructure of the Distributed Quantum Computing\necosystem.”\nThis article highlights the emerging area of distributed\nquantum computing over the quantum Internet, which we\nrefer to as quantum Internet computing, i.e., the idea of com-\nputing using quantumly connected distributed quantum\ncomputers over Internet-scale distances. Hence, quantum\nInternet computing is not a new concept in itself but a\nproposed “umbrella term” used here for the collection of\ntopics (listed below), from an analogy to (classical) Internet\ncomputing.\nInternet computing, where one does distributed comput-\ning but over Internet-scale distances and distributed sys-\ntems involve nodes connected via the Internet, is at the inter-\nsection of work in (classical) distributed computing and the\n(classical) Internet. Analogous to Internet computing, one\ncould ask the question of what would be at the intersection\nof work in distributed quantum computing and work on the\nquantum Internet, which brings us to the notion of quantum\nInternet computing.\nAlso, while the quantum Internet and distributed quan-\ntum computing are still nascent research areas, there are at\nleast three key topics which can be considered as relevant to\nquantum Internet computing:\n•\ndistributed quantum computing, including quantum\nprotocols from theoretical perspectives involving\ncommunication complexity studies, and distributed\nquantum computing via non-local or distributed\nquantum gates,\n•\nquantum cloud computing with a focus on delegat-\ning quantum computations, blind quantum comput-\ning, and verifying delegated quantum computations,\nand\n•\ncomputations and algorithms for the quantum Inter-\nnet including key ideas such as quantum entangle-\nment distillation, entanglement swapping, quantum\nIEEE IOT MAGAZINE, VOL. XX, NO. X, X 2022\n2\nrepeaters, and quantum Internet standards.3\nWe brieﬂy discuss the above topics in the following sections.\n2\nDISTRIBUTED QUANTUM COMPUTING\nDistributed quantum computing problems and quantum\nprotocols have been well-studied for over two decades,\nfrom a theoretical computer science perspective,4 many of\nwhich have their inspiration from classical distributed com-\nputing research. Quantum versions of classical distributed\ncomputing problems and protocols, and new forms of dis-\ntributed computing using quantum information, have been\nexplored, e.g., the distributed three-party product problem,\nthe distributed Deutsch-Jozsa promise problem and the\ndistributed intersection problem, demonstrating how, for\nsome problems, quantum information can enable fewer\nbits of communication to be used for a solution, and how\ncertain distributed computation problems can be solved\nwith quantum information, but cannot be solved classically.\nMany quantum protocols, including quantum coin ﬂipping,\nquantum leader election, quantum anonymous broadcast-\ning, quantum voting, quantum Byzantine Generals, quan-\ntum secret sharing, and quantum oblivious transfer, can\nbe viewed as “quantum versions” of classical distributed\ncomputing problems, and have been studied extensively.\nAnother area of study, which has also been considered\nas distributed quantum computing, is non-local gates, or\nthe non-local control of quantum gates, including early\nwork nearly over two decades ago.5 Key to performing\nsuch non-local control of quantum gates is the use of en-\ntanglement, which can be viewed as a resource for such\nnon-local computations. More recent work has looked at\nhow to partition the computations of distributed quantum\ncircuits over multiple QPUs, e.g., [3] as we mentioned earlier\n- with considerations including distributing computations\nin such a way as to optimize performance and to reduce the\nrequirements on entanglement, since if the entanglements\nrequired are generated at too low a rate, this will hold up\ncomputations. The key motivation here is to inter-link a\nset of quantum computers to form effectively a much more\npowerful quantum computer.\n3\nQUANTUM CLOUD COMPUTING AND DELEGAT-\nING QUANTUM COMPUTATIONS\nWe have seen big tech companies and startups offering\nquantum computing as a service similar to accessing other\ncloud service offerings, which is a fantastic resource for\nexperimentation and studies.\nMore formally, studies into delegating quantum com-\nputation from a client (which can be either classical, or\nalmost classical, i.e., with minimal capability to perform\n3. For example, see https://www.ietf.org/archive/id/draft-irtf-qirg-principles-10.html\n[last accessed: 1/8/2022]\n4. For example, see Buhrman and R¨ohrig’s paper dating back to\n2003: https://link.springer.com/chapter/10.1007/978-3-540-45138-9 1\n[last accessed: 1/8/2022]\n5. For example, see the work by Yimsiriwattana and Lomonaco\nJr.\nin\nhttps://arxiv.org/pdf/quant-ph/0402148.pdf\nand\na\ndistributed\nversion\nof\nShor’s\nfamous\nfactorization\nalgorithm\nhttps://arxiv.org/abs/2207.05976 [last accessed: 1/8/2022]\ncomputations such as teleporting qubits, applying simple\nPauli quantum operations, and doing basic measurements)\nwhich is much more restricted than the server (assumed\nto be a universal quantum computer) have been studied,\ncalled delegated quantum computing. And when the server\nis prevented from knowing the client’s inputs but still can\nperform delegated computations, by a technique such as\nthe quantum one-time pad (where the client applies Pauli\noperations to add uncertainty from the server’s perspective,\nthereby effectively encrypting the quantum inputs it sends\nto the server, and keeps track of operations it later needs\nto decrypt the outputs from the server), this is called blind\nquantum computing.\nIn order to be sure that the server does indeed perform\nthe required quantum operations delegated to it by the\nclient, the client can embed tests (or test runs) into the\ndelegated computations, so that the server (not being able\nto distinguish between tests and normal computations) can\nbe caught out if it did not perform the required compu-\ntations properly. That is, the client can verify if the server\nperformed the required quantum computations.6 Further\nabstractions for delegating quantum computations with\nsupporting cloud services continues to be investigated.\n4\nTHE QUANTUM INTERNET\nAs we mentioned earlier, work on the quantum Internet\nfocuses on how to efﬁciently enable robust entanglement\nshared among qubits over long geographical distances. If\ntwo nodes in different continents share entangled states,\nthen, this can be a resource to do non-local gates, i.e.,\nto perform distributed quantum computations, and enable\nquantum protocols over Internet-scale distances.\nThere have been the use of satellites to enable long dis-\ntance entanglement, as well as the use of optical ﬁbre cables\nto demonstrate entanglement. Key to the quantum Internet\nare ideas such as entanglement swapping and quantum\nrepeaters, including ideas such quantum distillation, to\nachieve high ﬁdelity distributed entangled states over long\ndistances, and quantum error correction - this continues to\nbe a research endeavour as mentioned earlier [2].\nThere are other interesting distributed quantum appli-\ncations to be considered including quantum cryptography,\nquantum sensing, and quantum positioning systems.\n5\nDISTRIBUTED QUANTUM COMPUTING OVER THE\nQUANTUM INTERNET: QUANTUM INTERNET COM-\nPUTING AND THE QUANTUM IOT?\nApart from the many quantum computers available over\nthe cloud by big tech and startups which work at very\nlow temperatures, room temperature quantum computers\nhave also started to emerge.7 This could pave the way\nfor quantum computers at the fog and at the edge, not\njust in remote clouds, and perhaps even mobile quantum\n6. An\nexcellent\nexample\nis\nthe\nwork\nby\nBroadbent\nat\nhttps://theoryofcomputing.org/articles/v014a011/\n[last\naccessed:\n1/8/2022]\n7. See https://spectrum.ieee.org/nitrogen-vacancy-diamond-quantum-computer-\nand also https://otd.harvard.edu/explore-innovation/technologies/scalable-room-t\n[last accessed: 1/8/2022]\nIEEE IOT MAGAZINE, VOL. XX, NO. X, X 2022\n3\ncomputers, or quantum computers embedded into every-\nday devices and objects, if ever! Will we then have the\nquantum Internet of Things (IoT)? The answer remains to\nbe seen, and “quantum entangled things across the world”\nwill likely complement the classical IoT. Future applications\nand potential of quantum Internet computing remains to\nbe investigated. Meanwhile, others have begun to look at\nthe connection between 6G networking and the quantum\nInternet [5].\nREFERENCES\n[1] W.\nKozlowski\nand\nS.\nWehner,\n“Towards\nlarge-scale\nq\n\n---\n\nIndex: 2\nTitle: Unconventional Quantum Computing Devices\nPublished: 2000-03-31\nAuthors: Seth Lloyd\nSource: Arxiv research paper\nContent: arXiv:quant-ph/0003151v1  31 Mar 2000\nUnconventional Quantum Computing Devices\nSeth Lloyd\nMechanical Engineering\nMIT 3-160\nCambridge, Mass. 02139\nAbstract: This paper investigates a variety of unconventional quantum computation de-\nvices, including fermionic quantum computers and computers that exploit nonlinear quan-\ntum mechanics. It is shown that unconventional quantum computing devices can in prin-\nciple compute some quantities more rapidly than ‘conventional’ quantum computers.\nComputers are physical: what they can and cannot do is determined by the laws\nof physics. When scientiﬁc progress augments or revises those laws, our picture of what\ncomputers can do changes. Currently, quantum mechanics is generally accepted as the\nfundamental dynamical theory of how physical systems behave. Quantum computers can\nin principle exploit quantum coherence to perform computational tasks that classical com-\nputers cannot [1-21]. If someday quantum mechanics should turn out to be incomplete\nor faulty, then our picture of what computers can do will change. In addition, the set\nof known quantum phenomena is constantly increasing: essentially any coherent quantum\nphenomenon involving nonlinear interactions between quantum degrees of freedom can\nin principle be exploited to perform quantum logic. This paper discusses how the revi-\nsion of fundamental laws and the discovery of new quantum phenomena can lead to new\ntechnologies and algorithms for quantum computers.\nSince new quantum eﬀects are discovered seemingly every day, let’s ﬁrst discuss two\nbasic tests that a phenomenon must pass to be able to function as a basis for quantum\ncomputation. These are 1) The phenomenon must be nonlinear, and 2) It must be coherent.\nTo support quantum logic, the phenomenon must involve some form of nonlinearity, e.g.,\na nonlinear interaction between quantum degrees of freedom. Without such a nonlinearity\nquantum devices, like linear classical devices, cannot perform even so simple a nonlinear\noperation as an AND gate.\nQuantum coherence is a prerequisite for performing tasks\nsuch as factoring using Shor’s algorithm [10], quantum simulation a la Feynman [11] and\nLloyd [12], or Grover’s data-base search algorithm [13], all of which require extended\nmanipulations of coherent quantum superpositions.\n1\nThe requirements of nonlinearity and coherence are not only necessary for a phe-\nnomenon to support quantum computation, they are also in principle suﬃcient. As shown\nin [14-15], essentially any nonlinear interaction between quantum degrees of freedom suf-\nﬁces to construct universal quantum logic gates that can be assembled into a quantum\ncomputer. In addition, the work of Preskill et al. [18] on robust quantum computation\nshows that an error rate of no more than 10−4 per quantum logic operation allows one to\nperform arbitrarily long quantum computations in principle.\nIn practice, of course, few if any quantum phenomena are likely to prove suﬃciently\ncontrollable to provide extended quantum computation. Promising devices under current\nexperimental investigation include ion traps [5,7], high ﬁnesse cavities for manipulating\nlight and atoms using quantum electrodynamics [6], and molecular systems that can be\nmade to compute using nuclear magnetic resonance [8-9]. Such devices store quantum\ninformation on the states of quantum systems such as photons, atoms, or nuclei, and\naccomplish quantum logic by manipulating the interactions between the systems via the\napplication of semiclassical potentials such as microwave or laser ﬁelds. We will call such\ndevices ‘conventional’ quantum computers, if only because such devices have actually been\nconstructed.\nThere is another sense in which such computers are conventional: although the de-\nvices described above have already been used to explore new regimes in physics and to\ncreate and investigate the properties of new and exotic quantum states of matter, they\nfunction according to well established and well understood laws of physics. Perhaps the\nmost striking examples of the ‘conventionality’ of current quantum logic devices are NMR\nquantum microprocessors that are operated using techniques that have been reﬁned for\nalmost half a century. Ion-trap and quantum electrodynamic quantum computers, though\ncertainly cutting edge devices, operate in a quantum electrodynamic regime where the\nfundamental physics has been understood for decades (that is not to say that new and\nunexpected physics does not arise frequently in this regime, rather that there is general\nagreement on how to model the dynamics of such devices).\nMake no mistake about it: a conventional quantum logic device is the best kind of\nquantum logic device to have around. It is exactly because the physics of nuclear magnetic\nresonance and quantum electrodynamics are well understood that devices based on this\nphysics can be used systematically to construct and manipulate the exotic quantum states\nthat form the basis for quantum computation.\nWith that recognition, let us turn to\n2\n‘unconventional’ quantum computers.\nPerhaps the most obvious basis for an unconventional quantum computer is the use\nof particles with non-Boltzmann statistics in a reﬁme where these statistics play a key role\nin the dynamics of the device. For example, Lloyd [16] has proposed the use of fermions\nas the fundamental carriers of quantum information, so that a site or state occupied by a\nfermion represents a 1 and an unoccupied site or state represents a 0. It is straightforward\nto design a universal quantum computer using a conditional hopping dynamics on an array\nof sites, in which a fermion hops from one site to another if only if other sites are occupied.\nIf the array is one-dimensional, then such a fermionic quantum computer is equivalent\nto a conventional quantum computer via the well-known technique of bosonization. If the\narray is two or more dimensional, however, a local operation involving fermions on the\nlattice cannot be mocked up by a local operation on a conventional quantum computer,\nwhich must explicitly keep track of the phases induced by Fermi statistics. As a result,\nsuch a fermionic computer can perform certain operations more rapidly than a conventional\nquantum computer. An obvious example of a problem that can be solved more rapidly on\na fermionic quantum computer is the problem of simulating a lattice fermionic system in\ntwo or more dimensions. To get the antisymmetrization right in second quantized form,\na conventional ‘Boltzmann’ quantum computer takes time proportional to Tℓd−1 where T\nis the time over which the simulation is to take place, ℓis the length of the lattice and\nd is the dimension, while a fermionic quantum computer takes time proportional to T.\n(Here we assume that the computations for both conventional and Fermionic quantum\ncomputers can take advantage of the intrinsic parallelizability of such simulations: if the\ncomputations are performed serially an additional factro of ℓd is required for both types\nof computer to update each site sequentially.)\nAs the lattice size ℓand the dimension d grow large, the diﬀerence between the two\ntypes of computer also grows large. Indeed, the problem of simulating fermions hopping\non a hypercube of dimension d as d →∞is evidently exponentially harder on a con-\nventional quantum computer than a Fermionic quantum computer.\nSince a variety of\ndiﬃcult problems such as the travelling-salesman problem and data-base search problem\ncan be mapped to particles hopping on a hypercube, it is interesting to speculate whether\nfermionic computers might provide an exponential speed-up on problems of interest in ad-\ndition to quantum simulation. No such problems are currently known, however. Fermionic\ncomputers could be realized in principle by manipulating the ways in which electrons and\n3\nholes hop from site to site on a semiconductor lattice (though problems of decoherence are\nlikely to be relatively severe for such systems).\nIt might also be possible to construct bosonic computers using photons, phonons, or\natoms in a Bose-Einstein condensate. Such systems can be highly coherent and support\nnonlinear interactions: phonons and photons can interact in a nonlinear fshion via their\ncommon nonlinear interaction with matter, and atoms in a Bose condensate can be made\nto interact bia quantum electrodynamics (by introduction of a cavity) or by collisions. So\nfar, however, the feature of Bose condensates that makes them so interesting from the point\nof view of physics — all particles in the same state — makes them less interesting from the\npoint of view of quantum computation. Many particles in the same state, which can be\nmanipulated coherently by a variety of techniques, explore the same volume of Hilbert space\nas a single particle in that state. As a result, it is unclear how such a bosonic system could\nprovide a speed-up over conventional quantum computation. More promising than Bose\ncondensates from the perspective of quantum computation and quantum communications,\nis the use of cavity quantum electrodynamics to ‘dial up’ or synthesize arbitrary states\nof the cavity ﬁeld. Such a use of bosonic states is important for the ﬁeld of quantum\ncommunications, which requires the ability to create and manipulate entangled states of\nthe electromagnetic ﬁeld.\nA third unconventional design for a quantum computer relies on ‘exotic’ statistics\nthat are neither fermionic nor bosonic. Kitaev has recently proposed a quantum computer\narchitecture based on ‘anyons,’ particles that when exchanged acquuire an arbitrary phase.\nExamples of anyons include two-dimensional topological defects in lattice systems of spins\nwith various symmetries. Kitaev noted that such anyons could perform quantum logic via\nAharonov-Bohm type interactions [19]. Preskill et al. have shown explicitly how anyonic\nsystems could compute in principle [20], and Lloyd et al.\nhave proposed methods of\nrealizing anyons using superconducting circuits (they could also in principle be constructed\nusing NMR quantum computers to mock up the anyonic dynamics in an eﬀectively two-\ndimensional space of spins) [21]. The advantage of using anyons for quantum computation\nis that their nonlocal topological nature can make them intrinsically error-correcting and\nvirtually immune to the eﬀects of noise and interference.\nAs the technologies of the microscale become better developed, more and more po-\ntential designs for quantum computers, both conventional and unconventional, are likely\nto arise. Additional technologies that could prove useful for the construction of quantum\n4\nlogic devices include photonic crystals, optical hole-burning techniques, electron spin res-\nonance, quantum dots, superconducting circuits in the quantum regime, etc. Since every\nquantum degree of freedom can in principle participate in a computation one cannot a\npriori rule out the possibility of using currently hard to control degrees of freedom such as\nquark and gluon in complex nuclei to process information. Needless to say, most if not all\nof the designs inspired by these technologies are likely to fail. There is room for optimism\nthat some such quantum computer designs will prove practicable, however.\nThe preceding unconventional designs for quantum computers were based on existing,\nexperimentally conﬁrmed physical phenomena (except in the case of non-abelian anyons).\nLet us now turn to designs based on speculative, hypothetical, and not yet veriﬁed phenom-\nena. (One of the most interesting of these phenomena is large-scale quantum computation\nitself: can we create and systematically transform entangled states involving hundreds or\nthousands of quantum variables?) A particularly powerful hypothesis from the point of\nview of quantum computation is that of nonlinear quantum mechanics.\nThe conventional picture of quantum mechanics is that it is linear in the sense that the\nsuperposition principle is obeyed exactly. (Of course, quantum systems can still exhibit\nnonlinear interaction\n\n---\n\nIndex: 3\nTitle: Geometrical perspective on quantum states and quantum computation\nPublished: 2013-11-20\nAuthors: Zeqian Chen\nSource: Arxiv research paper\nContent: arXiv:1311.4939v1  [quant-ph]  20 Nov 2013\nGeometrical perspective on quantum states and quantum computation\nZeqian Chen\nState Key Laboratory of Resonances and Atomic and Molecular Physics,\nWuhan Institute of Physics and Mathematics, Chinese Academy of Sciences,\n30 West District, Xiao-Hong-Shan, Wuhan 430071, China\nWe interpret quantum computing as a geometric evolution process by reformulating ﬁnite quantum\nsystems via Connes’ noncommutative geometry. In this formulation, quantum states are represented\nas noncommutative connections, while gauge transformations on the connections play a role of\nunitary quantum operations. Thereby, a geometrical model for quantum computation is presented,\nwhich is equivalent to the quantum circuit model. This result shows a geometric way of realizing\nquantum computing and as such, provides an alternative proposal of building a quantum computer.\nPACS numbers: 03.67.Lx, 03.65.Aa\nQuantum computation has the advantage of solving\neﬃciently some problems that are considered intractable\nby using conventional classical computation [1]. In this\ncontext, there are two remarkable algorithms found:\nShor’s factoring algorithm [2] and Grove’s search algo-\nrithm [3].\nBut it remains a challenge to ﬁnd eﬃcient\nquantum circuits that can perform these complicated\ntasks in practice, due to quantum decoherence. A cru-\ncial step in the theory of quantum computer has been\nthe discovery of error-correcting quantum codes [4] and\nfault-tolerant quantum computation [5, 6], which estab-\nlished a threshold theorem that proves that quantum de-\ncoherence can be corrected as long as the decoherence is\nsuﬃciently weak. To tackle this barrier, a revolutionary\nstrategy, topological quantum computation (see [7] and\nreferences therein), is to make the system immune to the\nusual sources of quantum decoherence, by involving the\nglobally robust topological nature of the computation.\nRecently, substantial progress in this ﬁeld has been made\non both theoretical and experimental fronts [8].\nIn this paper, we provide an alternative approach to\nquantum computation from a geometrical view of point.\nTo this end, we need to reformulate quantum mechanics\nvia Connes’ noncommutative geometry [9]. In this for-\nmulation, quantum states are represented as noncommu-\ntative connections, while gauge transformations on the\nconnections play a role of unitary quantum operations.\nIn this way, we present a geometrical model for quan-\ntum computation, which is equivalent to the quantum\ncircuit model. In this computational model, information\nis encoded in gauge states instead of quantum states and\nimplementing on gauge states is played by gauge transfor-\nmations. Therefore, our scheme shows a geometric way\nof realizing quantum computing and as such, provides an\nalternative proposal of building a quantum computer.\nLet H be a N dimensional Hilbert space associated\nwith a ﬁnite quantum system. Let A be the algebra of\nall (bounded) linear operators on H, and let U(A) = {u ∈\nA : uu∗= u∗u = I} with I being the unit operator on\nH. Given a selfadjoint operator D on H, (A, H, D) is a\nspectral triple in the sense of noncommutative geometry\n[9, 10]. A (noncommutative) connection on (A, H, D) is\ndeﬁned to be a selfadjoint operator V on H of the form\nthat follows\nV =\nX\nj\naj[D, bj]\n(1)\nwhere aj, bj ∈A and [a, b] = ab −ba. A gauge transform\non a connection V under u ∈U(A) is deﬁned as\nV 7−→Gu(V ) = uV u∗+ u[D, u∗].\n(2)\nFor avoiding triviality, we always assume that D ̸= 0 or\nI in what follows.\nFor any (pure) quantum state |ψ⟩⟨ψ| with ψ being a\nunit vector in H, we have\n|ψ⟩⟨ψ| = |ψ⟩⟨ϕ|i[D, b]|ϕ⟩⟨ψ|\nwhere i = √−1 and, b is a selfjoint operator on H such\nthat i[D, b] has eigenvalue 1 at |ϕ⟩. Such a selfjoint oper-\nator b always exists because D ̸= 0 or I. In this case,\n|ψ⟩⟨ψ| = ia∗[D, ba] −ia∗b[D, a]\n(3)\nwith a = |ϕ⟩⟨ψ|. Thus, every quantum state |ψ⟩⟨ψ| can\nbe represented as a connection, denoted by Vψ, i.e.,\nVψ = ia∗[D, ba] −ia∗b[D, a].\n(4)\nLet GD(H) be the set of all connections V which can\nbe written as V = Vψ + uDu∗−D with ψ being a unit\nvector in H and u ∈U(A). An element in GD(H) is said\nto be a gauge state on (A, H, D). Any quantum state is\nnecessarily a gauge state, but a gauge state need not to\nbe a quantum state. However, any gauge state V can be\nobtained from a quantum state by performing a gauge\ntransform. Indeed, if V = Vψ + uDu∗−D then V =\nGu(Vu∗ψ). Moreover, for any gauge state V on (A, H, D)\nwe have (see [11])\n• for any u ∈U(A), Gu(V ) is again a gauge state;\n• Guv(V ) = Gu(Gv(V )) for all u, v ∈U(A).\nTherefore, a gauge transform preserves gauge states.\nLet V be a gauge state which is prepared from a quan-\ntum state |ψ⟩⟨ψ| by operating a gauge transform Gu, i.e.,\n2\nV = Gu(Vψ). For any event E, the probability of E oc-\ncurring on V is\n⟨E⟩V = ⟨ψ|u∗Eu|ψ⟩.\n(5)\nNote that a gauge state may be prepared in several ways.\nHence, the probability of a event E occurring on a gauge\nstate V depends on the quantum state from which V is\nprepared.\nLet H be a selfadjoint operator on H. Assuming ut =\neitH for t ∈R, we have that the gauge transforms Vt =\nGt(V ) on a ﬁxed gauge state V under ut form a group\n(see [11]), that is,\nGt+s(V ) = Gt(Gs(V )).\n(6)\nThis yields a dynamical equation governed by the Hamil-\ntonian H for gauge states on (A, H, D) as follows [12]\nidVt\ndt = [Vt, H] + [D, H]\n(7)\nwith V0 = V. In particular, for a unit vector ψ we have\nVt = Gt(Vψ) = Vutψ + utDu∗\nt −D.\n(8)\nWe now turn to product of two spectral triples. Sup-\npose (Ai, Hi, Di), i = 1, 2, are two spectral triple associ-\nated with ﬁnite quantum systems. Put\nD = D1 ⊗I2 + I1 ⊗D2\n(9)\nwith Ii being the unit operator on Hi (i = 1, 2). Then\nD is a selfjoint operator on H1 ⊗H2. The spectral triple\n(A1⊗A2, H1⊗H2, D) is called the product of two spectral\ntriples (Ai, Hi, Di), i = 1, 2.\nNow we illustrate our scheme by using a qubit. Let\nH = C2 and\nσx =\n\u0014\n0 1\n1 0\n\u0015\n, σy =\n\u0014\n0 −i\ni\n0\n\u0015\n, σz =\n\u0014\n1\n0\n0 −1\n\u0015\n.\n(10)\nThen (M2, C2, D) is a spectral triple with D = σx, where\nM2 is the set of all 2×2 complex matrices. For |0⟩=\n\u0014\n1\n0\n\u0015\n,\nwe have\nV|0⟩=\n\u0014\n1 0\n0 0\n\u0015\n,\nGσx(V|0⟩) =\n\u0014\n0 0\n0 1\n\u0015\n,\nand\nGσy(V|0⟩) =\n\u0014\n0\n−2\n−2\n1\n\u0015\n,\nGσz(V|0⟩) =\n\u0014\n1\n−2\n−2\n0\n\u0015\n.\nFor |1⟩=\n\u0014\n0\n1\n\u0015\n, we have\nV|1⟩=\n\u0014\n0 0\n0 1\n\u0015\nand\nGσy(V|1⟩) =\n\u0014\n1\n−2\n−2\n0\n\u0015\n.\nHence Gσy(V|1⟩) = Gσz(V|0⟩) and so, the gauge state\nV =\n\u0014\n1\n−2\n−2\n0\n\u0015\ncan be prepared in two diﬀerent ways.\nWe are now ready to interpret quantum computation\nfrom a geometrical view of point.\nBut let us take a\nstep backward and discuss the standard quantum circuit\nmodel for computation [13]. Let H = (C2)⊗n, the tensor\nproduct of n copies of C2. A quantum circuit model on\nn qubits consists of\n• a initial state |ψ⟩, represented by a unit vector ψ ∈\nH;\n• a quantum circuit Γ = UNUN−1 · · · U1, where quan-\ntum “gates” Uk 1 ≤k ≤N, are unitary transfor-\nmations on either C2\ni or C2\ni ⊗C2\nj, 1 ≤i, j ≤n, the\nidentity on all remaining factors;\n• reading the output of the circuit Γ|ψ⟩by measuring\nthe ﬁrst qubit; the probability of observing |1⟩is\nP(Γ) = ⟨ψ|Γ∗Π1Γ|ψ⟩, where Π1 = |1⟩⟨1| ⊗I · · · ⊗I\nis the projection to |1⟩in the ﬁrst qubit.\nLet A = M2n. Put\nD =\nn\nX\ni=1\nI ⊗· · · ⊗I\n|\n{z\n}\ni−1\n⊗σx ⊗I · · · ⊗I\nwhere I is the identity on C2. A computational model\nbased on the spectral triple (A, H, D) is as follows:\n• Initialization of a gauge state Vψ in the spectral\ntriple (A, H, D), where ψ is a unit vector in H;\n• Gauge implementation of the computational pro-\ngram\nG(Γ) = GUN GUN−1 · · · GU1\nwhere “gates” GUk, 1 ≤k ≤N, are gauge transfor-\nmations induced by Uk;\n• Application of the projection operator Π1 for read-\ning the output of the computation G(Γ)(Vψ);\nthe probability of observing |1⟩is P(GΓ)\n=\n⟨ψ|Γ∗Π1Γ|ψ⟩because G(Γ)(Vψ) = GΓ(Vψ) (see\n[11]), i.e., G(Γ)(Vψ) = Γ|ψ⟩⟨ψ|Γ∗+ ΓDΓ∗−D.\nThus, we obtain a geometrical model on n qubits for\nquantum computation, which is evidently equivalent to\nthe quantum circuit model as described above. Due to\nthe essential role of gauge transformations played in this\ncomputational model, we call this scheme gauge quantum\ncomputation.\nAs illustration, we give the Deutsch-Jozsa algorithm\n[14] in gauge quantum computation. Let f : {0, 1}n 7→\n{0, 1} be a function that takes an n-bit into a bit. We\ncall f balanced if f(x) = 1 for exactly half of all possible\n3\nx and f(x) = 0 for the other half.\nGiven a function\nf that is either constant or balanced, we want to ﬁnd\nout which it is with certainty. More precisely, we select\none x ∈{0, 1}n and calculate f(x) with the result being\neither 0 or 1. What is the fewest number of queries that\nwe can make to determine whether or not f is constant?\nIn the classical case, at worst we will need to calculate f\n2n−1 + 1 times, because we may ﬁrst obtain 2n−1 zeros\nand will need one more query to decide.\nHowever, in\nthe setting of quantum computation we could achieve the\ngoal in just one query using the Deutsch-Jozsa algorithm.\nIn the sequel, we give a realization of the Deutsch-Jozsa\nalgorithm in gauge quantum computation.\nLet H = (C2)⊗(n+1) and A = M2n+1. Given a selfad-\njoint operator D on H that is not 0 or I, we get the desired\nspectral triple (A, H, D). For a given f, we deﬁne the as-\nsociated operator Uf on H as Uf|x, y⟩= |x, y ⊕f(x)⟩for\nx ∈{0, 1}n and y ∈{0, 1}. Recall that the Hadamard\noperator H on C2 is\nH =\n1\n√\n2\nX\nx,y∈{0,1}\n(−1)x·y|x⟩⟨y|\nwhere x·y signiﬁes ordinary multiplication. The following\nis the Deutsch-Jozsa algorithm in the setting of gauge\nquantum computation:\n• Initialization of a gauge state Vψ with ψ = |0⟩⊗n ⊗\n|1⟩;\n• Gauge implementation of the computational pro-\ngram G(Γ) = GH⊗n⊗IGUf GH⊗(n+1);\n• Application of the projection operator Π|0⟩⊗n for\nreading the output of the computation G(Γ)(Vψ),\nwhere Π|0⟩⊗n is the projection to |0⟩⊗n in the ﬁrst\nn qubits.\nThe ﬁnal gauge state is V = VΓψ + ΓDΓ∗−D with Γ =\n(H⊗n ⊗I)UfH⊗(n+1), where\nΓψ =\nX\nx,y∈{0,1}n\n(−1)x·y+f(x)\n2n\n|y⟩⊗|0⟩−|1⟩\n√\n2\n.\nSince the amplitude for the state |0⟩⊗n in the ﬁrst n\nqubits is P\nx(−1)f(x)/2n, the probability of observing 0\nis 1 if f is constant, or 0 if f is balanced. Thus we have\ntwo possibilities of obtaining the outcome zero or the\noutcome nonzero. In the ﬁrst case, f is certainly constant\nand in the second case f must be balanced. Therefore,\nwe only need to perform three times gauge transforms for\ndetermining whether or not f is constant.\nIn conclusion, we present a geometrical description of\nquantum computation via noncommutative geometry. In\nthis geometrical model, information is encoded in gauge\nstates and computational operation is implemented by\ngauge transforms instead of unitary transforms. In prin-\nciple, gauge transforms are easier to perform than uni-\ntary quantum operation [15]. Therefore, gauge quantum\ncomputation should be more accessible than the usual\nquantum circuit computation and as such, this provides\nan alternative proposal of building a quantum computer.\nThis work was supported in part by the NSFC under\nGrant No. 11171338 and National Basic Research Pro-\ngram of China under Grant No. 2012CB922102.\n[1] M. A. Nielsen, I. L. Chuang, Quantum Computation\nand Quantum Information (Cambridge University Press,\nCambridge, 2000).\n[2] P. Shor, Algorithms for quantum computation, discrete\nlogarithms and factoring, Proc. 35th Annual Symposium\non Foundations of Computer Science (IEEE Computer\nSociety Press, Los Alamitos, CA, 1994, 124-134).\n[3] L. Grover, Quantum mechanics helps in search for a nee-\ndle in a haystack, Phys. Rev. Lett. 79 (1997), 325-328.\n[4] P. Shor, Scheme for reducing decoherence in quantum\ncomputer memory, Phys. Rev. A 52 (1995), 2493-2496.\n[5] J. Preskill, Fault-tolerant quantum computation, arXiv:\nquant-ph/9712048, 1997.\n[6] P. Shor, Fault-tolerant quantum computation, Proc. 37th\nAnnual Symposium on Foundations of Computer Science\n(IEEE Computer Society Press, Los Alamitos, CA, 1996,\n56-65).\n[7] C. Nayak, S. H. Simon, A. Stern, M. Freedman, S. Das\nSarma, Non-Abelian anyons and topological quantum\ncomputation, Rev. Mod. Phys. 80 (2008), 1083-1159.\n[8] A.\nStern,\nN.\nH.\nLindner,\nTopological\n\n---\n",
  "knowledge": {
    "topic": "Quantum Computing",
    "introduction": "Overview of the principles, history, implementations, algorithms, architectures and networking of quantum computing — including fundamental quantum information concepts (qubits, superposition, entanglement, measurement), major algorithms and complexity implications, physical implementations (superconducting, ion traps, NMR, topological and unconventional devices), error correction and fault tolerance, and emerging distributed paradigms such as the quantum Internet and delegated/cloud quantum computing.",
    "sources": [
      {
        "id": 1,
        "title": "Quantum computing",
        "authors": [
          "Wikipedia contributors"
        ],
        "source": "Wikipedia",
        "url": "https://en.wikipedia.org/wiki/Quantum_computing"
      },
      {
        "id": 2,
        "title": "Timeline of quantum computing and communication",
        "authors": [
          "Wikipedia contributors"
        ],
        "source": "Wikipedia",
        "url": "https://en.wikipedia.org/wiki/Timeline_of_quantum_computing_and_communication"
      },
      {
        "id": 3,
        "title": "Superconducting quantum computing",
        "authors": [
          "Wikipedia contributors"
        ],
        "source": "Wikipedia",
        "url": "https://en.wikipedia.org/wiki/Superconducting_quantum_computing"
      },
      {
        "id": 4,
        "title": "The Rise of Quantum Internet Computing",
        "authors": [
          "Seng W. Loke"
        ],
        "source": "arXiv",
        "url": "https://arxiv.org/abs/2208.00733"
      },
      {
        "id": 5,
        "title": "Unconventional Quantum Computing Devices",
        "authors": [
          "Seth Lloyd"
        ],
        "source": "arXiv",
        "url": "https://arxiv.org/abs/quant-ph/0003151"
      },
      {
        "id": 6,
        "title": "Geometrical perspective on quantum states and quantum computation",
        "authors": [
          "Zeqian Chen"
        ],
        "source": "arXiv",
        "url": "https://arxiv.org/abs/1311.4939"
      }
    ],
    "topics": [
      {
        "id": "t1",
        "title": "Foundations of quantum information",
        "summary_points": [
          "Qubit: the two-level quantum information unit represented by |0⟩ and |1⟩; general state α|0⟩ + β|1⟩ with complex amplitudes α, β and normalization |α|^2 + |β|^2 = 1 (Born rule links amplitudes to measurement probabilities).",
          "Superposition: linear combination of basis states enables quantum parallelism but yields only probabilistic measurement outcomes; interference of amplitudes is used to amplify desired results.",
          "Entanglement: nonclassical correlations between qubits enabling nonlocal operations and serving as a resource for tasks (teleportation, non-local gates, quantum communication protocols).",
          "Quantum operations: closed-system evolution is unitary (quantum gates); measurement is non-unitary and probabilistic; computations are composed of sequences of unitary gates and measurements.",
          "Quantum vs classical simulation: quantum systems generally require exponentially large classical resources to simulate; this underpins motivation for quantum computers and quantum simulation algorithms."
        ],
        "subtopics": [
          {
            "id": "t1.1",
            "title": "Mathematical formalism",
            "summary_points": [
              "States are vectors in Hilbert space (Dirac notation |ψ⟩); amplitudes are complex and can interfere (constructive/destructive interference used in algorithms).",
              "Operators (matrices) represent gates; linear algebra is the core language for quantum programming and analysis.",
              "Measurement collapses state to an eigenstate with probabilities given by squared moduli of amplitudes (Born rule)."
            ],
            "references": [
              1,
              6
            ]
          },
          {
            "id": "t1.2",
            "title": "Resources and constraints",
            "summary_points": [
              "Entanglement and coherence are key resources; coherence must be preserved long enough to perform computations.",
              "Nonlinearity (in interactions) plus coherence is required to implement universal quantum logic; linear-only phenomena alone cannot realize universal gates."
            ],
            "references": [
              5,
              1
            ]
          }
        ],
        "references": [
          1,
          5,
          6
        ]
      },
      {
        "id": "t2",
        "title": "History and milestones",
        "summary_points": [
          "Early theoretical foundations: Paul Benioff (quantum Turing machine, 1980), Richard Feynman and Yuri Manin (quantum simulation motivation, early 1980s), David Deutsch (universal quantum computer, 1985).",
          "Cryptographic and algorithmic breakthroughs: Bennett & Brassard (quantum cryptography ideas), Simon (exponential oracle separation), Shor (1994 factoring algorithm), Grover (1996 search algorithm).",
          "Experimental progression: first small quantum logic gates and small-scale devices via trapped ions, NMR and superconducting circuits from the 1990s onward; increasing qubit counts and reduced error rates.",
          "Quantum advantage/supremacy claim: Google (2019) 53-qubit sampling experiment sparked debate about the classical hardness and definition of supremacy."
        ],
        "subtopics": [
          {
            "id": "t2.1",
            "title": "Key dates and contributions",
            "summary_points": [
              "1970s–1980s: Holevo bound, reversible computation, Benioff and Toffoli; 1982–1985: no-cloning theorem rediscovery, Deutsch universal machine proposal.",
              "1990s: rapid algorithmic advances (Deutsch–Jozsa, Bernstein–Vazirani, Simon, Shor, Grover) and early experimental realizations (ion traps, NMR).",
              "2000s–2010s: progress on error correction, DiVincenzo criteria, topological ideas (Kitaev), superconducting and ion-trap scaling, demonstrations of multi-qubit processors."
            ],
            "references": [
              2,
              1
            ]
          },
          {
            "id": "t2.2",
            "title": "Experimental milestones",
            "summary_points": [
              "First two-qubit demonstrations in the late 1990s via NMR and trapped ions; continuous improvements in qubit counts and gate fidelities across platforms.",
              "Superconducting QPUs (Google, IBM, others) and trapped-ion systems dominate many near-term demonstrations; claims of quantum advantage focus on specialized sampling tasks rather than broadly useful computations."
            ],
            "references": [
              2,
              3,
              1
            ]
          }
        ],
        "references": [
          2,
          1,
          3
        ]
      },
      {
        "id": "t3",
        "title": "Algorithms, complexity and applications",
        "summary_points": [
          "Shor's algorithm: polynomial-time quantum factoring and discrete logarithm algorithm with direct cryptographic impact (would break widely used public-key systems if run on a sufficiently large, low-error quantum computer).",
          "Grover's algorithm: quadratic speedup for unstructured search; widely applicable but less dramatic than Shor's exponential speedups.",
          "Quantum simulation: Feynman's original motivation — quantum computers can efficiently simulate quantum systems that are intractable classically, foundational for quantum chemistry and materials science.",
          "Oracle and complexity separations: Deutsch–Jozsa, Simon, Bernstein–Vazirani demonstrate theoretical separations (BQP vs classical classes) in oracle settings; Gottesman–Knill theorem identifies efficiently classically simulable quantum circuits (stabilizer circuits)."
        ],
        "subtopics": [
          {
            "id": "t3.1",
            "title": "Practical and near-term algorithms",
            "summary_points": [
              "Variational hybrid algorithms (VQE, QAOA) and sampling tasks are targeted for noisy intermediate-scale quantum (NISQ) devices; these aim to use limited qubits and noisy gates to solve useful problems approximately.",
              "Many NISQ demonstrations show promise for specialized tasks (optimization heuristics, quantum chemistry approximations), but scalability and error mitigation remain open."
            ],
            "references": [
              1,
              2
            ]
          },
          {
            "id": "t3.2",
            "title": "Cryptographic and scientific implications",
            "summary_points": [
              "Large-scale fault-tolerant quantum computers would threaten RSA/elliptic-curve/DH cryptosystems; post-quantum cryptography and migration are critical.",
              "High-fidelity quantum simulation could transform computational chemistry, condensed-matter physics, and materials design by avoiding exponential classical overhead."
            ],
            "references": [
              1,
              5
            ]
          }
        ],
        "references": [
          1,
          2,
          5
        ]
      },
      {
        "id": "t4",
        "title": "Physical implementations and technologies",
        "summary_points": [
          "Multiple physical platforms under active research: superconducting circuits, trapped ions, nuclear magnetic resonance (NMR), photonics, spin qubits (silicon, NV centers), topological (anyons), and proposed unconventional devices (fermionic, bosonic, nonlinear-mechanics-based).",
          "Each platform trades off coherence time, gate speed, connectivity, manufacturability and scaling prospects; engineering and materials advances (e.g., Josephson junction fabrication, cryogenics) are central.",
          "Superconducting qubits: use Josephson junctions to create anharmonic energy spacing enabling two-level addressing; manufactured with lithography, operated at millikelvin temperatures in dilution refrigerators; widely used by industry and research labs."
        ],
        "subtopics": [
          {
            "id": "t4.1",
            "title": "Superconducting qubits and circuits",
            "summary_points": [
              "Josephson junctions supply nonlinear inductance making anharmonic oscillators (transmon, Xmon, fluxonium etc.) that behave as qubits; operation requires microwave control and cryogenic cooling (< 15 mK typical).",
              "Manufacturing leverages microfabrication (lithography, deposition, oxidation); qubit quality improvements focus on materials (e.g., niobium, tantalum), coherence times, and reducing coupling to noise sources.",
              "Architectural concerns: chip layout (1D/2D arrays), readout resonators, control wiring and scaling of cryogenic infrastructure are central engineering challenges."
            ],
            "references": [
              3
            ]
          },
          {
            "id": "t4.2",
            "title": "Trapped ions, NMR and other qubits",
            "summary_points": [
              "Trapped-ion qubits encode states in atomic internal levels with long coherence and high-fidelity gates but slower operations and challenges in dense scaling; NMR demonstrated early multi-qubit gates but mixed-state limits entanglement utility at scale.",
              "Topological proposals (anyons) aim for intrinsic error protection via nonlocal encoding; experimental realization (non-abelian anyons) remains an active research frontier.",
              "Unconventional devices (fermionic encodings, bosonic modes, Bose–Einstein condensates, cavity QED) offer alternative trade-offs; some promise faster simulation for particular problems or intrinsic error-resilience."
            ],
            "references": [
              2,
              5,
              3
            ]
          }
        ],
        "references": [
          3,
          5,
          2
        ]
      },
      {
        "id": "t5",
        "title": "Error correction, fault tolerance and engineering limits",
        "summary_points": [
          "Decoherence and noise are primary practical barriers; quantum error correction (QEC) schemes and fault-tolerant constructions enable arbitrarily long computations provided gate error rates are below threshold values.",
          "DiVincenzo's criteria enumerate practical requirements for a physical quantum computer (scalable qubits, initialization, long coherence, universal gates, readout; plus communication criteria for quantum networks).",
          "Topological codes and surface codes are leading QEC approaches due to high thresholds and locality properties; implementing QEC imposes heavy qubit overhead (logical vs physical qubits)."
        ],
        "subtopics": [
          {
            "id": "t5.1",
            "title": "Threshold theorem and QEC",
            "summary_points": [
              "Threshold results show that with per-gate error rates below ~10^-4–10^-2 (platform-dependent) and appropriate QEC, arbitrarily long computations can be made reliable.",
              "Common codes: Shor code, Steane code, surface codes, and topological codes (Kitaev) — each with trade-offs in overhead, locality, and error models handled."
            ],
            "references": [
              2,
              5
            ]
          },
          {
            "id": "t5.2",
            "title": "Engineering challenges",
            "summary_points": [
              "Scaling requires improvements in coherence, gate fidelity, crosstalk control, thermal management (cryogenics), interconnect complexity, and reproducible fabrication.",
              "Platform-specific noise sources (dielectric loss, quasiparticles in superconductors, motional heating in ions, spin bath for solid-state spins) drive research into materials and device design."
            ],
            "references": [
              3,
              1
            ]
          }
        ],
        "references": [
          2,
          3,
          5
        ]
      },
      {
        "id": "t6",
        "title": "Quantum Internet and distributed quantum computing",
        "summary_points": [
          "Quantum Internet: networked quantum nodes sharing entanglement across long distances to enable teleportation of qubits, distributed algorithms, quantum key distribution, and non-local gates using entanglement as a resource.",
          "Distributed quantum computing links multiple QPUs to form larger, collaborative computations; partitioning and minimizing entanglement resources is central to make distributed execution practical.",
          "Quantum cloud and delegated computing: clients can delegate computation to remote quantum servers; blind quantum computing and verifiable delegated protocols enable privacy and integrity when clients are weak or classical."
        ],
        "subtopics": [
          {
            "id": "t6.1",
            "title": "Core technologies for quantum networking",
            "summary_points": [
              "Entanglement distribution mechanisms: entanglement swapping, purification/distillation, and quantum repeaters mitigate losses and decoherence over long distances.",
              "Physical channels include optical fibres, free-space links and satellites; experimental demonstrations of long-distance entanglement distribution have been reported using both fibre and satellite links."
            ],
            "references": [
              4,
              1
            ]
          },
          {
            "id": "t6.2",
            "title": "Protocols and applications",
            "summary_points": [
              "Distributed protocols include quantum anonymous broadcasting, leader election, secret sharing, Byzantine agreement variants, and non-local implementation of gates; some tasks provably reduce communication complexity compared to classical counterparts.",
              "Quantum IoT and edge/fog possibilities: potential (speculative) future of small or room-temperature quantum devices at the edge enabling new distributed sensing and computing paradigms; significant engineering and materials advances required."
            ],
            "references": [
              4,
              2
            ]
          }
        ],
        "references": [
          4,
          1,
          2
        ]
      },
      {
        "id": "t7",
        "title": "Theoretical frameworks and alternative views",
        "summary_points": [
          "Geometrical formulation: quantum states can be represented via noncommutative geometry as connections and gauge transforms can implement unitary operations — an alternate, equivalent computational model with potential conceptual/implementation implications.",
          "Unconventional theoretical devices: fermionic encodings, bosonic/cavity modes, anyons (topological computation) and speculative nonlinear quantum mechanics point to different computational-resource trade-offs and potential new algorithms.",
          "Complexity considerations: alternative statistics (fermions, anyons) and dimensionality can change simulation complexity for certain problems (e.g., fermionic lattice simulations may be faster on fermionic devices)."
        ],
        "subtopics": [
          {
            "id": "t7.1",
            "title": "Geometrical/gauge perspective",
            "summary_points": [
              "Recasts quantum states as gauge connections in a spectral triple; gauge transforms correspond to unitary operations and provide an equivalent computational model to the circuit model.",
              "Offers a different mathematical viewpoint that may suggest alternative physical implementations or simplifications for certain operations."
            ],
            "references": [
              6
            ]
          },
          {
            "id": "t7.2",
            "title": "Unconventional device classes",
            "summary_points": [
              "Fermionic quantum computers encode information in occupation of fermionic modes; in >1D lattices they can simulate fermionic dynamics more naturally and sometimes more efficiently than conventional qubit encodings.",
              "Anyons and topological systems promise intrinsic noise-resilience via nonlocal encodings; bosonic and cavity-QED approaches enable rich state synthesis important for communications.",
              "Speculative alterations to quantum mechanics (e.g., nonlinear variants) would radically alter computational power, but remain hypothetical."
            ],
            "references": [
              5,
              3
            ]
          }
        ],
        "references": [
          6,
          5,
          3
        ]
      },
      {
        "id": "t8",
        "title": "Implications, current status and outlook",
        "summary_points": [
          "Near-term devices (NISQ era) provide experimental platforms for exploring algorithms, error mitigation, and applications in chemistry, optimization and sampling — but are not yet broadly superior to classical systems for general-purpose tasks.",
          "Long-term transformative potentials include breaking currently deployed public-key cryptography, enabling large-scale quantum simulation, novel sensing/positioning capabilities, and secure distributed quantum services.",
          "Research directions: improving coherence and gate fidelity, scalable architectures, error correction overhead reduction, quantum network protocols, and exploration of unconventional hardware and theoretical models."
        ],
        "subtopics": [
          {
            "id": "t8.1",
            "title": "Near-term vs long-term",
            "summary_points": [
              "Near-term: focus on hybrid algorithms, benchmarking, demonstrating quantum advantage on niche tasks and building quantum cloud access.",
              "Long-term: develop fault-tolerant universal quantum computers, large-scale quantum networks, and transition to post-quantum cryptography for classical infrastructures."
            ],
            "references": [
              1,
              4,
              5
            ]
          }
        ],
        "references": [
          1,
          4,
          5
        ]
      }
    ],
    "conclusion": "Quantum computing is a multidisciplinary field combining quantum mechanics, computer science, materials science and engineering. Foundational principles (qubits, superposition, entanglement, unitary evolution and measurement) enable algorithms with provable and conjectured speedups (Shor, Grover, quantum simulation, oracle separations). Multiple hardware platforms (superconducting circuits, trapped ions, NMR, photonics, topological and unconventional devices) each offer different trade-offs; superconducting and trapped-ion systems lead current experimental scaling. Overcoming decoherence and implementing quantum error correction are essential for fault-tolerant, large-scale quantum computation. The quantum Internet and distributed quantum computing extend capabilities to networked QPUs, enabling delegated/verified computation and long-distance entanglement-based protocols. Alternative theoretical perspectives (geometrical/gauge formulations, fermionic or anyonic encodings) suggest diverse implementations and complexity implications. The near-term focus is on NISQ-era applications, benchmarking and engineering improvements, while long-term goals include scalable fault-tolerant machines and quantum networks with broad scientific, cryptographic and technological impact."
  },
  "report_parts": [
    "## Foundations of quantum information\n\nA qubit is the fundamental two-level unit of quantum information, conventionally denoted by the orthonormal basis states |0⟩ and |1⟩ and described by a general state α|0⟩ + β|1⟩ with complex amplitudes α and β subject to the normalization condition |α|^2 + |β|^2 = 1. [1, 6] The Born rule connects these amplitudes to experimentally observable probabilities: the probability of obtaining a particular basis outcome upon measurement equals the squared modulus of the corresponding amplitude. [1, 6]\n\nSuperposition denotes that a qubit may occupy linear combinations of basis states, a feature that enables quantum parallelism in algorithmic contexts while yielding only probabilistic outcomes upon measurement. [6] Quantum algorithms exploit interference of complex amplitudes—constructive interference to amplify desired computational outcomes and destructive interference to suppress undesired ones—to increase the likelihood of obtaining correct results after measurement. [6]\n\nEntanglement embodies nonclassical correlations between multiple qubits that cannot be described as products of individual qubit states, and it functions as a distinct resource enabling nonlocal operations and protocols such as teleportation, non-local gates, and quantum communication schemes. [1, 5] Quantum operations on these states follow two principal modalities: closed-system evolution is unitary and implemented by quantum gates, whereas measurement is a fundamentally non-unitary, probabilistic operation; quantum computations are therefore composed of sequences of unitary gates interleaved with measurements. [1, 6] Finally, classical simulation of general quantum systems typically requires resources that scale exponentially with system size, a scaling that motivates the development of quantum computers and dedicated quantum simulation algorithms. [5]\n\n### Mathematical formalism\n\nQuantum states are represented as vectors in a complex Hilbert space and are commonly written in Dirac (bra–ket) notation as |ψ⟩. [1, 6] The amplitudes that appear in these vector representations are complex numbers whose relative phases enable interference phenomena; such constructive and destructive interference of amplitudes is exploited in quantum algorithms to manipulate outcome probabilities. [6] Linear algebra provides the primary language for describing and analyzing quantum systems: operators, represented by matrices acting on state vectors, correspond to quantum gates and transformations and permit systematic composition and analysis of quantum programs. [1, 6]\n\nMeasurement is described within this formalism as a process that projects the pre-measurement state onto an eigenstate of the measured observable, with the probability of each outcome given by the squared modulus of the amplitude associated with that eigenstate, as prescribed by the Born rule. [1, 6] This representation makes explicit the dual character of quantum mechanics in computation: reversible, deterministic unitary evolution between measurements and irreversible, probabilistic collapse at measurement. [1, 6]\n\n### Resources and constraints\n\nEntanglement and coherence are identified as central resources for quantum information processing: entanglement provides the nonclassical correlations exploited in many quantum protocols, and coherence—the maintenance of well-defined relative phases among amplitudes—must be preserved for the duration of computational or communication tasks to enable interference-based advantages. [5, 1] The practical requirement to maintain coherence over sufficient time intervals constrains implementations and informs error mitigation and control strategies. [5, 1]\n\nRealizing universal quantum logic requires both coherence and the ability to implement suitable interactions; specifically, nonlinearity in the effective interactions, together with preserved coherence, is necessary to implement universal quantum gates, whereas purely linear phenomena alone are insufficient to realize universal quantum computation. [5, 1] These resource and constraint considerations thus define the operational boundary between what can be achieved with available quantum physical processes and what remains classically intractable. [5, 1]",
    "## History and milestones\n\nThe theoretical foundations of quantum computing were established in the late twentieth century through a sequence of pioneering proposals that reframed computation in quantum-mechanical terms. Paul Benioff introduced the concept of a quantum Turing machine, and subsequent work by Richard Feynman and Yuri Manin emphasized the potential of quantum systems for simulating quantum physics, motivating the field's early development; David Deutsch later proposed the notion of a universal quantum computer, formalizing the idea of general-purpose quantum computation [2,1]. These formative contributions laid the conceptual groundwork for treating quantum information and quantum gates as objects of algorithmic and engineering interest [2,1].\n\nThe 1990s saw rapid algorithmic and cryptographic advances that demonstrated clear potential advantages for quantum devices over classical ones. Foundational work in quantum cryptography and information-theoretic limits, together with algorithmic breakthroughs such as Simon’s oracle separation, Shor’s polynomial-time factoring algorithm, and Grover’s search algorithm, established both theoretical separations and concrete applications that motivated substantial research and investment in the field [2,1]. These advances catalyzed experimental efforts to build small-scale quantum processors and to explore implementations capable of running the new algorithms [2,1].\n\nExperimentally, the community progressed from demonstrations of small quantum logic gates to progressively larger, more coherent devices implemented across several hardware platforms, including trapped ions, nuclear magnetic resonance (NMR), and superconducting circuits, with steady improvements in qubit counts and gate fidelities over the decades [3,1]. A major recent milestone was a 2019 sampling experiment by Google involving a 53-qubit device that sparked debate about the classical hardness of the task and the appropriate definition and benchmarks for claims of quantum advantage or supremacy [3,2]. These theoretical, algorithmic, and experimental strands together trace a trajectory from foundational ideas to contemporary demonstrations and ongoing discussions about practical quantum advantage [2,3,1].\n\n### Key dates and contributions\n\nWork from the 1970s through the 1980s established essential information-theoretic limits and models relevant to quantum information, including results such as the Holevo bound and developments in reversible computation by researchers like Benioff and Toffoli, which connected thermodynamic and information-theoretic considerations to computation [2,1]. In the early 1980s, motivations for quantum simulation were articulated by figures such as Feynman and Manin, and by the mid-1980s Deutsch proposed the first formal notion of a universal quantum machine, crystallizing the idea that quantum mechanics could underwrite a new model of general computation [2,1].\n\nThe 1990s were characterized by rapid algorithmic progress and the beginning of experimental realizations: a sequence of algorithms (Deutsch–Jozsa, Bernstein–Vazirani, Simon, Shor, and Grover) demonstrated various separations and speedups relative to classical algorithms, while experimental platforms such as ion traps and NMR began to realize small quantum gates and primitive multi-qubit systems [2,1]. These dual advances in theory and practice reinforced each other, with algorithmic promises motivating hardware development and experimental constraints informing theoretical work [2,1].\n\nFrom the 2000s into the 2010s the field concentrated on error correction, architectural criteria for scalable quantum computation, and new theoretical approaches such as topological quantum computing proposed by Kitaev, alongside engineering progress in dominant platforms like superconducting circuits and trapped ions that demonstrated incremental scaling and multi-qubit processors [2,1]. During this period the community refined requirements for fault tolerance, emphasized systematic criteria for viable quantum hardware, and pursued both incremental and conceptual routes toward larger, reliable quantum systems [2,1].\n\n### Experimental milestones\n\nInitial experimental demonstrations achieved two-qubit operations and small-scale coherent control in the late 1990s using platforms such as NMR and trapped ions, establishing the feasibility of elementary quantum logic on physical devices and initiating a trajectory of continuous improvements in qubit counts and gate fidelities across multiple technologies [3,1]. Over subsequent decades these incremental advances accumulated into demonstrable multi-qubit processors and increasingly sophisticated control techniques, with fidelity and coherence improvements enabling more complex experiments and algorithmic tests [3,1].\n\nIn the contemporary landscape, superconducting quantum processing units developed by groups such as Google and IBM, together with advanced trapped-ion systems, have dominated many near-term demonstrations and scaling efforts, each platform exhibiting distinct trade-offs in connectivity, coherence, and control that shape experimental strategies [3,1]. Claims of quantum advantage or supremacy have typically focused on specialized sampling or benchmarking tasks rather than on broadly useful computations, exemplified by debates following Google's 2019 53-qubit sampling experiment about the classical hardness of the demonstrated task and about suitable definitions and benchmarks for supremacy [3,1].",
    "## Algorithms, complexity and applications\n\nShor's algorithm provides a polynomial-time quantum algorithm for integer factoring and for computing discrete logarithms; this result has a direct cryptographic impact because, if implemented on a sufficiently large and low-error quantum computer, it would break widely used public-key systems [1]. Grover's algorithm offers a quadratic speedup for unstructured search problems, a broadly applicable improvement that is nevertheless less dramatic than Shor's exponential speedups [2]. These landmark algorithms illustrate distinct classes of quantum algorithmic advantage: exponential algorithmic separations in certain algebraic problems and polynomial (quadratic) speedups for generic search tasks [1,2].\n\nQuantum simulation was the original motivation for quantum computation as articulated by Feynman: quantum computers can efficiently simulate quantum systems that are intractable for classical computers, a capability that forms the conceptual foundation for applications in quantum chemistry and materials science [1,5]. The potential for quantum simulation to avoid the exponential overhead encountered by classical simulation is central to its anticipated scientific impact, as it would enable more direct and tractable modeling of many-body quantum phenomena [1,5].\n\nTheoretical work on oracles and complexity separations has clarified where quantum computation provably outperforms classical models in relativized settings. Examples such as the Deutsch–Jozsa, Simon, and Bernstein–Vazirani problems demonstrate separations between BQP and various classical complexity classes in oracle models, establishing formal instances of quantum advantage under specific query assumptions [1,2]. At the same time, the Gottesman–Knill theorem identifies a class of quantum circuits (stabilizer circuits) that are efficiently simulable on classical computers, thereby delineating limits on quantum speedup and highlighting the nuanced landscape of quantum versus classical simulability [2].\n\n### Practical and near-term algorithms\n\nVariational hybrid algorithms, including the variational quantum eigensolver (VQE) and the quantum approximate optimization algorithm (QAOA), together with sampling-based tasks, are primary targets for noisy intermediate-scale quantum (NISQ) devices. These approaches are designed to leverage a limited number of qubits and imperfect gates by combining quantum circuit evaluation with classical optimization loops, aiming to produce useful approximate solutions within current hardware constraints [1,2]. Sampling tasks similarly seek to exploit near-term devices' propensity to generate classically hard-to-sample distributions for specialized applications [2].\n\nEmpirical NISQ demonstrations have shown promise in specialized domains such as heuristic optimization and approximate quantum chemistry calculations, indicating that near-term devices can provide meaningful, if limited, gains on particular problem instances [1,2]. However, questions of scalability and the effectiveness of error mitigation techniques remain open: extending these demonstrations to larger problem sizes and certifying their advantage over classical methods continue to be active challenges for the field [1,2].\n\n### Cryptographic and scientific implications\n\nThe development of large-scale, fault-tolerant quantum computers would pose a direct threat to widely deployed public-key cryptosystems, including RSA, elliptic-curve, and Diffie–Hellman schemes, because Shor-like algorithms would be able to break the underlying hard problems on which those systems rely [1,5]. Consequently, the design and deployment of post-quantum cryptographic algorithms and coordinated migration strategies are critical to maintaining secure communications in a future where such quantum capabilities exist [1,5].\n\nHigh-fidelity quantum simulation promises transformative effects for fields that depend on accurate modeling of quantum systems, notably computational chemistry, condensed-matter physics, and materials design. By enabling efficient simulation of many-body quantum behavior without the exponential scaling faced by classical methods, quantum simulation could open new avenues for discovering molecular structures, characterizing material properties, and guiding experimental design in ways that are currently infeasible on classical platforms [1,5].",
    "## Physical implementations and technologies\n\nMultiple physical platforms for quantum information processing are under active investigation, including superconducting circuits, trapped ions, nuclear magnetic resonance (NMR), photonics, spin qubits (such as silicon and NV centers), topological approaches based on anyons, and a variety of proposed unconventional devices (for example, fermionic and bosonic encodings, Bose–Einstein condensates, and nonlinear-mechanics-based systems) [3,5,2]. Each of these platforms embodies distinct physical degrees of freedom and control techniques, and research efforts examine the relative merits of these diverse approaches for computation, simulation, and sensing tasks [3,5,2].\n\nDesign and evaluation of these platforms involve trade-offs among coherence time, gate speed, connectivity, manufacturability, and long-term scaling prospects; these trade-offs determine which applications a platform can realistically address in the near and longer term [3]. Consequently, engineering and materials advances are central to progress: for solid-state platforms, improvements in component fabrication (for example, Josephson junction processing), materials selection, and cryogenic engineering can yield significant gains in coherence and reproducibility, while for other platforms analogous advances in trapping, optical control, and vacuum or cryogenic environments are similarly critical [3]. The interplay between device physics, materials science, and systems engineering therefore shapes both short-term performance and long-range scaling strategies across the field [3].\n\nSuperconducting qubits: use Josephson junctions to create anharmonic energy level spacing that enables addressing of an effective two-level system; these circuits are typically fabricated using lithographic techniques and operated at millikelvin temperatures in dilution refrigerators, and they form a dominant platform in both industrial and academic efforts [3].  \n\n### Superconducting qubits and circuits\n\nSuperconducting qubits derive their effective two-level behavior from the nonlinear inductance provided by Josephson junctions, which produces anharmonic oscillators such as transmons, Xmons, and fluxonium that can be manipulated as qubits [3]. Operation of these devices requires high-precision microwave control for single- and two-qubit gates and dilution-refrigerator temperatures (often below roughly 15 mK) to suppress thermal excitations and environmental dissipation [3]. These physical and control requirements define the basic operational paradigm for contemporary superconducting quantum processors [3].\n\nManufacturing of superconducting circuits leverages established microfabrication processes including lithography, thin-film deposition, and controlled oxidation steps for Josephson junctions; ongoing quality improvements focus on materials choices (for example, niobium and tantalum), surface treatment, and fabrication reproducibility in order to extend coherence times and reduce coupling to loss and noise sources [3]. Achieving reproducible, high-coherence devices therefore depends as much on materials and process engineering as on circuit design [3].\n\nAt the architectural level, superconducting systems face engineering challenges related to chip layout (one-dimensional and two-dimensional qubit arrays), the integration of readout resonators and control lines, and the practical scaling of cryogenic infrastructure to accommodate increasing qubit counts; these considerations influence both immediate device layouts and long-term strategies for modular or layered architectures [3]. Addressing control wiring density, heat load management, and high-fidelity multiplexed readout are central to translating improvements in individual qubit performance into scalable processors [3].\n\n### Trapped ions, NMR and other qubits\n\nTrapped-ion qubits encode quantum information in internal atomic levels and are notable for long coherence times and very high-fidelity gates, but they typically exhibit slower gate speeds and face specific challenges in dense scaling and integration relative to some solid-state approaches [2,5,3]. Nuclear magnetic resonance (NMR) systems played an early role in demonstrating multi-qubit control and gate implementations, but their reliance on mixed-state ensembles limits the usefulness of entanglement at large scales for general-purpose quantum computing [2,5,3]. These platforms illustrate the common trade-off between coherence and gate speed/connectivity that recurs across physical implementations [2,5,3].\n\nTopological proposals seek to encode quantum information nonlocally using excitations such as non-Abelian anyons, with the aim of intrinsic protection from certain local errors and thereby reduced overhead for active error correction; however, experimental realization of non-Abelian anyons and practical topological qubits remains an active area of fundamental research [5,3,2]. Beyond these more established and proposed routes, unconventional devices and encodings—such as fermionic encodings, bosonic mode approaches, Bose–Einstein condensate–based schemes, and cavity QED implementations—offer alternative trade-offs and occasionally problem-specific advantages, including prospects for faster simulation of particular Hamiltonians or forms of intrinsic error resilience tailored to specific noise models [5,3,2].",
    "## Error correction, fault tolerance and engineering limits\n\nDecoherence and environmental noise constitute the primary practical barriers to constructing useful quantum processors; without mitigation, these effects rapidly corrupt quantum information and limit computation times. Quantum error correction (QEC) schemes and fault-tolerant constructions are therefore essential: when gate error rates can be suppressed below certain threshold values and appropriate QEC is applied, it becomes possible in principle to perform arbitrarily long quantum computations by suppressing logical error rates through encoding and concatenation [2,5]. These threshold results set quantitative engineering targets that guide both device development and system-level architecture decisions [2,5].\n\nPractical realization of scalable quantum computation additionally depends on a set of concrete hardware and system requirements summarized by DiVincenzo’s criteria, which enumerate the necessities for a physical quantum computer: a scalable collection of well-characterized qubits, reliable initialization to known states, coherence times long compared with gate times, a universal set of quantum gates implemented with high fidelity, and high-fidelity state readout; related communication criteria extend these requirements to quantum networking contexts [2]. These criteria provide a framework for assessing whether a given platform can meet the operational demands imposed by error correction and fault tolerance [2].\n\nAmong QEC approaches, topological codes—particularly surface codes derived from Kitaev’s ideas—have emerged as leading candidates because of their favorable threshold values and locality properties that map well onto two-dimensional device layouts. Surface and other topological codes trade locality and high threshold against significant resource costs: implementing robust QEC generally requires many physical qubits per logical qubit, producing heavy overheads that dominate near-term architectural and engineering considerations [5]. This imbalance between the logical capabilities offered by QEC and the large multiplicative cost in physical resources defines one of the central engineering limits for near-term quantum systems [5].\n\n### Threshold theorem and QEC\n\nThreshold theorems establish that reliable, arbitrarily long quantum computation is achievable provided that physical gate error rates lie below a platform-dependent threshold, typically quoted in the range approximately 10^-4 to 10^-2; beneath such thresholds, concatenated or topological QEC constructions can reduce logical error rates exponentially with encoding overhead, enabling fault-tolerant operation in principle [2,5]. These results both motivate and quantify the required improvements in gate fidelity and coherence that experimental platforms must attain to move from demonstration-scale experiments to practically useful machines [2,5].\n\nA variety of quantum error-correcting codes has been developed, each with distinct strengths and engineering implications. Examples include the Shor code and the Steane code, which illustrate early concatenated-code approaches, and surface codes and other topological codes (following Kitaev’s construction), which emphasize local checks and high thresholds. Each class of code presents trade-offs among overhead (the ratio of physical to logical qubits), geometric locality of syndrome measurements, and the types of error models they handle most effectively; these trade-offs inform the choice of code for a given hardware platform and target application [2,5].\n\n### Engineering challenges\n\nScaling quantum processors beyond small demonstrators requires coordinated improvements across multiple engineering dimensions: longer coherence times, higher gate fidelities, tighter control of crosstalk between qubits, effective thermal management (including cryogenic infrastructure where applicable), manageable interconnect complexity, and reproducible fabrication processes that yield uniform device performance across many qubits [3]. Each of these engineering factors interacts with error-correction requirements, since achieving low logical error rates depends on both intrinsic device quality and system-level engineering that preserves that quality at scale [3].\n\nDifferent physical platforms are affected by distinct, platform-specific noise mechanisms that drive targeted research in materials and device design. Examples of such noise sources include dielectric loss and quasiparticles in superconducting circuits, motional heating in trapped-ion systems, and spin-bath interactions in solid-state spin qubits; addressing these mechanisms motivates improvements in materials, fabrication, and packaging to reduce their impact on coherence and gate fidelity [3]. These platform-specific challenges underscore that engineering limits are not purely algorithmic or theoretical but are tightly coupled to the physical origins of noise in each candidate technology, shaping the pathway to practical, fault-tolerant quantum machines [3].",
    "## Quantum Internet and distributed quantum computing\n\nThe quantum internet is conceived as a network of quantum nodes that share entanglement across long distances to enable quantum teleportation of qubits, support distributed quantum algorithms, provide quantum key distribution, and realize non-local quantum gates by using entanglement as a consumable resource [4]. Such a network treats entanglement as a primary resource that can be generated, distributed, and consumed to effect operations that have no classical analogue, thereby forming the foundational medium for remote quantum information processing and secure communication [4].\n\nDistributed quantum computing refers to architectures in which multiple quantum processing units (QPUs) are linked so that they can cooperate to solve problems that exceed the capacity of any single QPU. Practical distributed execution depends critically on partitioning quantum computations across nodes and on strategies to minimize the entanglement resources required for inter-node operations, since entanglement generation and maintenance are costly relative to local quantum operations [4]. Efficient partitioning and resource management therefore become central design concerns for scalable distributed quantum systems [4].\n\nDelegated and cloud-based quantum computing models envision clients offloading quantum computations to remote quantum servers. Protocols for blind quantum computing and verifiable delegated computation have been proposed to preserve client privacy and to provide integrity guarantees when clients are technologically weak or classical, enabling meaningful client–server quantum services even when trust or client capability is limited [4]. These delegated models form a layer of service abstraction that complements physically networked quantum resources and expands accessible use cases for quantum computation [4].\n\n### Core technologies for quantum networking\n\nEntanglement distribution over long distances is enabled by several key techniques that mitigate loss and decoherence. Entanglement swapping allows intermediate nodes to connect shorter entangled links into longer ones, while entanglement purification or distillation improves fidelity by probabilistically extracting higher-quality entanglement from multiple noisy pairs. Quantum repeaters combine these ideas to extend entanglement distribution ranges by segmenting channels and managing errors and losses across segments [4]. Together, these mechanisms constitute the primary toolbox for overcoming the distance- and noise-related limitations of quantum channels [4].\n\nPhysical channels for quantum networking include optical fibres, free-space optical links, and satellite-mediated links, each offering distinct trade-offs in terms of loss, deployment, and available distance. Experimental demonstrations have reported long-distance entanglement distribution using both fibre and satellite links, illustrating the feasibility of multiple transmission modalities for future networks [4, 1]. The coexistence of these channel types suggests hybrid network topologies in which different link technologies are employed according to geographical and application-specific constraints [4, 1].\n\n### Protocols and applications\n\nA range of distributed quantum protocols has been identified that exploit entanglement and quantum communication primitives to realize tasks such as anonymous broadcasting, leader election, secret sharing, variants of Byzantine agreement, and non-local implementations of quantum gates. Some of these quantum protocols can provably reduce communication complexity relative to their classical counterparts, demonstrating potential fundamental advantages of distributed quantum information processing [4, 2]. These protocols illustrate both the novel functionalities enabled by entanglement and the ways in which quantum resources can be traded against communication or classical computation.\n\nLooking forward, the notion of a quantum Internet has been extended speculatively to include a “quantum IoT” or edge/fog paradigms in which small, possibly room-temperature quantum devices deployed at the edge participate in distributed sensing and computing tasks. Realizing such edge-enabled quantum systems would open new distributed application classes, but would also demand substantial engineering and materials advances to produce compact, robust, and deployable quantum hardware [4, 2]. These prospective developments emphasize the long-term interplay between device-level advances and network-level protocol design in shaping future distributed quantum ecosystems [4, 2].",
    "## Theoretical frameworks and alternative views\n\nQuantum computation admits alternative formal and physical frameworks that reframe how quantum information and operations are represented and implemented. One such formal alternative is a geometrical formulation in which quantum states and their transformations are expressed in the language of noncommutative geometry and spectral triples; in this view, gauge connections encode states and gauge transformations correspond to unitary operations, yielding an alternate but equivalent model to the conventional circuit model [6]. Complementing this mathematical rephrasing are a variety of unconventional physical device classes—fermionic encodings, bosonic and cavity modes, anyonic (topological) systems, and even speculative nonlinear modifications of quantum mechanics—that propose different ways to store, manipulate, and protect quantum information and thus imply different trade-offs in computability and implementation [5,3].\n\nThese alternative frameworks can materially affect complexity and resource considerations for particular tasks. The choice of particle statistics and the spatial dimensionality of a device can change the simulation complexity for problems that reflect those statistics, such as lattice fermion simulations that may be more naturally and sometimes more efficiently carried out on devices built from fermionic modes rather than qubit encodings [5,3]. More broadly, alternative encodings and physical substrates suggest distinct pathways to algorithm design, noise resilience, and hardware specialization that merit consideration alongside the standard qubit-circuit paradigm [5,3].\n\n### Geometrical/gauge perspective\n\nThe geometrical or gauge perspective recasts the state space of a quantum system in terms of structures from noncommutative geometry, particularly spectral triples, so that quantum states are represented by gauge connections and state evolution by gauge transformations. In this formalism the operation of performing a unitary on a state corresponds to applying an appropriate gauge transform, and this correspondence furnishes an alternate computational model that is formally equivalent to the circuit model of quantum computation [6]. Viewing quantum computation through this mathematical lens emphasizes global and geometric properties of states and operations, which can reveal conceptual simplifications and suggest different implementation paradigms for certain classes of operations [6].\n\n### Unconventional device classes\n\nFermionic quantum computation encodes information in the occupation numbers of fermionic modes and thus directly leverages fermionic exchange statistics. On lattices of more than one spatial dimension, such fermionic devices can represent and simulate fermionic dynamics in a more natural manner than encodings that map fermions onto qubits, and for some fermionic-lattice problems this natural representation can lead to more efficient simulations relative to conventional qubit-based encodings [5]. This directness can reduce overheads associated with Jordan–Wigner–type mappings and related encodings, particularly in higher-dimensional geometries [5].\n\nTopological approaches that employ anyons promise intrinsic noise resilience because quantum information is stored nonlocally in topological degrees of freedom, making it less susceptible to local errors, while bosonic and cavity-QED platforms enable the synthesis of rich continuous-variable or multimode states that are important for communication and certain information-processing tasks [5,3]. These device classes therefore present distinct practical advantages—topological systems for error protection and bosonic/cavity systems for state engineering and interfacing—each implying different algorithmic and hardware trade-offs [5,3]. Finally, speculative alterations to the standard linear quantum-mechanical framework, such as nonlinear variants, if they were realized would profoundly change computational power and complexity landscapes, though such modifications remain hypothetical and are considered primarily as thought-experiments regarding resource implications [3].",
    "## Implications, current status and outlook\n\nNear-term, noisy intermediate-scale quantum (NISQ) devices serve primarily as experimental platforms for exploring algorithms, error mitigation techniques, and application-specific demonstrations in areas such as chemistry, optimization, and sampling, but they are not yet broadly superior to classical systems for general-purpose computation [1,4]. These platforms enable the empirical benchmarking of algorithmic ideas and hardware capabilities, offering insight into practical constraints such as coherence times, gate fidelities, and connectivity that inform both algorithm design and engineering priorities [1,4]. The current status therefore emphasizes iterative experimentation, hybrid classical–quantum workflows, and the development of cloud-accessible quantum resources that allow broader user engagement and reproducibility of results [1].\n\nOver the long term, the field envisions transformative potentials that could materially alter computational and communication paradigms. These include the capacity to break certain currently deployed public-key cryptographic schemes, the realization of large-scale quantum simulation for complex many-body and chemical systems, the development of novel sensing and positioning capabilities, and the provisioning of secure distributed quantum services that have no classical analogue [4,5]. Achieving these outcomes requires moving beyond noisy devices to fault-tolerant, scalable architectures and networks that can support sustained, error-corrected operations at scale [4,5].\n\nKey research directions bridge the near- and long-term agendas and focus on technical levers that can reduce the gap to transformative capability. Principal efforts include improving coherence and gate fidelity, designing scalable hardware architectures, reducing the resource overhead of quantum error correction, developing quantum network protocols for distributed computation and communication, and exploring unconventional hardware approaches and theoretical models that may offer alternative paths to scale or functionality [1,5]. Progress along these vectors will determine the pace at which niche demonstrations mature into broadly impactful technologies and will guide complementary policy and infrastructure responses such as cryptographic transition planning [1,5].\n\n### Near-term vs long-term\n\nNear-term work concentrates on hybrid algorithm development that couples classical and quantum processing to extract advantage within the constraints of existing devices, along with rigorous benchmarking to characterize when and where quantum components provide meaningful contribution. Demonstrating quantum advantage on carefully chosen niche tasks and expanding quantum cloud access are practical priorities that enable wider experimentation, validation of methods, and accumulation of empirical evidence about performance boundaries [1]. These activities also inform incremental improvements in device control, error mitigation strategies, and software tooling that are essential for near-term progress [1].\n\nLong-term efforts are oriented toward realizing fault-tolerant universal quantum computers and interoperable large-scale quantum networks capable of supporting complex, error-corrected workloads and distributed quantum services [4,5]. Concurrently, there is an imperative to transition classical infrastructures to post-quantum cryptographic schemes in anticipation of future cryptanalytic capabilities enabled by large-scale quantum computers, thereby mitigating the security risks associated with advances in quantum computation [4,5]. These long-horizon goals require sustained research on scalable architectures, error correction cost reduction, and network protocols, as well as coordinated planning across technical, industry, and policy domains [4,5]."
  ]
}