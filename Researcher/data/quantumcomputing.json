{
  "topic": "Quantum Computing",
  "source": "both",
  "wikipedia_docs": "Index: 1\nTitle: Quantum computing\nSource: https://en.wikipedia.org/wiki/Quantum_computing\nContent: A quantum computer is a (real or theoretical) computer that uses quantum mechanical phenomena in an essential way: it exploits superposed and entangled states, and the intrinsically non-deterministic outcomes of quantum measurements, as features of its computation. Quantum computers can be viewed as sampling from quantum systems that evolve in ways classically described as operating on an enormous number of possibilities simultaneously, though still subject to strict computational constraints. By contrast, ordinary (\"classical\") computers operate according to deterministic rules. Any classical computer can, in principle, be replicated by a (classical) mechanical device such as a Turing machine, with only polynomial overhead in time. Quantum computers, on the other hand are believed to require exponentially more resources to simulate classically. It is widely believed that a scalable quantum computer could perform some calculations exponentially faster than any classical computer. Theoretically, a large-scale quantum computer could break some widely used public-key cryptographic schemes and aid physicists in performing physical simulations. However, current hardware implementations of quantum computation are largely experimental and only suitable for specialized tasks.\nThe basic unit of information in quantum computing, the qubit (or \"quantum bit\"), serves the same function as the bit in ordinary or \"classical\" computing. However, unlike a classical bit, which can be in one of two states (a binary), a qubit can exist in a linear combination of two states known as a quantum superposition. The result of measuring a qubit is one of the two states given by a probabilistic rule. If a quantum computer manipulates the qubit in a particular way, wave interference effects amplify probability of the desired measurement result. The design of quantum algorithms involves creating procedures that allow a quantum computer to perform this amplification.\nQuantum computers are not yet practical for real-world applications. Physically engineering high-quality qubits has proven to be challenging. If a physical qubit is not sufficiently isolated from its environment, it suffers from quantum decoherence, introducing noise into calculations. National governments have invested heavily in experimental research aimed at developing scalable qubits with longer coherence times and lower error rates. Example implementations include superconductors (which isolate an electrical current by eliminating electrical resistance) and ion traps (which confine a single atomic particle using electromagnetic fields). Researchers have claimed, and are widely believed to be correct, that certain quantum devices can outperform classical computers on narrowly defined tasks, a milestone referred to as quantum advantage or quantum supremacy. These tasks are not necessarily useful for real-world applications.\n\n\n== History ==\n\nFor many years, the fields of quantum mechanics and computer science formed distinct academic communities. Modern quantum theory developed in the 1920s to explain perplexing physical phenomena observed at atomic scales, and digital computers emerged in the following decades to replace human computers for tedious calculations. Both disciplines had practical applications during World War II; computers played a major role in wartime cryptography, and quantum physics was essential for nuclear physics used in the Manhattan Project.\nAs physicists applied quantum mechanical models to computational problems and swapped digital bits for qubits, the fields of quantum mechanics and computer science began to converge. In 1980, Paul Benioff introduced the quantum Turing machine, which uses quantum theory to describe a simplified computer.\nWhen digital computers became faster, physicists faced an exponential increase in overhead when simulating quantum dynamics, prompting Yuri Manin and Richard Feynman to independently suggest that hardware based on quantum phenomena might be more efficient for computer simulation.\nIn a 1984 paper, Charles Bennett and Gilles Brassard applied quantum theory to cryptography protocols and demonstrated that quantum key distribution could enhance information security.\nQuantum algorithms then emerged for solving oracle problems, such as Deutsch's algorithm in 1985, the Bernstein–Vazirani algorithm in 1993, and Simon's algorithm in 1994.\nThese algorithms did not solve practical problems, but demonstrated mathematically that one could gain more information by querying a black box with a quantum state in superposition, sometimes referred to as quantum parallelism.\n\nPeter Shor built on these results with his 1994 algorithm for breaking the widely used RSA and Diffie–Hellman encryption protocols, which drew significant attention to the field of quantum computing. In 1996, Grover's algorithm established a quantum speedup for the widely applicable unstructured search problem. The same year, Seth Lloyd proved that quantum computers could simulate quantum systems without the exponential overhead present in classical simulations, validating Feynman's 1982 conjecture.\nOver the years, experimentalists have constructed small-scale quantum computers using trapped ions and superconductors.\nIn 1998, a two-qubit quantum computer demonstrated the feasibility of the technology, and subsequent experiments have increased the number of qubits and reduced error rates.\nIn 2019, Google AI and NASA announced that they had achieved quantum supremacy with a 54-qubit machine, performing a computation that is impossible for any classical computer.\nThis announcement was met with a rebuttal from Google's direct competitor, IBM. IBM contended that the calculation Google claimed would take 10,000 years could be performed in just 2.5 days on its own Summit supercomputer if its architecture were optimized, sparking a debate over the precise threshold for \"quantum supremacy\".\n\n\n== Quantum information processing ==\nComputer engineers typically describe a modern computer's operation in terms of classical electrodynamics.\nWithin these \"classical\" computers, some components (such as semiconductors and random number generators) may rely on quantum behavior, but these components are not isolated from their environment, so any quantum information quickly decoheres.\nWhile programmers may depend on probability theory when designing a randomized algorithm, quantum mechanical notions like superposition and interference are largely irrelevant for program analysis.\nQuantum programs, in contrast, rely on precise control of coherent quantum systems. Physicists describe these systems mathematically using linear algebra. Complex numbers model probability amplitudes, vectors model quantum states, and matrices model the operations that can be performed on these states. Programming a quantum computer is then a matter of composing operations in such a way that the resulting program computes a useful result in theory and is implementable in practice.\nAs physicist Charlie Bennett describes the relationship between quantum and classical computers,\n\nA classical computer is a quantum computer ... so we shouldn't be asking about \"where do quantum speedups come from?\" We should say, \"well, all computers are quantum. ... Where do classical slowdowns come from?\"\n\n\n=== Quantum information ===\nJust as the bit is the basic concept of classical information theory, the qubit is the fundamental unit of quantum information. The same term qubit is used to refer to an abstract mathematical model and to any physical system that is represented by that model. A classical bit, by definition, exists in either of two physical states, which can be denoted 0 and 1. A qubit is also described by a state, and two states often written \n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n      \n    \n    {\\displaystyle |0\\rangle }\n  \n and \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle |1\\rangle }\n  \n serve as the quantum counterparts of the classical states 0 and 1. However, the quantum states \n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n      \n    \n    {\\displaystyle |0\\rangle }\n  \n and \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle |1\\rangle }\n  \n belong to a vector space, meaning that they can be multiplied by constants and added together, and the result is again a valid quantum state. Such a combination is known as a superposition of \n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n      \n    \n    {\\displaystyle |0\\rangle }\n  \n and \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle |1\\rangle }\n  \n.\nA two-dimensional vector mathematically represents a qubit state. Physicists typically use Dirac notation for quantum mechanical linear algebra, writing \n  \n    \n      \n        \n          |\n        \n        ψ\n        ⟩\n      \n    \n    {\\displaystyle |\\psi \\rangle }\n  \n 'ket psi' for a vector labeled \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n . Because a qubit is a two-state system, any qubit state takes the form \n  \n    \n      \n        α\n        \n          |\n        \n        0\n        ⟩\n        +\n        β\n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle \\alpha |0\\rangle +\\beta |1\\rangle }\n  \n , where \n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n      \n    \n    {\\displaystyle |0\\rangle }\n  \n and \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle |1\\rangle }\n  \n are the standard basis states, and \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n and \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n are the probability amplitudes, which are in general complex numbers. If either \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n or \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n is zero, the qubit is effectively a classical bit; when both are nonzero, the qubit is in superposition. Such a quantum state vector acts similarly to a (classical) probability vector, with one key difference: unlike probabilities, probability amplitudes are not necessarily positive numbers. Negative amplitudes allow for destructive wave interference.\nWhen a qubit is measured in the standard basis, the result is a classical bit. The Born rule describes the norm-squared correspondence between amplitudes and probabilities—when measuring a qubit \n  \n    \n      \n        α\n        \n          |\n        \n        0\n        ⟩\n        +\n        β\n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle \\alpha |0\\rangle +\\beta |1\\rangle }\n  \n, the state collapses to \n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n      \n    \n    {\\displaystyle |0\\rangle }\n  \n with probability \n  \n    \n      \n        \n          |\n        \n        α\n        \n          \n            |\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle |\\alpha |^{2}}\n  \n, or to \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n      \n    \n    {\\displaystyle |1\\rangle }\n  \n with probability \n  \n    \n      \n        \n          |\n        \n        β\n        \n          \n            |\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle |\\beta |^{2}}\n  \n.\nAny valid qubit state has coefficients \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n and \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n such that \n  \n    \n      \n        \n          |\n        \n        α\n        \n          \n            |\n          \n          \n            2\n          \n        \n        +\n        \n          |\n        \n        β\n        \n          \n            |\n          \n          \n            2\n          \n        \n        =\n\n---\n\nIndex: 2\nTitle: Timeline of quantum computing and communication\nSource: https://en.wikipedia.org/wiki/Timeline_of_quantum_computing_and_communication\nContent: This is a timeline of quantum computing and communication.\n\n\n== 1960s ==\n\n\n=== 1968/69/70 ===\nStephen Wiesner invents conjugate coding.\n\n\n=== 1969 ===\n13 June – James L. Park (Washington State University, Pullman)'s paper is received by Foundations of Physics, in which he describes the non possibility of disturbance in a quantum transition state in the context of a disproof of quantum jumps in the concept of the atom described by Bohr.\n\n\n== 1970s ==\n\n\n=== 1973 ===\nAlexander Holevo's paper is published. The Holevo bound describes a limit of the quantity of classical information which is possible to quanta encode.\nCharles H. Bennett shows that computation can be done reversibly.\n\n\n=== 1975 ===\nR. P. Poplavskii publishes \"Thermodynamical models of information processing\" (in Russian) which shows the computational infeasibility of simulating quantum systems on classical computers, due to the superposition principle.\nRoman Stanisław Ingarden, a Polish mathematical physicist, submits the paper \"Quantum Information Theory\" in Reports on Mathematical Physics, vol. 10, pp. 43–72, published 1976. It is one of the first attempts at creating a quantum information theory, showing that Shannon information theory cannot directly be generalized to the quantum case, but rather that it is possible to construct a quantum information theory, which is a generalization of Shannon's theory, within the formalism of a generalized quantum mechanics of open systems and a generalized concept of observables (the so-called semi-observables).\n\n\n== 1980s ==\n\n\n=== 1980 ===\nPaul Benioff describes the first quantum mechanical model of a computer. In this work, Benioff showed that a computer could operate under the laws of quantum mechanics by describing a Schrödinger equation description of Turing machines, laying a foundation for further work in quantum computing. The paper was submitted in June 1979 and published in April 1980.\nYuri Manin briefly motivates the idea of quantum computing.\nTommaso Toffoli introduces the reversible Toffoli gate, which (together with initialized ancilla bits) is functionally complete for reversible classical computation.\n\n\n=== 1981 ===\nAt the first Conference on the Physics of Computation, held at the Massachusetts Institute of Technology (MIT) in May, Paul Benioff and Richard Feynman give talks on quantum computing. Benioff's talk built on his earlier 1980 work showing that a computer can operate under the laws of quantum mechanics. The talk was titled \"Quantum mechanical Hamiltonian models of discrete processes that erase their own histories: application to Turing machines\". In Feynman's talk, he observed that it appeared to be impossible to efficiently simulate the evolution of a quantum nature system on a classical computer, and he proposed a basic model for a quantum computer. Feynman's conjecture on a quantum simulating computer, published 1982, understood as – the reality of quantum mechanics expressed as an effective quantum system necessitates quantum computers, is conventionally accepted as a beginning of quantum computing.\n\n\n=== 1982 ===\nPaul Benioff further develops his original model of a quantum mechanical Turing machine.\nWilliam Wootters and Wojciech H. Zurek, and independently Dennis Dieks rediscover the no-cloning theorem of James L. Park.\n\n\n=== 1984 ===\nCharles Bennett and Gilles Brassard employ Wiesner's conjugate coding for distribution of cryptographic keys.\n\n\n=== 1985 ===\nDavid Deutsch, at the University of Oxford, England, describes the first universal quantum computer. Just as a Universal Turing machine can simulate any other Turing machine efficiently (Church–Turing thesis), so the universal quantum computer is able to simulate any other quantum computer with at most a polynomial slowdown.\nAsher Peres points out the need for quantum error correction schemes and discusses a repetition code for amplitude errors.\n\n\n=== 1988 ===\nYoshihisa Yamamoto and K. Igeta propose the first physical realization of a quantum computer, including Feynman's CNOT gate. Their approach uses atoms and photons and is the progenitor of modern quantum computing and networking protocols using photons to transmit qubits and atoms to perform two-qubit operations.\n\n\n=== 1989 ===\nGerard J. Milburn proposes a quantum-optical realization of a Fredkin gate.\nBikas Chakrabarti & collaborators from Saha Institute of Nuclear Physics, Kolkata, India, propose that quantum fluctuations could help explore rugged energy landscapes by escaping from local minima of glassy systems having tall but thin barriers by tunneling (instead of climbing over using thermal excitations), suggesting the effectiveness of quantum annealing over classical simulated annealing.\n\n\n== 1990s ==\n\n\n=== 1991 ===\nArtur Ekert at the University of Oxford, proposes entanglement-based secure communication.\n\n\n=== 1992 ===\nDavid Deutsch and Richard Jozsa propose a computational problem that can be solved efficiently with the deterministic Deutsch–Jozsa algorithm on a quantum computer, but for which no deterministic classical algorithm is possible. This was perhaps the earliest result in the computational complexity of quantum computers, proving that they were capable of performing some well-defined computation more efficiently than any classical computer.\nEthan Bernstein and Umesh Vazirani propose the Bernstein–Vazirani algorithm. It is a restricted version of the Deutsch–Jozsa algorithm where instead of distinguishing between two different classes of functions, it tries to learn a string encoded in a function. The Bernstein–Vazirani algorithm was designed to prove an oracle separation between complexity classes BQP and BPP.\nResearch groups at Max Planck Institute of Quantum Optics (Garching) and shortly after at NIST (Boulder) experimentally realize the first crystallized strings of laser-cooled ions. Linear ion crystals constitute the qubit basis for most quantum computing and simulation experiments with trapped ions.\n\n\n=== 1993 ===\nDaniel R. Simon, at Université de Montréal, Quebec, Canada, invent an oracle problem, Simon's problem, for which a quantum computer would be exponentially faster than a conventional computer. This algorithm introduces the main ideas which were then developed in Peter Shor's factorization algorithm.\n\n\n=== 1994 ===\nPeter Shor, at AT&T's Bell Labs in New Jersey, publishes Shor's algorithm. It would allow a quantum computer to factor large integers quickly. It solves both the factoring problem and the discrete log problem. The algorithm can theoretically break many of the cryptosystems in use today. Its invention sparked tremendous interest in quantum computers.\nThe first United States Government workshop on quantum computing is organized by NIST in Gaithersburg, Maryland, in autumn.\nIsaac Chuang and Yoshihisa Yamamoto propose a quantum-optical realization of a quantum computer to implement Deutsch's algorithm. Their work introduced dual-rail encoding for photonic qubits.\nIn December, Ignacio Cirac, at University of Castilla–La Mancha at Ciudad Real, and Peter Zoller at the University of Innsbruck propose an experimental realization of the controlled NOT gate with cold trapped ions.\n\n\n=== 1995 ===\nThe first United States Department of Defense workshop on quantum computing and quantum cryptography is organized by United States Army physicists Charles M. Bowden, Jonathan Dowling, and Henry O. Everitt; it took place in February at the University of Arizona in Tucson.\nPeter Shor proposes the first schemes for quantum error correction.\nChristopher Monroe and David J. Wineland at NIST (Boulder, Colorado) experimentally realize the first quantum logic gate – the controlled NOT gate – with trapped ions, following the Cirac-Zoller proposal.\nIndependently, Subhash Kak and Ronald Chrisley propose the first quantum neural network.\n\n\n=== 1996 ===\nLov Grover, at Bell Labs, invents the quantum database search algorithm. The quadratic speedup is not as dramatic as the speedup for factoring, discrete logs, or physics simulations. However, the algorithm can be applied to a much wider variety of problems. Any problem that can be solved by random, brute-force search, may take advantage of this quadratic speedup in the number of search queries.\nThe United States Government, particularly in a joint partnership of the Army Research Office (now part of the Army Research Laboratory) and the National Security Agency, issues the first public call for research proposals in quantum information processing.\nAndrew Steane designs Steane code for error correction.\nDavid DiVincenzo, of IBM, proposes a list of minimal requirements for creating a quantum computer, now called DiVincenzo's criteria.\nSeth Lloyd proves Feynman's conjecture on quantum simulation.\n\n\n=== 1997 ===\nDavid G. Cory, Amr Fahmy and Timothy Havel, and at the same time Neil Gershenfeld and Isaac Chuang at MIT publish the first papers realizing gates for quantum computers based on bulk nuclear spin resonance, or thermal ensembles. The technology is based on a nuclear magnetic resonance (NMR) machine, which is similar to the medical magnetic resonance imaging machine.\nAlexei Kitaev describes the principles of topological quantum computation as a method for dealing with the problem of decoherence.\nDaniel Loss and David DiVincenzo propose the Loss-DiVincenzo quantum computer, using as qubits the intrinsic spin-1/2 degree of freedom of individual electrons confined to quantum dots.\n\n\n=== 1998 ===\nThe first experimental demonstration of a quantum algorithm is reported. A working 2-qubit NMR quantum computer was used to solve Deutsch's problem by Jonathan A. Jones and Michele Mosca at Oxford University and shortly after by Isaac L. Chuang at IBM's Almaden Research Center, in California, and Mark Kubinec and the University of California, Berkeley together with coworkers at Stanford University in California and MIT in Massachusetts.\nThe first working 3-qubit NMR computer is reported.\nBruce Kane proposes a silicon-based nuclear spin quantum computer, using nuclear spins of individual phosphorus atoms in silicon as the qubits and donor electrons to mediate the coupling between qubits.\nThe first execution of Grover's algorithm on an NMR computer is reported.\nHidetoshi Nishimori & colleagues from Tokyo Institute of Technology show that a quantum annealing algorithm can perform better than classical simulated annealing under certain conditions.\nDaniel Gottesman and Emanuel Knill independently prove that a certain subclass of quantum computations can be efficiently emulated with classical resources (Gottesman–Knill theorem).\n\n\n=== 1999 ===\nSamuel L. Braunstein and collaborators show that none of the bulk NMR experiments performed to date contain any entanglement; the quantum states being too strongly mixed. This is seen as evidence that NMR computers would likely not yield a benefit over classical computers. It remains an open question, however, whether entanglement is necessary for quantum computational speedup.\nGabriel Aeppli, Thomas Rosenbaum and colleagues demonstrate experimentally the basic concepts of quantum annealing in a condensed matter system.\nYasunobu Nakamura and Jaw-Shen Tsai demonstrate that a superconducting circuit can be used as a qubit.\n\n\n== 2000s ==\n\n\n=== 2000 ===\nArun K. Pati and Samuel L. Braunstein prove the quantum no-deleting theorem. This is dual to the no-cloning theorem which shows that one cannot delete a copy of an unknown qubit. Together with the stronger no-cloning theorem, the no-deleting theorem has the implication that quantum information can neither be created nor be destroyed.\nThe first working 5-qubit NMR computer is demonstrated at the Technical University of Munich, Germany.\nThe first execution of order finding (part of Shor's algorithm) at IBM's Almaden Research Center and Stanford University is demonstrated.\nThe first working 7-qubit NMR computer is demonstrated at the Los Alamos National Laboratory in New Mexico.\nThe textbook, Quantum Comp\n\n---\n\nIndex: 3\nTitle: Superconducting quantum computing\nSource: https://en.wikipedia.org/wiki/Superconducting_quantum_computing\nContent: Superconducting quantum computing is a branch of solid state  physics and quantum computing that implements superconducting electronic circuits using superconducting qubits as artificial atoms, or quantum dots. For superconducting qubits, the two logic states are the ground state and the excited state, denoted \n  \n    \n      \n        \n          |\n        \n        g\n        ⟩\n        \n           and \n        \n        \n          |\n        \n        e\n        ⟩\n      \n    \n    {\\displaystyle |g\\rangle {\\text{ and }}|e\\rangle }\n  \n respectively. Research in superconducting quantum computing is conducted by companies such as Google, IBM, IMEC, BBN Technologies, Rigetti, and Intel.  Many recently developed QPUs (quantum processing units, or quantum chips) use superconducting architecture.\nAs of May 2016, up to 9 fully controllable qubits are demonstrated in the 1D array, and up to 16 in 2D architecture. In October 2019, the Martinis group, partnered with Google, published an article demonstrating novel quantum supremacy, using a chip composed of 53 superconducting qubits.\n\n\n== Background ==\nClassical computation models rely on physical implementations consistent with the laws of classical mechanics. Classical descriptions are accurate only for specific systems consisting of a relatively large number of atoms. A more general description of nature is given by quantum mechanics. Quantum computation studies quantum phenomena applications beyond the scope of classical approximation, with the purpose of performing quantum information processing and communication. Various models of quantum computation exist, but the most popular models incorporate concepts of qubits and quantum gates (or gate-based superconducting quantum computing).\nSuperconductors are implemented due to the fact that at low temperatures they have infinite conductivity and zero resistance. Each qubit is built using semiconductor circuits with an LC circuit: a capacitor and an inductor.\nSuperconducting capacitors and inductors are used to produce a resonant circuit that dissipates almost no energy, as heat  can disrupt quantum information. The superconducting resonant circuits are a class of artificial atoms that can be used as qubits. Theoretical and physical implementations of quantum circuits are widely different. Implementing a quantum circuit had its own set of challenges and must abide by DiVincenzo's criteria, conditions proposed by theoretical physicist David P DiVincenzo, which is set of criteria for the physical implementation of superconducting quantum computing, where the initial five criteria ensure that the quantum computer is in line with the postulates of quantum mechanics and the remaining two pertaining to the relaying of this information over a network.\nWe map the ground and excited states of these atoms to the 0 and 1 state as these are discrete and distinct energy values and therefore it is in line with the postulates of quantum mechanics. In such a construction however an electron can jump to multiple other energy states and not be confined to our excited state; therefore, it is imperative that the system be limited to be affected only by photons with energy difference required to jump from the ground state to the excited state. However, this leaves one major issue, we require uneven spacing between our energy levels to prevent photons with the same energy from causing transitions between neighboring pairs of states. Josephson junctions are superconducting elements with a nonlinear inductance, which is critically important for qubit implementation. The use of this nonlinear element in the resonant superconducting circuit produces uneven spacings between the energy levels.\n\n\n=== Qubits ===\nA qubit is a generalization of a bit (a system with two possible states) capable of occupying a quantum superposition of both states. A quantum gate, on the other hand, is a generalization of a logic gate describing the transformation of one or more qubits once a gate is applied given their initial state. Physical implementation of qubits and gates is challenging for the same reason that quantum phenomena are difficult to observe in everyday life given the minute scale on which they occur. One approach to achieving quantum computers is by implementing superconductors whereby quantum effects are macroscopically observable, though at the price of extremely low operation temperatures.\n\n\n=== Superconductors ===\nUnlike typical conductors, superconductors possess a critical temperature at which resistivity plummets to zero and conductivity is drastically increased. In superconductors, the basic charge carriers are pairs of electrons (known as Cooper pairs), rather than single fermions as found in typical conductors.  Cooper pairs are loosely bound and have an energy state lower than that of Fermi energy. Electrons forming Cooper pairs possess equal and opposite momentum and spin so that the total spin of the Cooper pair is an integer spin. Hence, Cooper pairs are bosons. Two such superconductors which have been used in superconducting qubit models are niobium and tantalum, both d-band superconductors.\n\n\n==== Bose–Einstein condensates ====\nOnce cooled to nearly absolute zero, a collection of bosons collapse into their lowest energy quantum state (the ground state) to form a state of matter known as Bose–Einstein condensate. Unlike fermions, bosons may occupy the same quantum energy level (or quantum state) and do not obey the Pauli exclusion principle. Classically, Bose-Einstein Condensate can be conceptualized as multiple particles occupying the same position in space and having equal momentum. Because interactive forces between bosons are minimized, Bose-Einstein Condensates effectively act as a superconductor. Thus, superconductors are implemented in quantum computing because they possess both near infinite conductivity and near zero resistance. The advantages of a superconductor over a typical conductor, then, are twofold in that superconductors can, in theory, transmit signals nearly instantaneously and run infinitely with no energy loss. The prospect of actualizing superconducting quantum computers becomes all the more promising considering NASA's recent development of the Cold Atom Lab in outer space where Bose-Einstein Condensates are more readily achieved and sustained (without rapid dissipation) for longer periods of time without the constraints of gravity.\n\n\n=== Electrical circuits ===\nAt each point of a superconducting electronic circuit (a network of electrical elements), the condensate wave function describing charge flow is well-defined by some complex probability amplitude. In typical conductor electrical circuits, this same description is true for individual charge carriers except that the various wave functions are averaged in macroscopic analysis,  making it impossible to observe quantum effects. The condensate wave function becomes useful in allowing design and measurement of macroscopic quantum effects. Similar to the discrete atomic energy levels in the Bohr model, only discrete numbers of magnetic flux quanta can penetrate a superconducting loop. In both cases, quantization results from complex amplitude continuity. Differing from microscopic implementations of quantum computers (such as atoms or photons), parameters of superconducting circuits are designed by setting (classical) values to the electrical elements composing them such as by adjusting capacitance or inductance.\nTo obtain a quantum mechanical description of an electrical circuit, a few steps are required. Firstly, all electrical elements must be described by the condensate wave function amplitude and phase rather than by closely related macroscopic current and voltage descriptions used for classical circuits. For instance, the square of the wave function amplitude at any arbitrary point in space corresponds to the probability of finding a charge carrier there. Therefore, the squared amplitude corresponds to a classical charge distribution. The second requirement to obtain a quantum mechanical description of an electrical circuit is that generalized Kirchhoff's circuit laws are applied at every node of the circuit network to obtain the system's equations of motion. Finally, these equations of motion must be reformulated to Lagrangian mechanics such that a quantum Hamiltonian is derived describing the total energy of the system.\n\n\n== Technology ==\n\n\n=== Manufacturing ===\nSuperconducting quantum computing devices are typically designed in the radio-frequency spectrum, cooled in dilution refrigerators below 15 mK and addressed with conventional electronic instruments, e.g. frequency synthesizers and spectrum analyzers. Typical dimensions fall on the range of micrometers, with sub-micrometer resolution, allowing for the convenient design of a Hamiltonian system with well-established integrated circuit technology. Manufacturing superconducting qubits follows a process involving lithography, depositing of metal, etching, and controlled oxidation as described in. Manufacturers continue to improve the lifetime of superconducting qubits and have made significant improvements since the early 2000s.\n\n\n=== Josephson junctions ===\n\nOne distinguishable attribute of superconducting quantum circuits is the use of Josephson junctions. Josephson junctions are an electrical element which does not exist in normal conductors. Recall that a junction is a weak connection between two leads of wire (in this case a superconductive wire) on either side of a thin layer of insulator material only a few atoms thick, usually implemented using shadow evaporation technique. The resulting Josephson junction device exhibits the Josephson Effect whereby the junction produces a supercurrent. An image of a single Josephson junction is shown to the right. The condensate wave function on the two sides of the junction are weakly correlated, meaning that they are allowed to have different superconducting phases. This distinction of nonlinearity contrasts continuous superconducting wire for which the wave function across the junction must be continuous. Current flow through the junction occurs by quantum tunneling, seeming to instantaneously \"tunnel\" from one side of the junction to the other. This tunneling phenomenon is unique to quantum systems. Thus, quantum tunneling is used to create nonlinear inductance, essential for qubit design as it allows a design of anharmonic oscillators for which energy levels are discretized (or quantized) with nonuniform spacing between energy levels, denoted \n  \n    \n      \n        Δ\n        E\n      \n    \n    {\\displaystyle \\Delta E}\n  \n. In contrast, the quantum harmonic oscillator cannot be used as a qubit as there is no way to address only two of its states, given that the spacing between every energy level and the next is exactly the same.\n\n\n== Qubit archetypes ==\nThe three primary superconducting qubit archetypes are the phase, charge and flux qubit. Many hybridizations of these archetypes exist including the fluxonium, transmon, Xmon, and quantronium. For any qubit implementation the logical quantum states \n  \n    \n      \n        {\n        \n          |\n        \n        0\n        ⟩\n        ,\n        \n          |\n        \n        1\n        ⟩\n        }\n      \n    \n    {\\displaystyle \\{|0\\rangle ,|1\\rangle \\}}\n  \n are mapped to different states of the physical system (typically to discrete energy levels or their quantum superpositions). Each of the three archetypes possess a distinct range of Josephson energy to charging energy ratio. Josephson energy refers to the energy stored in Josephson junctions when current passes through, and charging energy is the energy required for one Cooper pair to charge the junction's total capacitance. Josephson energy can be written as \n\n  \n    \n      \n        \n          U\n          \n            j\n          \n        \n        =\n        −\n        \n          \n            \n              \n                I\n\n---\n",
  "arxiv_docs": "Index: 1\nTitle: The Rise of Quantum Internet Computing\nPublished: 2022-08-01\nAuthors: Seng W. Loke\nSource: Arxiv research paper\nContent: arXiv:2208.00733v1  [cs.ET]  1 Aug 2022\nIEEE IOT MAGAZINE, VOL. XX, NO. X, X 2022\n1\nThe Rise of Quantum Internet Computing\nSeng W. Loke, Member, IEEE\nAbstract—This article highlights quantum Internet computing as referring to distributed quantum computing over the quantum Internet,\nanalogous to (classical) Internet computing involving (classical) distributed computing over the (classical) Internet. Relevant to\nquantum Internet computing would be areas of study such as quantum protocols for distributed nodes using quantum information for\ncomputations, quantum cloud computing, delegated veriﬁable blind or private computing, non-local gates, and distributed quantum\napplications, over Internet-scale distances.\nIndex Terms—quantum Internet computing, quantum Internet, distributed quantum computing, Internet computing, distributed\nsystems, Internet\n”This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this\nversion may no longer be accessible.”\n✦\n1\nINTRODUCTION\nT\nHERE have been tremendous developments in quantum\ncomputing, quantum cryptography, quantum commu-\nnications and the quantum Internet, and we have seen\nincreased investments and intensive research in quantum\ncomputing in recent years [1], [2]. The quantum Internet will\nnot necessarily replace the (classical) Internet we know and\nuse today, at least not in the near future, but can complement\nthe current Internet. The quantum Internet aims to enable\nrobust quantum teleportation (or transmission) of qubits,1\nand entanglement among qubits,2 over long Internet-scale\ndistances, which are key to many of the quantum protocols\nincluding quantum key distribution, quantum voting, and\nothers, as well as for non-local control of quantum gates.\nThere have been efforts to build quantum computers,\nand it remains to see if any one paradigm becomes the\ndominant or best way of building such quantum comput-\ners. At the same time, even as researchers develop more\npowerful quantum computers (supporting more qubits for\noperations, and at lower error rates), there is an opportunity\nfor connecting multiple quantum computers from differ-\nent sites to achieve much more complex quantum com-\nputations, i.e., inter-linking multiple quantum computers\non different sites to perform distributed computing with\na distributed system of quantum computers (or quantum\nprocessing units (QPUs) at different nodes), arriving at the\nnotion of distributed quantum computing, e.g., [3].\nWhile distributed quantum computing can involve mul-\ntiple QPUs next to each other or at the same site, with the\nquantum Internet, one can envision distributed quantum\n•\nSeng W. Loke is with the School of Information Technology, Deakin\nUniversity, Melbourne, Australia.\nE-mail: see https://www.deakin.edu.au/about-deakin/people/seng-loke.\nManuscript received X XX, 20XX; revised X XX, 20XX.\n1. A qubit is the basic unit of quantum information, and can be\nthought of as a two-state, or two-levelled, quantum-mechanical system,\nsuch as an electron’s spin, where the two levels are spin up and spin\ndown, or a photon’s polarization, where the two states are the vertical\npolarization and the horizontal polarization.\n2. Multiple qubits at different sites can share an entangled state, a\nsuperpositon of “specially correlated” states, to be used in distributed\nalgorithms.\ncomputing over nodes geographically far apart. As noted\nin [4], the idea is the quantum Internet as the “underly-\ning infrastructure of the Distributed Quantum Computing\necosystem.”\nThis article highlights the emerging area of distributed\nquantum computing over the quantum Internet, which we\nrefer to as quantum Internet computing, i.e., the idea of com-\nputing using quantumly connected distributed quantum\ncomputers over Internet-scale distances. Hence, quantum\nInternet computing is not a new concept in itself but a\nproposed “umbrella term” used here for the collection of\ntopics (listed below), from an analogy to (classical) Internet\ncomputing.\nInternet computing, where one does distributed comput-\ning but over Internet-scale distances and distributed sys-\ntems involve nodes connected via the Internet, is at the inter-\nsection of work in (classical) distributed computing and the\n(classical) Internet. Analogous to Internet computing, one\ncould ask the question of what would be at the intersection\nof work in distributed quantum computing and work on the\nquantum Internet, which brings us to the notion of quantum\nInternet computing.\nAlso, while the quantum Internet and distributed quan-\ntum computing are still nascent research areas, there are at\nleast three key topics which can be considered as relevant to\nquantum Internet computing:\n•\ndistributed quantum computing, including quantum\nprotocols from theoretical perspectives involving\ncommunication complexity studies, and distributed\nquantum computing via non-local or distributed\nquantum gates,\n•\nquantum cloud computing with a focus on delegat-\ning quantum computations, blind quantum comput-\ning, and verifying delegated quantum computations,\nand\n•\ncomputations and algorithms for the quantum Inter-\nnet including key ideas such as quantum entangle-\nment distillation, entanglement swapping, quantum\nIEEE IOT MAGAZINE, VOL. XX, NO. X, X 2022\n2\nrepeaters, and quantum Internet standards.3\nWe brieﬂy discuss the above topics in the following sections.\n2\nDISTRIBUTED QUANTUM COMPUTING\nDistributed quantum computing problems and quantum\nprotocols have been well-studied for over two decades,\nfrom a theoretical computer science perspective,4 many of\nwhich have their inspiration from classical distributed com-\nputing research. Quantum versions of classical distributed\ncomputing problems and protocols, and new forms of dis-\ntributed computing using quantum information, have been\nexplored, e.g., the distributed three-party product problem,\nthe distributed Deutsch-Jozsa promise problem and the\ndistributed intersection problem, demonstrating how, for\nsome problems, quantum information can enable fewer\nbits of communication to be used for a solution, and how\ncertain distributed computation problems can be solved\nwith quantum information, but cannot be solved classically.\nMany quantum protocols, including quantum coin ﬂipping,\nquantum leader election, quantum anonymous broadcast-\ning, quantum voting, quantum Byzantine Generals, quan-\ntum secret sharing, and quantum oblivious transfer, can\nbe viewed as “quantum versions” of classical distributed\ncomputing problems, and have been studied extensively.\nAnother area of study, which has also been considered\nas distributed quantum computing, is non-local gates, or\nthe non-local control of quantum gates, including early\nwork nearly over two decades ago.5 Key to performing\nsuch non-local control of quantum gates is the use of en-\ntanglement, which can be viewed as a resource for such\nnon-local computations. More recent work has looked at\nhow to partition the computations of distributed quantum\ncircuits over multiple QPUs, e.g., [3] as we mentioned earlier\n- with considerations including distributing computations\nin such a way as to optimize performance and to reduce the\nrequirements on entanglement, since if the entanglements\nrequired are generated at too low a rate, this will hold up\ncomputations. The key motivation here is to inter-link a\nset of quantum computers to form effectively a much more\npowerful quantum computer.\n3\nQUANTUM CLOUD COMPUTING AND DELEGAT-\nING QUANTUM COMPUTATIONS\nWe have seen big tech companies and startups offering\nquantum computing as a service similar to accessing other\ncloud service offerings, which is a fantastic resource for\nexperimentation and studies.\nMore formally, studies into delegating quantum com-\nputation from a client (which can be either classical, or\nalmost classical, i.e., with minimal capability to perform\n3. For example, see https://www.ietf.org/archive/id/draft-irtf-qirg-principles-10.html\n[last accessed: 1/8/2022]\n4. For example, see Buhrman and R¨ohrig’s paper dating back to\n2003: https://link.springer.com/chapter/10.1007/978-3-540-45138-9 1\n[last accessed: 1/8/2022]\n5. For example, see the work by Yimsiriwattana and Lomonaco\nJr.\nin\nhttps://arxiv.org/pdf/quant-ph/0402148.pdf\nand\na\ndistributed\nversion\nof\nShor’s\nfamous\nfactorization\nalgorithm\nhttps://arxiv.org/abs/2207.05976 [last accessed: 1/8/2022]\ncomputations such as teleporting qubits, applying simple\nPauli quantum operations, and doing basic measurements)\nwhich is much more restricted than the server (assumed\nto be a universal quantum computer) have been studied,\ncalled delegated quantum computing. And when the server\nis prevented from knowing the client’s inputs but still can\nperform delegated computations, by a technique such as\nthe quantum one-time pad (where the client applies Pauli\noperations to add uncertainty from the server’s perspective,\nthereby effectively encrypting the quantum inputs it sends\nto the server, and keeps track of operations it later needs\nto decrypt the outputs from the server), this is called blind\nquantum computing.\nIn order to be sure that the server does indeed perform\nthe required quantum operations delegated to it by the\nclient, the client can embed tests (or test runs) into the\ndelegated computations, so that the server (not being able\nto distinguish between tests and normal computations) can\nbe caught out if it did not perform the required compu-\ntations properly. That is, the client can verify if the server\nperformed the required quantum computations.6 Further\nabstractions for delegating quantum computations with\nsupporting cloud services continues to be investigated.\n4\nTHE QUANTUM INTERNET\nAs we mentioned earlier, work on the quantum Internet\nfocuses on how to efﬁciently enable robust entanglement\nshared among qubits over long geographical distances. If\ntwo nodes in different continents share entangled states,\nthen, this can be a resource to do non-local gates, i.e.,\nto perform distributed quantum computations, and enable\nquantum protocols over Internet-scale distances.\nThere have been the use of satellites to enable long dis-\ntance entanglement, as well as the use of optical ﬁbre cables\nto demonstrate entanglement. Key to the quantum Internet\nare ideas such as entanglement swapping and quantum\nrepeaters, including ideas such quantum distillation, to\nachieve high ﬁdelity distributed entangled states over long\ndistances, and quantum error correction - this continues to\nbe a research endeavour as mentioned earlier [2].\nThere are other interesting distributed quantum appli-\ncations to be considered including quantum cryptography,\nquantum sensing, and quantum positioning systems.\n5\nDISTRIBUTED QUANTUM COMPUTING OVER THE\nQUANTUM INTERNET: QUANTUM INTERNET COM-\nPUTING AND THE QUANTUM IOT?\nApart from the many quantum computers available over\nthe cloud by big tech and startups which work at very\nlow temperatures, room temperature quantum computers\nhave also started to emerge.7 This could pave the way\nfor quantum computers at the fog and at the edge, not\njust in remote clouds, and perhaps even mobile quantum\n6. An\nexcellent\nexample\nis\nthe\nwork\nby\nBroadbent\nat\nhttps://theoryofcomputing.org/articles/v014a011/\n[last\naccessed:\n1/8/2022]\n7. See https://spectrum.ieee.org/nitrogen-vacancy-diamond-quantum-computer-\nand also https://otd.harvard.edu/explore-innovation/technologies/scalable-room-t\n[last accessed: 1/8/2022]\nIEEE IOT MAGAZINE, VOL. XX, NO. X, X 2022\n3\ncomputers, or quantum computers embedded into every-\nday devices and objects, if ever! Will we then have the\nquantum Internet of Things (IoT)? The answer remains to\nbe seen, and “quantum entangled things across the world”\nwill likely complement the classical IoT. Future applications\nand potential of quantum Internet computing remains to\nbe investigated. Meanwhile, others have begun to look at\nthe connection between 6G networking and the quantum\nInternet [5].\nREFERENCES\n[1] W.\nKozlowski\nand\nS.\nWehner,\n“Towards\nlarge-scale\nq\n\n---\n\nIndex: 2\nTitle: Unconventional Quantum Computing Devices\nPublished: 2000-03-31\nAuthors: Seth Lloyd\nSource: Arxiv research paper\nContent: arXiv:quant-ph/0003151v1  31 Mar 2000\nUnconventional Quantum Computing Devices\nSeth Lloyd\nMechanical Engineering\nMIT 3-160\nCambridge, Mass. 02139\nAbstract: This paper investigates a variety of unconventional quantum computation de-\nvices, including fermionic quantum computers and computers that exploit nonlinear quan-\ntum mechanics. It is shown that unconventional quantum computing devices can in prin-\nciple compute some quantities more rapidly than ‘conventional’ quantum computers.\nComputers are physical: what they can and cannot do is determined by the laws\nof physics. When scientiﬁc progress augments or revises those laws, our picture of what\ncomputers can do changes. Currently, quantum mechanics is generally accepted as the\nfundamental dynamical theory of how physical systems behave. Quantum computers can\nin principle exploit quantum coherence to perform computational tasks that classical com-\nputers cannot [1-21]. If someday quantum mechanics should turn out to be incomplete\nor faulty, then our picture of what computers can do will change. In addition, the set\nof known quantum phenomena is constantly increasing: essentially any coherent quantum\nphenomenon involving nonlinear interactions between quantum degrees of freedom can\nin principle be exploited to perform quantum logic. This paper discusses how the revi-\nsion of fundamental laws and the discovery of new quantum phenomena can lead to new\ntechnologies and algorithms for quantum computers.\nSince new quantum eﬀects are discovered seemingly every day, let’s ﬁrst discuss two\nbasic tests that a phenomenon must pass to be able to function as a basis for quantum\ncomputation. These are 1) The phenomenon must be nonlinear, and 2) It must be coherent.\nTo support quantum logic, the phenomenon must involve some form of nonlinearity, e.g.,\na nonlinear interaction between quantum degrees of freedom. Without such a nonlinearity\nquantum devices, like linear classical devices, cannot perform even so simple a nonlinear\noperation as an AND gate.\nQuantum coherence is a prerequisite for performing tasks\nsuch as factoring using Shor’s algorithm [10], quantum simulation a la Feynman [11] and\nLloyd [12], or Grover’s data-base search algorithm [13], all of which require extended\nmanipulations of coherent quantum superpositions.\n1\nThe requirements of nonlinearity and coherence are not only necessary for a phe-\nnomenon to support quantum computation, they are also in principle suﬃcient. As shown\nin [14-15], essentially any nonlinear interaction between quantum degrees of freedom suf-\nﬁces to construct universal quantum logic gates that can be assembled into a quantum\ncomputer. In addition, the work of Preskill et al. [18] on robust quantum computation\nshows that an error rate of no more than 10−4 per quantum logic operation allows one to\nperform arbitrarily long quantum computations in principle.\nIn practice, of course, few if any quantum phenomena are likely to prove suﬃciently\ncontrollable to provide extended quantum computation. Promising devices under current\nexperimental investigation include ion traps [5,7], high ﬁnesse cavities for manipulating\nlight and atoms using quantum electrodynamics [6], and molecular systems that can be\nmade to compute using nuclear magnetic resonance [8-9]. Such devices store quantum\ninformation on the states of quantum systems such as photons, atoms, or nuclei, and\naccomplish quantum logic by manipulating the interactions between the systems via the\napplication of semiclassical potentials such as microwave or laser ﬁelds. We will call such\ndevices ‘conventional’ quantum computers, if only because such devices have actually been\nconstructed.\nThere is another sense in which such computers are conventional: although the de-\nvices described above have already been used to explore new regimes in physics and to\ncreate and investigate the properties of new and exotic quantum states of matter, they\nfunction according to well established and well understood laws of physics. Perhaps the\nmost striking examples of the ‘conventionality’ of current quantum logic devices are NMR\nquantum microprocessors that are operated using techniques that have been reﬁned for\nalmost half a century. Ion-trap and quantum electrodynamic quantum computers, though\ncertainly cutting edge devices, operate in a quantum electrodynamic regime where the\nfundamental physics has been understood for decades (that is not to say that new and\nunexpected physics does not arise frequently in this regime, rather that there is general\nagreement on how to model the dynamics of such devices).\nMake no mistake about it: a conventional quantum logic device is the best kind of\nquantum logic device to have around. It is exactly because the physics of nuclear magnetic\nresonance and quantum electrodynamics are well understood that devices based on this\nphysics can be used systematically to construct and manipulate the exotic quantum states\nthat form the basis for quantum computation.\nWith that recognition, let us turn to\n2\n‘unconventional’ quantum computers.\nPerhaps the most obvious basis for an unconventional quantum computer is the use\nof particles with non-Boltzmann statistics in a reﬁme where these statistics play a key role\nin the dynamics of the device. For example, Lloyd [16] has proposed the use of fermions\nas the fundamental carriers of quantum information, so that a site or state occupied by a\nfermion represents a 1 and an unoccupied site or state represents a 0. It is straightforward\nto design a universal quantum computer using a conditional hopping dynamics on an array\nof sites, in which a fermion hops from one site to another if only if other sites are occupied.\nIf the array is one-dimensional, then such a fermionic quantum computer is equivalent\nto a conventional quantum computer via the well-known technique of bosonization. If the\narray is two or more dimensional, however, a local operation involving fermions on the\nlattice cannot be mocked up by a local operation on a conventional quantum computer,\nwhich must explicitly keep track of the phases induced by Fermi statistics. As a result,\nsuch a fermionic computer can perform certain operations more rapidly than a conventional\nquantum computer. An obvious example of a problem that can be solved more rapidly on\na fermionic quantum computer is the problem of simulating a lattice fermionic system in\ntwo or more dimensions. To get the antisymmetrization right in second quantized form,\na conventional ‘Boltzmann’ quantum computer takes time proportional to Tℓd−1 where T\nis the time over which the simulation is to take place, ℓis the length of the lattice and\nd is the dimension, while a fermionic quantum computer takes time proportional to T.\n(Here we assume that the computations for both conventional and Fermionic quantum\ncomputers can take advantage of the intrinsic parallelizability of such simulations: if the\ncomputations are performed serially an additional factro of ℓd is required for both types\nof computer to update each site sequentially.)\nAs the lattice size ℓand the dimension d grow large, the diﬀerence between the two\ntypes of computer also grows large. Indeed, the problem of simulating fermions hopping\non a hypercube of dimension d as d →∞is evidently exponentially harder on a con-\nventional quantum computer than a Fermionic quantum computer.\nSince a variety of\ndiﬃcult problems such as the travelling-salesman problem and data-base search problem\ncan be mapped to particles hopping on a hypercube, it is interesting to speculate whether\nfermionic computers might provide an exponential speed-up on problems of interest in ad-\ndition to quantum simulation. No such problems are currently known, however. Fermionic\ncomputers could be realized in principle by manipulating the ways in which electrons and\n3\nholes hop from site to site on a semiconductor lattice (though problems of decoherence are\nlikely to be relatively severe for such systems).\nIt might also be possible to construct bosonic computers using photons, phonons, or\natoms in a Bose-Einstein condensate. Such systems can be highly coherent and support\nnonlinear interactions: phonons and photons can interact in a nonlinear fshion via their\ncommon nonlinear interaction with matter, and atoms in a Bose condensate can be made\nto interact bia quantum electrodynamics (by introduction of a cavity) or by collisions. So\nfar, however, the feature of Bose condensates that makes them so interesting from the point\nof view of physics — all particles in the same state — makes them less interesting from the\npoint of view of quantum computation. Many particles in the same state, which can be\nmanipulated coherently by a variety of techniques, explore the same volume of Hilbert space\nas a single particle in that state. As a result, it is unclear how such a bosonic system could\nprovide a speed-up over conventional quantum computation. More promising than Bose\ncondensates from the perspective of quantum computation and quantum communications,\nis the use of cavity quantum electrodynamics to ‘dial up’ or synthesize arbitrary states\nof the cavity ﬁeld. Such a use of bosonic states is important for the ﬁeld of quantum\ncommunications, which requires the ability to create and manipulate entangled states of\nthe electromagnetic ﬁeld.\nA third unconventional design for a quantum computer relies on ‘exotic’ statistics\nthat are neither fermionic nor bosonic. Kitaev has recently proposed a quantum computer\narchitecture based on ‘anyons,’ particles that when exchanged acquuire an arbitrary phase.\nExamples of anyons include two-dimensional topological defects in lattice systems of spins\nwith various symmetries. Kitaev noted that such anyons could perform quantum logic via\nAharonov-Bohm type interactions [19]. Preskill et al. have shown explicitly how anyonic\nsystems could compute in principle [20], and Lloyd et al.\nhave proposed methods of\nrealizing anyons using superconducting circuits (they could also in principle be constructed\nusing NMR quantum computers to mock up the anyonic dynamics in an eﬀectively two-\ndimensional space of spins) [21]. The advantage of using anyons for quantum computation\nis that their nonlocal topological nature can make them intrinsically error-correcting and\nvirtually immune to the eﬀects of noise and interference.\nAs the technologies of the microscale become better developed, more and more po-\ntential designs for quantum computers, both conventional and unconventional, are likely\nto arise. Additional technologies that could prove useful for the construction of quantum\n4\nlogic devices include photonic crystals, optical hole-burning techniques, electron spin res-\nonance, quantum dots, superconducting circuits in the quantum regime, etc. Since every\nquantum degree of freedom can in principle participate in a computation one cannot a\npriori rule out the possibility of using currently hard to control degrees of freedom such as\nquark and gluon in complex nuclei to process information. Needless to say, most if not all\nof the designs inspired by these technologies are likely to fail. There is room for optimism\nthat some such quantum computer designs will prove practicable, however.\nThe preceding unconventional designs for quantum computers were based on existing,\nexperimentally conﬁrmed physical phenomena (except in the case of non-abelian anyons).\nLet us now turn to designs based on speculative, hypothetical, and not yet veriﬁed phenom-\nena. (One of the most interesting of these phenomena is large-scale quantum computation\nitself: can we create and systematically transform entangled states involving hundreds or\nthousands of quantum variables?) A particularly powerful hypothesis from the point of\nview of quantum computation is that of nonlinear quantum mechanics.\nThe conventional picture of quantum mechanics is that it is linear in the sense that the\nsuperposition principle is obeyed exactly. (Of course, quantum systems can still exhibit\nnonlinear interaction\n\n---\n\nIndex: 3\nTitle: Geometrical perspective on quantum states and quantum computation\nPublished: 2013-11-20\nAuthors: Zeqian Chen\nSource: Arxiv research paper\nContent: arXiv:1311.4939v1  [quant-ph]  20 Nov 2013\nGeometrical perspective on quantum states and quantum computation\nZeqian Chen\nState Key Laboratory of Resonances and Atomic and Molecular Physics,\nWuhan Institute of Physics and Mathematics, Chinese Academy of Sciences,\n30 West District, Xiao-Hong-Shan, Wuhan 430071, China\nWe interpret quantum computing as a geometric evolution process by reformulating ﬁnite quantum\nsystems via Connes’ noncommutative geometry. In this formulation, quantum states are represented\nas noncommutative connections, while gauge transformations on the connections play a role of\nunitary quantum operations. Thereby, a geometrical model for quantum computation is presented,\nwhich is equivalent to the quantum circuit model. This result shows a geometric way of realizing\nquantum computing and as such, provides an alternative proposal of building a quantum computer.\nPACS numbers: 03.67.Lx, 03.65.Aa\nQuantum computation has the advantage of solving\neﬃciently some problems that are considered intractable\nby using conventional classical computation [1]. In this\ncontext, there are two remarkable algorithms found:\nShor’s factoring algorithm [2] and Grove’s search algo-\nrithm [3].\nBut it remains a challenge to ﬁnd eﬃcient\nquantum circuits that can perform these complicated\ntasks in practice, due to quantum decoherence. A cru-\ncial step in the theory of quantum computer has been\nthe discovery of error-correcting quantum codes [4] and\nfault-tolerant quantum computation [5, 6], which estab-\nlished a threshold theorem that proves that quantum de-\ncoherence can be corrected as long as the decoherence is\nsuﬃciently weak. To tackle this barrier, a revolutionary\nstrategy, topological quantum computation (see [7] and\nreferences therein), is to make the system immune to the\nusual sources of quantum decoherence, by involving the\nglobally robust topological nature of the computation.\nRecently, substantial progress in this ﬁeld has been made\non both theoretical and experimental fronts [8].\nIn this paper, we provide an alternative approach to\nquantum computation from a geometrical view of point.\nTo this end, we need to reformulate quantum mechanics\nvia Connes’ noncommutative geometry [9]. In this for-\nmulation, quantum states are represented as noncommu-\ntative connections, while gauge transformations on the\nconnections play a role of unitary quantum operations.\nIn this way, we present a geometrical model for quan-\ntum computation, which is equivalent to the quantum\ncircuit model. In this computational model, information\nis encoded in gauge states instead of quantum states and\nimplementing on gauge states is played by gauge transfor-\nmations. Therefore, our scheme shows a geometric way\nof realizing quantum computing and as such, provides an\nalternative proposal of building a quantum computer.\nLet H be a N dimensional Hilbert space associated\nwith a ﬁnite quantum system. Let A be the algebra of\nall (bounded) linear operators on H, and let U(A) = {u ∈\nA : uu∗= u∗u = I} with I being the unit operator on\nH. Given a selfadjoint operator D on H, (A, H, D) is a\nspectral triple in the sense of noncommutative geometry\n[9, 10]. A (noncommutative) connection on (A, H, D) is\ndeﬁned to be a selfadjoint operator V on H of the form\nthat follows\nV =\nX\nj\naj[D, bj]\n(1)\nwhere aj, bj ∈A and [a, b] = ab −ba. A gauge transform\non a connection V under u ∈U(A) is deﬁned as\nV 7−→Gu(V ) = uV u∗+ u[D, u∗].\n(2)\nFor avoiding triviality, we always assume that D ̸= 0 or\nI in what follows.\nFor any (pure) quantum state |ψ⟩⟨ψ| with ψ being a\nunit vector in H, we have\n|ψ⟩⟨ψ| = |ψ⟩⟨ϕ|i[D, b]|ϕ⟩⟨ψ|\nwhere i = √−1 and, b is a selfjoint operator on H such\nthat i[D, b] has eigenvalue 1 at |ϕ⟩. Such a selfjoint oper-\nator b always exists because D ̸= 0 or I. In this case,\n|ψ⟩⟨ψ| = ia∗[D, ba] −ia∗b[D, a]\n(3)\nwith a = |ϕ⟩⟨ψ|. Thus, every quantum state |ψ⟩⟨ψ| can\nbe represented as a connection, denoted by Vψ, i.e.,\nVψ = ia∗[D, ba] −ia∗b[D, a].\n(4)\nLet GD(H) be the set of all connections V which can\nbe written as V = Vψ + uDu∗−D with ψ being a unit\nvector in H and u ∈U(A). An element in GD(H) is said\nto be a gauge state on (A, H, D). Any quantum state is\nnecessarily a gauge state, but a gauge state need not to\nbe a quantum state. However, any gauge state V can be\nobtained from a quantum state by performing a gauge\ntransform. Indeed, if V = Vψ + uDu∗−D then V =\nGu(Vu∗ψ). Moreover, for any gauge state V on (A, H, D)\nwe have (see [11])\n• for any u ∈U(A), Gu(V ) is again a gauge state;\n• Guv(V ) = Gu(Gv(V )) for all u, v ∈U(A).\nTherefore, a gauge transform preserves gauge states.\nLet V be a gauge state which is prepared from a quan-\ntum state |ψ⟩⟨ψ| by operating a gauge transform Gu, i.e.,\n2\nV = Gu(Vψ). For any event E, the probability of E oc-\ncurring on V is\n⟨E⟩V = ⟨ψ|u∗Eu|ψ⟩.\n(5)\nNote that a gauge state may be prepared in several ways.\nHence, the probability of a event E occurring on a gauge\nstate V depends on the quantum state from which V is\nprepared.\nLet H be a selfadjoint operator on H. Assuming ut =\neitH for t ∈R, we have that the gauge transforms Vt =\nGt(V ) on a ﬁxed gauge state V under ut form a group\n(see [11]), that is,\nGt+s(V ) = Gt(Gs(V )).\n(6)\nThis yields a dynamical equation governed by the Hamil-\ntonian H for gauge states on (A, H, D) as follows [12]\nidVt\ndt = [Vt, H] + [D, H]\n(7)\nwith V0 = V. In particular, for a unit vector ψ we have\nVt = Gt(Vψ) = Vutψ + utDu∗\nt −D.\n(8)\nWe now turn to product of two spectral triples. Sup-\npose (Ai, Hi, Di), i = 1, 2, are two spectral triple associ-\nated with ﬁnite quantum systems. Put\nD = D1 ⊗I2 + I1 ⊗D2\n(9)\nwith Ii being the unit operator on Hi (i = 1, 2). Then\nD is a selfjoint operator on H1 ⊗H2. The spectral triple\n(A1⊗A2, H1⊗H2, D) is called the product of two spectral\ntriples (Ai, Hi, Di), i = 1, 2.\nNow we illustrate our scheme by using a qubit. Let\nH = C2 and\nσx =\n\u0014\n0 1\n1 0\n\u0015\n, σy =\n\u0014\n0 −i\ni\n0\n\u0015\n, σz =\n\u0014\n1\n0\n0 −1\n\u0015\n.\n(10)\nThen (M2, C2, D) is a spectral triple with D = σx, where\nM2 is the set of all 2×2 complex matrices. For |0⟩=\n\u0014\n1\n0\n\u0015\n,\nwe have\nV|0⟩=\n\u0014\n1 0\n0 0\n\u0015\n,\nGσx(V|0⟩) =\n\u0014\n0 0\n0 1\n\u0015\n,\nand\nGσy(V|0⟩) =\n\u0014\n0\n−2\n−2\n1\n\u0015\n,\nGσz(V|0⟩) =\n\u0014\n1\n−2\n−2\n0\n\u0015\n.\nFor |1⟩=\n\u0014\n0\n1\n\u0015\n, we have\nV|1⟩=\n\u0014\n0 0\n0 1\n\u0015\nand\nGσy(V|1⟩) =\n\u0014\n1\n−2\n−2\n0\n\u0015\n.\nHence Gσy(V|1⟩) = Gσz(V|0⟩) and so, the gauge state\nV =\n\u0014\n1\n−2\n−2\n0\n\u0015\ncan be prepared in two diﬀerent ways.\nWe are now ready to interpret quantum computation\nfrom a geometrical view of point.\nBut let us take a\nstep backward and discuss the standard quantum circuit\nmodel for computation [13]. Let H = (C2)⊗n, the tensor\nproduct of n copies of C2. A quantum circuit model on\nn qubits consists of\n• a initial state |ψ⟩, represented by a unit vector ψ ∈\nH;\n• a quantum circuit Γ = UNUN−1 · · · U1, where quan-\ntum “gates” Uk 1 ≤k ≤N, are unitary transfor-\nmations on either C2\ni or C2\ni ⊗C2\nj, 1 ≤i, j ≤n, the\nidentity on all remaining factors;\n• reading the output of the circuit Γ|ψ⟩by measuring\nthe ﬁrst qubit; the probability of observing |1⟩is\nP(Γ) = ⟨ψ|Γ∗Π1Γ|ψ⟩, where Π1 = |1⟩⟨1| ⊗I · · · ⊗I\nis the projection to |1⟩in the ﬁrst qubit.\nLet A = M2n. Put\nD =\nn\nX\ni=1\nI ⊗· · · ⊗I\n|\n{z\n}\ni−1\n⊗σx ⊗I · · · ⊗I\nwhere I is the identity on C2. A computational model\nbased on the spectral triple (A, H, D) is as follows:\n• Initialization of a gauge state Vψ in the spectral\ntriple (A, H, D), where ψ is a unit vector in H;\n• Gauge implementation of the computational pro-\ngram\nG(Γ) = GUN GUN−1 · · · GU1\nwhere “gates” GUk, 1 ≤k ≤N, are gauge transfor-\nmations induced by Uk;\n• Application of the projection operator Π1 for read-\ning the output of the computation G(Γ)(Vψ);\nthe probability of observing |1⟩is P(GΓ)\n=\n⟨ψ|Γ∗Π1Γ|ψ⟩because G(Γ)(Vψ) = GΓ(Vψ) (see\n[11]), i.e., G(Γ)(Vψ) = Γ|ψ⟩⟨ψ|Γ∗+ ΓDΓ∗−D.\nThus, we obtain a geometrical model on n qubits for\nquantum computation, which is evidently equivalent to\nthe quantum circuit model as described above. Due to\nthe essential role of gauge transformations played in this\ncomputational model, we call this scheme gauge quantum\ncomputation.\nAs illustration, we give the Deutsch-Jozsa algorithm\n[14] in gauge quantum computation. Let f : {0, 1}n 7→\n{0, 1} be a function that takes an n-bit into a bit. We\ncall f balanced if f(x) = 1 for exactly half of all possible\n3\nx and f(x) = 0 for the other half.\nGiven a function\nf that is either constant or balanced, we want to ﬁnd\nout which it is with certainty. More precisely, we select\none x ∈{0, 1}n and calculate f(x) with the result being\neither 0 or 1. What is the fewest number of queries that\nwe can make to determine whether or not f is constant?\nIn the classical case, at worst we will need to calculate f\n2n−1 + 1 times, because we may ﬁrst obtain 2n−1 zeros\nand will need one more query to decide.\nHowever, in\nthe setting of quantum computation we could achieve the\ngoal in just one query using the Deutsch-Jozsa algorithm.\nIn the sequel, we give a realization of the Deutsch-Jozsa\nalgorithm in gauge quantum computation.\nLet H = (C2)⊗(n+1) and A = M2n+1. Given a selfad-\njoint operator D on H that is not 0 or I, we get the desired\nspectral triple (A, H, D). For a given f, we deﬁne the as-\nsociated operator Uf on H as Uf|x, y⟩= |x, y ⊕f(x)⟩for\nx ∈{0, 1}n and y ∈{0, 1}. Recall that the Hadamard\noperator H on C2 is\nH =\n1\n√\n2\nX\nx,y∈{0,1}\n(−1)x·y|x⟩⟨y|\nwhere x·y signiﬁes ordinary multiplication. The following\nis the Deutsch-Jozsa algorithm in the setting of gauge\nquantum computation:\n• Initialization of a gauge state Vψ with ψ = |0⟩⊗n ⊗\n|1⟩;\n• Gauge implementation of the computational pro-\ngram G(Γ) = GH⊗n⊗IGUf GH⊗(n+1);\n• Application of the projection operator Π|0⟩⊗n for\nreading the output of the computation G(Γ)(Vψ),\nwhere Π|0⟩⊗n is the projection to |0⟩⊗n in the ﬁrst\nn qubits.\nThe ﬁnal gauge state is V = VΓψ + ΓDΓ∗−D with Γ =\n(H⊗n ⊗I)UfH⊗(n+1), where\nΓψ =\nX\nx,y∈{0,1}n\n(−1)x·y+f(x)\n2n\n|y⟩⊗|0⟩−|1⟩\n√\n2\n.\nSince the amplitude for the state |0⟩⊗n in the ﬁrst n\nqubits is P\nx(−1)f(x)/2n, the probability of observing 0\nis 1 if f is constant, or 0 if f is balanced. Thus we have\ntwo possibilities of obtaining the outcome zero or the\noutcome nonzero. In the ﬁrst case, f is certainly constant\nand in the second case f must be balanced. Therefore,\nwe only need to perform three times gauge transforms for\ndetermining whether or not f is constant.\nIn conclusion, we present a geometrical description of\nquantum computation via noncommutative geometry. In\nthis geometrical model, information is encoded in gauge\nstates and computational operation is implemented by\ngauge transforms instead of unitary transforms. In prin-\nciple, gauge transforms are easier to perform than uni-\ntary quantum operation [15]. Therefore, gauge quantum\ncomputation should be more accessible than the usual\nquantum circuit computation and as such, this provides\nan alternative proposal of building a quantum computer.\nThis work was supported in part by the NSFC under\nGrant No. 11171338 and National Basic Research Pro-\ngram of China under Grant No. 2012CB922102.\n[1] M. A. Nielsen, I. L. Chuang, Quantum Computation\nand Quantum Information (Cambridge University Press,\nCambridge, 2000).\n[2] P. Shor, Algorithms for quantum computation, discrete\nlogarithms and factoring, Proc. 35th Annual Symposium\non Foundations of Computer Science (IEEE Computer\nSociety Press, Los Alamitos, CA, 1994, 124-134).\n[3] L. Grover, Quantum mechanics helps in search for a nee-\ndle in a haystack, Phys. Rev. Lett. 79 (1997), 325-328.\n[4] P. Shor, Scheme for reducing decoherence in quantum\ncomputer memory, Phys. Rev. A 52 (1995), 2493-2496.\n[5] J. Preskill, Fault-tolerant quantum computation, arXiv:\nquant-ph/9712048, 1997.\n[6] P. Shor, Fault-tolerant quantum computation, Proc. 37th\nAnnual Symposium on Foundations of Computer Science\n(IEEE Computer Society Press, Los Alamitos, CA, 1996,\n56-65).\n[7] C. Nayak, S. H. Simon, A. Stern, M. Freedman, S. Das\nSarma, Non-Abelian anyons and topological quantum\ncomputation, Rev. Mod. Phys. 80 (2008), 1083-1159.\n[8] A.\nStern,\nN.\nH.\nLindner,\nTopological\n\n---\n",
  "knowledge": {
    "topic": "Quantum Computing",
    "introduction": "Concise, structured extraction of core concepts, methods, historical milestones, hardware platforms, theoretical models, distributed computing/networking aspects, and open challenges in quantum computing drawn from the provided documents.",
    "sources": [
      {
        "id": 1,
        "title": "Quantum computing",
        "authors": [
          "Wikipedia contributors"
        ],
        "source": "Wikipedia",
        "url": "https://en.wikipedia.org/wiki/Quantum_computing"
      },
      {
        "id": 2,
        "title": "Timeline of quantum computing and communication",
        "authors": [
          "Wikipedia contributors"
        ],
        "source": "Wikipedia",
        "url": "https://en.wikipedia.org/wiki/Timeline_of_quantum_computing_and_communication"
      },
      {
        "id": 3,
        "title": "Superconducting quantum computing",
        "authors": [
          "Wikipedia contributors"
        ],
        "source": "Wikipedia",
        "url": "https://en.wikipedia.org/wiki/Superconducting_quantum_computing"
      },
      {
        "id": 4,
        "title": "The Rise of Quantum Internet Computing",
        "authors": [
          "Seng W. Loke"
        ],
        "source": "arXiv",
        "url": "https://arxiv.org/abs/2208.00733"
      },
      {
        "id": 5,
        "title": "Unconventional Quantum Computing Devices",
        "authors": [
          "Seth Lloyd"
        ],
        "source": "arXiv",
        "url": "https://arxiv.org/abs/quant-ph/0003151"
      },
      {
        "id": 6,
        "title": "Geometrical perspective on quantum states and quantum computation",
        "authors": [
          "Zeqian Chen"
        ],
        "source": "arXiv",
        "url": "https://arxiv.org/abs/1311.4939"
      }
    ],
    "topics": [
      {
        "id": "t1",
        "title": "Foundations: Qubits, Superposition, Entanglement, Measurement",
        "summary_points": [
          "Qubit as fundamental unit: described by a normalized two-dimensional complex vector α|0⟩ + β|1⟩ with probability amplitudes α, β; measurement yields classical outcomes with probabilities |α|^2 and |β|^2 (Born rule) (source 1).",
          "Superposition: a single qubit can be in linear combination of basis states enabling parallel representation of possibilities; interference of amplitudes is key to algorithmic speedups (source 1).",
          "Entanglement: multi-qubit correlations beyond classical probability distributions; acts as a resource enabling nonlocal protocols and distributed quantum computation (sources 1, 4).",
          "Quantum states and operations are modeled with linear algebra: complex amplitudes (vectors), unitary operations (matrices), and projective measurements; quantum programs are sequences of unitary gates followed by measurements (source 1).",
          "Quantum coherence is necessary to maintain superpositions and entanglement for computation; decoherence (loss of isolation to environment) introduces noise and destroys quantum data (sources 1, 5).",
          "Non-classical features allowing computation: negative and complex amplitudes permit destructive/constructive interference; no-cloning and no-deleting theorems constrain information handling and cryptographic protocols (sources 1, 2)."
        ],
        "subtopics": [
          {
            "id": "t1.1",
            "title": "Mathematical description and alternative formalisms",
            "summary_points": [
              "Dirac notation: states as |ψ⟩, density operators for mixed states; operations as unitary evolutions and measurement projections (source 1).",
              "Geometric/noncommutative re-formulation: quantum states can be represented as connections in Connes' noncommutative geometry; gauge transforms correspond to unitary operations and yield a geometric equivalent to the circuit model (source 6).",
              "Requirements for computational viability: phenomena used for qubits must be both nonlinear (to implement gates beyond linear evolution) and coherent (to preserve superpositions during operations) (source 5)."
            ],
            "references": [
              1,
              5,
              6
            ]
          }
        ],
        "references": [
          1,
          5,
          6
        ]
      },
      {
        "id": "t2",
        "title": "Quantum Algorithms, Complexity and Demonstrated Speedups",
        "summary_points": [
          "Key algorithms demonstrating quantum advantage: Shor's algorithm (integer factorization and discrete log) giving exponential speedup over known classical algorithms; Grover's algorithm offering quadratic speedup for unstructured search; Deutsch–Jozsa, Bernstein–Vazirani, and Simon's algorithms illustrate oracle separations (sources 1, 2).",
          "Quantum simulation: Feynman's and Lloyd's ideas show quantum computers can efficiently simulate quantum systems that are intractable classically (source 1).",
          "Complexity viewpoint: BQP (bounded-error quantum polynomial time) is the class of problems efficiently solvable on a quantum computer; some oracle separations and complexity-theoretic results indicate tasks where quantum models outperform classical ones (sources 1, 2).",
          "Quantum advantage/supremacy: experimentally observed narrow, specialized tasks where quantum devices outperform classical supercomputers (e.g., Google's 2019 claim with a 54-qubit device) — practical usefulness of such demonstrations is task-dependent and sometimes disputed (source 1).",
          "Algorithmic design principle: exploit superposition and interference to amplify desired outcomes and suppress others; many algorithms rely on structured transforms (e.g., quantum Fourier transform) or amplitude amplification (source 1)."
        ],
        "subtopics": [
          {
            "id": "t2.1",
            "title": "Historical algorithm milestones",
            "summary_points": [
              "Early oracle algorithms: Deutsch (1985), Bernstein–Vazirani (1993), Simon (1994) established possible quantum advantages in oracle settings (source 2).",
              "Shor (1994) transformed the field by showing polynomial-time factoring on a quantum machine, motivating intense research and cryptographic concern (sources 1, 2).",
              "Grover (1996) introduced amplitude amplification yielding quadratic speedups applicable to broad classes of problems (sources 1, 2)."
            ],
            "references": [
              1,
              2
            ]
          },
          {
            "id": "t2.2",
            "title": "Quantum simulation and physics applications",
            "summary_points": [
              "Quantum computers can simulate quantum dynamics without exponential classical overhead, validating Feynman's conjecture (source 2).",
              "Simulation advantages target chemistry, condensed matter, and high-energy problems where classical methods scale poorly (sources 1, 5)."
            ],
            "references": [
              1,
              5,
              2
            ]
          }
        ],
        "references": [
          1,
          2,
          5
        ]
      },
      {
        "id": "t3",
        "title": "Physical Implementations and Qubit Technologies",
        "summary_points": [
          "Multiple physical platforms under active development: superconducting circuits, trapped ions, photonic systems, nuclear magnetic resonance (NMR), semiconductor spins, topological anyon approaches, and proposals using Bose–Einstein condensates — each trades off coherence time, gate speed, scalability, and control complexity (sources 1, 2, 3, 5).",
          "Engineering challenge: creating high-fidelity, long-lived, addressable qubits while minimizing coupling to environment; cryogenic operation (millikelvin) is required for many implementations, notably superconducting qubits (source 3).",
          "DiVincenzo's criteria enumerate practical requirements for a quantum computer: scalable qubits, initialization, long coherence times, universal gate set, readout, and (for communication) qubit interconversion and transmission (source 2)."
        ],
        "subtopics": [
          {
            "id": "t3.1",
            "title": "Superconducting qubits (design, Josephson junctions, archetypes)",
            "summary_points": [
              "Superconducting qubits are artificial atoms built from superconducting circuits (LC resonators plus Josephson junctions) that provide anharmonic energy levels mapping |g⟩, |e⟩ to logical |0⟩, |1⟩ (source 3).",
              "Josephson junctions: thin insulating barrier between superconductors enabling tunneling of Cooper pairs; their nonlinear inductance creates anharmonicity necessary to isolate two levels for qubit operation (source 3).",
              "Archetypes: charge, flux, and phase qubits and hybrid variants (transmon, Xmon, fluxonium, quantronium); these differ by Josephson to charging energy ratio and noise sensitivities (source 3).",
              "Manufacturing and control: devices patterned with lithography, cooled in dilution refrigerators (< 15 mK), controlled by microwave electronics; current industrial development by companies (Google, IBM, Rigetti, Intel) aims at scaling QPUs while reducing error rates (source 3)."
            ],
            "references": [
              3
            ]
          },
          {
            "id": "t3.2",
            "title": "Other platforms: trapped ions, NMR, photonics, topological systems",
            "summary_points": [
              "Trapped ions: qubits encoded in internal states of ions, gates implemented via laser-mediated interactions; strong coherence times and high-fidelity gates but challenges in scaling and interconnects (sources 1, 2).",
              "NMR/ensemble approaches historically demonstrated small algorithms (2–7 qubits) but suffer from lack of entanglement in bulk systems and scaling limitations (source 2).",
              "Photonic systems: good for communication and room-temperature operations; used in quantum communication, linear-optical quantum computing, and boson sampling experiments (sources 1, 5).",
              "Topological/anyonic approaches: exploit non-abelian anyons' intrinsic fault resistance via topological degrees of freedom (Kitaev, Preskill); promising for error robustness but require exotic physical systems still under active research (sources 2, 5)."
            ],
            "references": [
              1,
              2,
              5
            ]
          }
        ],
        "references": [
          3,
          1,
          2,
          5
        ]
      },
      {
        "id": "t4",
        "title": "Noise, Decoherence, Error Correction and Fault Tolerance",
        "summary_points": [
          "Decoherence: coupling of qubits to environment causes loss of coherence and entanglement, effectively introducing errors in quantum computations; isolation, materials, and control engineering are primary mitigation targets (sources 1, 3).",
          "Quantum error correction (QEC): encodes logical qubits into entangled states of multiple physical qubits to detect and correct errors without measuring encoded quantum information directly; Shor, Steane, and later codes establish frameworks for QEC (sources 2, 1).",
          "Fault tolerance and thresholds: theory shows arbitrarily long quantum computation is possible if physical gate/error rates are below a threshold (on order ~10^-4 per gate in many constructions); fault-tolerant designs impose overhead in qubits and operations (sources 5, 6, 2).",
          "Topological protection (e.g., anyons) and hardware improvements (materials, fabrication, cryogenics, control electronics) are complementary strategies to reduce physical error rates before applying QEC (sources 3, 5)."
        ],
        "subtopics": [
          {
            "id": "t4.1",
            "title": "Practical implications and DiVincenzo's criteria",
            "summary_points": [
              "DiVincenzo's list of practical requirements guides hardware design: scalable qubits, initialization, long coherence relative to gate times, universal gates, qubit-specific measurement, and two additional criteria for quantum communication (mapping and transmission of flying qubits) (source 2).",
              "Trade-offs: hardware choices (e.g., transmon vs flux qubits) reflect balancing coherence time vs gate speed vs connectivity vs fabrication practicality; QEC overheads motivate reducing base error rates via hardware improvements (sources 3, 5)."
            ],
            "references": [
              2,
              3,
              5
            ]
          }
        ],
        "references": [
          1,
          2,
          3,
          5,
          6
        ]
      },
      {
        "id": "t5",
        "title": "Distributed Quantum Computing and the Quantum Internet",
        "summary_points": [
          "Quantum Internet computing: concept analogous to classical Internet computing — interconnecting geographically distributed QPUs over entanglement-enabled links to perform distributed quantum computation and services (source 4).",
          "Key quantum-network primitives: entanglement distribution, entanglement swapping, quantum repeaters, entanglement distillation, and quantum error correction to extend high-fidelity entanglement over long distances (source 4).",
          "Distributed computation use-cases: non-local gates via shared entanglement, delegated quantum computation (client delegates to more capable remote server), blind quantum computing (privacy-preserving delegation), and verification of delegated computation by embedding tests (source 4).",
          "Practical enablers and demonstrations: satellite-based entanglement distribution and fiber-based links have shown long-distance entanglement; rates/fidelities and repeater development remain bottlenecks for Internet-scale quantum links (source 4)."
        ],
        "subtopics": [
          {
            "id": "t5.1",
            "title": "Delegated, blind and verifiable quantum computation",
            "summary_points": [
              "Delegated computation: weaker clients (classical or 'almost-classical') can send encrypted quantum inputs (quantum one-time pad) to servers to perform computations without revealing inputs (blind quantum computing) (source 4).",
              "Verification techniques: clients embed hidden tests so that a dishonest server cannot distinguish tests from actual computations; when tests pass, client gains assurance of correctness (source 4).",
              "Quantum cloud services: current industrial offerings allow remote access to QPUs; research focuses on protocols providing privacy, verifiability, and efficient delegation (source 4)."
            ],
            "references": [
              4
            ]
          },
          {
            "id": "t5.2",
            "title": "Network-level challenges and opportunities",
            "summary_points": [
              "Entanglement generation rate, fidelity, and loss management determine whether distributed quantum algorithms can proceed without being bottlenecked by link resources; partitioning circuits among QPUs trades local computation vs entanglement consumption (source 4).",
              "Potential applications beyond computing include quantum cryptography, quantum sensing, and quantum positioning systems; future integration with edge/fog computing and IoT scenarios is speculative but an area of interest (source 4)."
            ],
            "references": [
              4
            ]
          }
        ],
        "references": [
          4,
          1,
          2
        ]
      },
      {
        "id": "t6",
        "title": "Unconventional and Theoretical Models",
        "summary_points": [
          "Unconventional platforms might exploit particle statistics or novel physical effects (fermionic/bosonic carriers, anyons, Bose–Einstein condensates, nonlinear quantum mechanics) potentially yielding algorithmic or simulation advantages beyond conventional gate-model architectures (source 5).",
          "Fermionic quantum computers: encode information in occupancy of fermionic modes; for certain lattice fermion simulations (2D+), fermionic architectures can be asymptotically faster than bosonic/qubit encodings due to intrinsic antisymmetrization (source 5).",
          "Anyons and topological computation: non-abelian anyons provide topologically-protected operations less sensitive to local noise; braiding operations can implement logical gates with built-in error resilience (source 5).",
          "Nonlinear quantum mechanics (speculative): if quantum mechanics were nonlinear at some scale, new computational capabilities beyond the conventional quantum model could arise, but this remains hypothetical (source 5)."
        ],
        "subtopics": [
          {
            "id": "t6.1",
            "title": "Fermionic and bosonic encodings",
            "summary_points": [
              "Fermionic encodings can be natural for simulating fermionic systems (electrons); bosonic encodings (photons, phonons) are suited for communications and some computing models (source 5).",
              "Practical challenges: fermionic systems may have severe decoherence; bosonic systems (e.g., Bose–Einstein condensates) have limited usefulness for computation because many particles in same state do not explore larger Hilbert space than single particle in that state (source 5)."
            ],
            "references": [
              5
            ]
          },
          {
            "id": "t6.2",
            "title": "Topological (anyonic) quantum computation",
            "summary_points": [
              "Anyons in 2D systems acquire arbitrary phases on exchange; non-abelian anyons support braiding operations that can realize universal quantum gates and are intrinsically error-resistant because information is stored nonlocally (source 5).",
              "Topological approaches are promising for fault tolerance but require exotic condensed-matter realizations still under experimental development (source 5)."
            ],
            "references": [
              5,
              2
            ]
          }
        ],
        "references": [
          5,
          2
        ]
      },
      {
        "id": "t7",
        "title": "Alternative theoretical/formal perspectives and modeling",
        "summary_points": [
          "Gauge/geometrical view: quantum states can be represented as noncommutative connections; gauge transforms correspond to unitary gates — provides an equivalent geometric model to quantum circuits and suggests alternative realizations where gauge operations implement computation (source 6).",
          "Equivalence to circuit model: the geometrical/gauge model reproduces standard algorithms (e.g., Deutsch–Jozsa) showing that gauge transforms + measurement probabilities reproduce circuit outcomes and offering potential new insights into implementation pathways (source 6).",
          "Conceptual implications: alternative formalisms can reveal new implementation strategies (easier operations viewed as gauge transforms) and may inform error mitigation or design of novel quantum processors (source 6)."
        ],
        "subtopics": [
          {
            "id": "t7.1",
            "title": "Practical and conceptual outcomes of geometric reformulation",
            "summary_points": [
              "Provides a different perspective to encode information (gauge states) and implement operations (gauge transforms) which may suggest new control paradigms or hardware abstractions (source 6).",
              "Demonstrates formal equivalence to circuit model ensuring that algorithmic results and complexity analysis transfer between formalisms (source 6)."
            ],
            "references": [
              6
            ]
          }
        ],
        "references": [
          6
        ]
      },
      {
        "id": "t8",
        "title": "Historical timeline and milestones",
        "summary_points": [
          "Early theoretical foundations: Benioff's quantum Turing machine (1980), Feynman's proposal (1981/82) to use quantum systems to simulate quantum dynamics, and Deutsch's universal quantum computer model (1985) laid groundwork (sources 2, 1).",
          "Algorithmic breakthroughs: Simon (1993) provided an exponential oracle separation; Shor (1994) demonstrated polynomial-time factoring; Grover (1996) introduced amplitude-amplification search (source 2).",
          "Experimental progress: first small-scale quantum gates and demonstrations across multiple platforms (NMR, trapped ions, superconducting circuits) from late 1990s; notable recent claim of quantum supremacy by Google/NASA in 2019 with a 54-qubit superconducting device (sources 2, 1).",
          "Community growth: government and industry investments, workshops and public funding calls since the mid-1990s accelerated both theoretical and experimental work (source 2)."
        ],
        "subtopics": [
          {
            "id": "t8.1",
            "title": "Key experimental milestones",
            "summary_points": [
              "1998–2000: demonstration of small NMR processors and execution of parts of Shor's algorithm; 1999–2000 showed superconducting circuits and qubit ideas making experimental progress (source 2).",
              "2019: Google announced quantum supremacy using a 54-qubit superconducting processor; debates with classical supercomputer performance estimates highlighted the nuance in defining supremacy and usefulness (source 1)."
            ],
            "references": [
              2,
              1
            ]
          }
        ],
        "references": [
          2,
          1
        ]
      }
    ],
    "conclusion": "Quantum computing is an interdisciplinary field combining quantum physics, computer science, and engineering. Foundations rest on qubits, superposition, entanglement and unitary evolution; the algorithmic promise (e.g., Shor, Grover, quantum simulation) motivates development despite substantial practical challenges (decoherence, error correction, scaling). Multiple hardware modalities (superconducting circuits, trapped ions, photonics, topological anyons, unconventional fermionic/bosonic designs) are pursued, each with distinct trade-offs. Error correction and fault-tolerant thresholds set demanding engineering targets, while the concept of a quantum Internet enables distributed quantum computing, delegated/ blind computation, and new networked applications. Alternative theoretical perspectives (geometric/gauge formulations) and speculative models (nonlinear mechanics) expand conceptual horizons. Continued progress requires parallel advances in algorithms, hardware, materials, control, networking, and theory to realize practical, scalable quantum computation.",
    "conclusion_references": [
      1,
      2,
      3,
      4,
      5,
      6
    ]
  },
  "report_parts": [
    "## Foundations: Qubits, Superposition, Entanglement, Measurement\n\nA qubit is the fundamental unit of quantum information and is described by a normalized two-dimensional complex vector of the form α|0⟩ + β|1⟩, where α and β are complex probability amplitudes and |α|^2 + |β|^2 = 1. Measurement of a qubit in the computational basis yields classical outcomes with probabilities given by the Born rule, specifically |α|^2 for the outcome corresponding to |0⟩ and |β|^2 for the outcome corresponding to |1⟩ [1]. Quantum states and their transformations are naturally expressed in the language of linear algebra: state vectors (or density operators for mixed states) represent quantum information, unitary matrices represent reversible evolutions, and projective measurements capture the probabilistic extraction of classical data [1].\n\nSuperposition permits a single qubit to exist as a linear combination of basis states, enabling a parallel representation of possibilities that is distinct from classical probability distributions; the interference of complex amplitudes, through constructive and destructive combinations, is a central mechanism by which quantum algorithms can achieve computational speedups over classical counterparts [1]. Entanglement denotes correlations among multiple qubits that cannot be reduced to classical joint probability distributions; such nonclassical correlations function as a resource, enabling nonlocal protocols and forms of distributed quantum computation that have no direct classical analog [1]. Together, superposition and entanglement form the nonclassical substrate on which quantum programs—typically sequences of unitary gates followed by measurements—operate to manipulate and ultimately read out quantum information [1].\n\nMaintaining quantum coherence is necessary to preserve both superpositions and entanglement during computation; loss of isolation from the environment (decoherence) introduces noise that degrades these quantum resources and thereby destroys stored or processed quantum data [1,5]. The nonclassical character of quantum amplitudes—being complex-valued and allowing negative interference effects—enables both constructive and destructive interference patterns that lie at the heart of many quantum advantages, while fundamental constraints such as the no-cloning and no-deleting implications constrain how quantum information can be copied or erased and have direct consequences for information processing and cryptographic protocols [1].\n\n### Mathematical description and alternative formalisms\n\nThe standard formalism uses Dirac notation to represent pure states as kets |ψ⟩ and extends to mixed states via density operators; quantum operations are modeled by unitary evolutions for closed-system dynamics and by projective measurements (or more general measurement operators) to represent the probabilistic extraction of classical outcomes [1]. This linear-algebraic framework provides a compact and rigorous language for describing state preparation, gate application, and measurement in the circuit model of quantum computation [1].\n\nBeyond the conventional Hilbert-space formalism, quantum states and operations admit geometric and noncommutative reformulations: in one such approach states can be represented as connections within Connes' noncommutative geometry, and gauge transformations in that geometric picture correspond to unitary operations, yielding a geometric analogue of the circuit model [6]. This alternative viewpoint emphasizes structural and symmetry aspects of quantum theory and can provide different intuitions for quantum dynamics and control [6].\n\nFor physical implementations to be computationally viable, the phenomena that realize qubits and gates must satisfy additional practical requirements: they must support nonlinearity in the sense required to implement gates that produce nontrivial logical evolution beyond simple, uncontrolled linear effects, and they must preserve coherence sufficiently during operations to maintain superpositions and entanglement long enough for computation to proceed. Both nonlinearity (for implementable gate action) and coherence (for error-free superposition and entanglement) are therefore essential physical prerequisites for scalable quantum computation [5].",
    "## Quantum Algorithms, Complexity and Demonstrated Speedups\n\nQuantum computation has produced several landmark algorithms that provably outperform the best known classical methods for particular tasks. Shor's algorithm for integer factorization and the discrete logarithm problem demonstrates an exponential speedup over known classical algorithms, establishing a fundamental example of large-scale quantum advantage [1, 2]. Grover's algorithm provides a generic quadratic improvement for unstructured search and, more generally, for a wide class of problems amenable to amplitude amplification techniques [1, 2]. Early oracle-based results such as the Deutsch–Jozsa, Bernstein–Vazirani, and Simon algorithms served to illustrate separations between quantum and classical oracle models and clarified in what formal senses quantum procedures can outperform classical ones in query complexity settings [2].\n\nA central motivation for quantum computing derives from its ability to simulate quantum systems more efficiently than classical computers. This program originates in Feynman's insight and subsequent formalizations, which argue that quantum devices can efficiently reproduce quantum dynamics without incurring the exponential overhead typical of classical simulation methods [2]. Such simulation capabilities are viewed as particularly promising for domains where classical methods scale poorly: quantum simulation targets problems in quantum chemistry, condensed-matter physics, and high-energy physics that are otherwise intractable or require uncontrolled approximations on classical machines [1, 5].\n\nFrom a complexity-theoretic perspective, the class BQP (bounded-error quantum polynomial time) captures decision problems efficiently solvable on a quantum computer, and a variety of oracle separations and related complexity results indicate concrete tasks in which quantum models outperform classical counterparts [1, 2]. At the experimental level, demonstrations of so-called quantum advantage or supremacy have been reported for narrowly tailored problems; such demonstrations show quantum devices outperforming classical supercomputers on specialized tasks, exemplified by a reported 2019 experiment using a 54-qubit device, though the practical utility and generality of these demonstrations remain task-dependent and have been subject to debate [1]. The common algorithmic design principle underlying many quantum speedups is the exploitation of superposition and interference to amplify desired computational outcomes while suppressing undesired ones; implementations of this principle include structured transforms such as the quantum Fourier transform and techniques for amplitude amplification [1].\n\n### Historical algorithm milestones\n\nThe earliest milestones in quantum algorithmics established formal separations in oracle models and clarified the potential for quantum advantage. Deutsch's early formulation, followed by the Bernstein–Vazirani and Simon algorithms, demonstrated that quantum queries to oracles can reveal global properties of functions with fewer queries than classical procedures, thereby providing clear examples of quantum speedups in the query complexity setting [2]. These results framed the theoretical landscape in which later algorithms would be developed.\n\nShor's 1994 algorithm marked a pivotal moment by showing that integer factorization and the discrete logarithm problem can be solved in polynomial time on a quantum computer, a result that reshaped both research priorities and practical considerations in cryptography [1, 2]. Following this breakthrough, Grover's 1996 algorithm introduced amplitude amplification as a broadly applicable technique yielding a quadratic speedup for unstructured search problems; this approach has since been adapted and embedded within many quantum algorithmic frameworks [1, 2].\n\n### Quantum simulation and physics applications\n\nThe proposition that quantum devices can efficiently simulate quantum systems traces back to Feynman's proposal and subsequent formal work showing that quantum simulation can avoid the exponential scaling that plagues classical approaches to many-body quantum dynamics [2]. This theoretical foundation has motivated sustained interest in building quantum hardware capable of implementing Hamiltonian dynamics and related simulation tasks.\n\nPractical targets for quantum simulation include problems in quantum chemistry, condensed-matter physics, and certain high-energy physics contexts where classical numerical methods either scale poorly or rely on uncontrolled approximations; these application areas are frequently cited as near- to medium-term domains in which quantum simulators and quantum algorithms may demonstrate substantive advantages [1, 5].",
    "## Physical Implementations and Qubit Technologies\n\nMultiple physical platforms for quantum information processing are under active development, including superconducting circuits, trapped ions, photonic systems, nuclear magnetic resonance (NMR), semiconductor spins, topological anyon approaches, and proposals using Bose–Einstein condensates; each platform embodies different trade-offs among coherence time, gate speed, scalability, and control complexity [1, 2, 3, 5]. The central engineering challenge across these platforms is the creation of qubits that are simultaneously high-fidelity, long-lived, and individually addressable while minimizing unwanted coupling to the environment; meeting these requirements typically increases control complexity and imposes demanding materials and fabrication constraints [1, 2, 3, 5]. For several leading implementations, notably superconducting qubits, operation at cryogenic temperatures in dilution refrigerators (millikelvin base temperatures) is required to suppress thermal excitations and enable coherent quantum dynamics [3].\n\nA common evaluative framework for assessing the practicality of different implementations is DiVincenzo’s criteria, which list the operational capabilities required of a functioning quantum computer: a scalable physical system of well-characterized qubits, the ability to initialize qubits to a known state, coherence times sufficient for computation, a universal set of quantum gates, and reliable, qubit-specific measurement; for quantum communication tasks the criteria further include the ability to interconvert stationary and flying qubits and to transmit qubits between locations [2]. These criteria provide a shared vocabulary for comparing technologies and for identifying concrete technical targets—such as initialization fidelity, gate error rates, and measurement efficiency—that experimental and engineering efforts must address [2].\n\nBecause no single platform presently optimizes all desirable attributes, research pursues both platform-specific improvements and strategies that combine complementary strengths; for example, the communication advantages of photonic systems have motivated proposals where photonics serves communication functions while matter-based qubits focus on information processing, reflecting pragmatic divisions of role rather than a single universal solution [1, 2, 3, 5]. Industrial and academic groups are concurrently concentrating on reducing error rates and scaling qubit counts within particular architectures through advances in materials, device design, and control systems [3].\n\n### Superconducting qubits (design, Josephson junctions, archetypes)\n\nSuperconducting qubits are engineered, macroscopic “artificial atoms” formed from superconducting circuits in which linear circuit elements (inductors and capacitors) are combined with Josephson junctions to produce discrete, anharmonic energy spectra; the lowest two energy levels are used to represent the logical qubit states (often labeled |g⟩ and |e⟩ or |0⟩ and |1⟩) [3]. The deliberate introduction of anharmonicity by the Josephson element is essential for isolating a two-level subspace from the otherwise harmonic spectrum of an LC resonator, thereby enabling controlled qubit manipulations and gate implementations [3].\n\nThe Josephson junction itself consists of a thin insulating barrier between superconductors that permits tunneling of Cooper pairs; the junction’s nonlinear inductance underlies the circuit’s anharmonicity and exerts primary control over spectroscopic and dynamical properties of the qubit [3]. Varied design choices, particularly the ratio of Josephson energy to charging energy, give rise to different archetypes—historical classes such as charge, flux, and phase qubits and modern hybrid designs including the transmon, Xmon, fluxonium, and quantronium—each exhibiting distinct sensitivities to noise and different operational trade-offs [3].\n\nFabrication of superconducting qubits uses planar lithographic patterning and thin-film processes familiar from microelectronics, and devices are cooled in dilution refrigerators to millikelvin temperatures (typical base temperatures below ≈15 mK) to maintain superconductivity and suppress thermal excitations [3]. Control and readout are implemented using microwave electronic systems that address and measure individual devices, and active industrial development by several companies seeks to scale quantum processing units and reduce gate and readout error rates through improved materials, device designs, and control approaches [3].\n\n### Other platforms: trapped ions, NMR, photonics, topological systems\n\nTrapped-ion platforms encode qubits in internal electronic or hyperfine states of ions confined by electromagnetic fields; entangling operations are generally mediated by laser-driven couplings to collective motional modes. These systems are notable for long coherence times and high-fidelity gate implementations, but they face practical challenges in scaling to large qubit numbers and in engineering efficient interconnects for modular architectures [1, 2].\n\nNuclear magnetic resonance (NMR) and related ensemble approaches played a historical role in early demonstrations of small quantum algorithms implemented across a few qubits (typically on the order of 2–7 qubits); however, bulk ensemble NMR implementations exhibit limitations for scalable quantum computation, including issues related to generating and using entanglement at scale and inherent barriers to extending these methods to large qubit counts [2]. Photonic systems are well suited to quantum communication and can operate at or near room temperature; they underpin proposals for linear-optical quantum computing and have been central to experimental platforms such as boson sampling. While photonics offers clear advantages for communication-oriented tasks, practical hurdles remain for realizing scalable, universal photonic quantum computing and are an active subject of research [1, 5].\n\nTopological approaches aim to encode and manipulate quantum information in nonlocal, topologically protected degrees of freedom—most prominently through non-abelian anyons—thereby providing intrinsic resilience against certain classes of local errors. This intrinsic fault-resistance makes topological schemes a promising route toward robust quantum information processing, but realizing the requisite exotic physical systems and demonstrating controllable anyonic operations remain active and challenging areas of theoretical and experimental work [2, 5].",
    "## Noise, Decoherence, Error Correction and Fault Tolerance\n\nDecoherence arises from the coupling of qubits to their surrounding environment, producing loss of phase coherence and degradation of entanglement that manifests as errors in quantum computations; mitigation of decoherence therefore focuses on improving isolation, materials, and control engineering to reduce unwanted environmental coupling [1, 3]. This environmental interaction effectively converts coherent quantum information into classical noise processes, making the control of materials properties and the engineering of control signals central to practical quantum processor design [1, 3]. Efforts to limit decoherence thus operate both at the device-material level and at the systems-control level to extend coherence times relative to operational timescales [1, 3].\n\nQuantum error correction (QEC) provides a systematic means to protect quantum information by encoding logical qubits into entangled states of multiple physical qubits so that errors can be detected and corrected without directly measuring the encoded quantum information; foundational constructions such as the Shor and Steane codes established the general framework for these encodings and subsequent developments have built on these principles [2, 1]. By distributing logical information across several physical degrees of freedom, QEC transforms local physical errors into syndromes that can be measured and corrected while preserving the underlying quantum state, thereby addressing the principal error mechanisms introduced by decoherence and imperfect control [2, 1].\n\nFault-tolerance theory demonstrates that arbitrarily long quantum computations become feasible provided physical gate and error rates can be reduced below a certain threshold; many fault-tolerant constructions place this threshold on the order of 10^-4 error per gate, although achieving it incurs substantial overheads in both qubit count and the number of operations required for encoded, fault-tolerant gates [5, 6, 2]. Because these fault-tolerant architectures impose significant resource overhead, complementary strategies that reduce base physical error rates are crucial: topological protection schemes (for example, approaches based on anyons) and hardware-focused improvements in materials, fabrication, cryogenics, and control electronics serve to lower the physical error burden prior to application of QEC, thereby reducing the overall required overhead for fault-tolerant operation [3, 5].\n\n### Practical implications and DiVincenzo's criteria\n\nDiVincenzo’s criteria enumerate the practical requirements that guide quantum hardware design: a scalable physical system of well-characterized qubits, the ability to initialize qubits to a known state, coherence times long compared to gate operation times, a universal set of quantum gates, qubit-specific measurement capability, and two additional criteria addressing the mapping to and transmission of flying qubits for quantum communication tasks [2]. These criteria serve as a checklist for assessing whether a candidate technology can support both computation and the communication of quantum information in realistic architectures [2].\n\nIn practice, hardware choices reflect trade-offs among coherence time, gate speed, qubit connectivity, and fabrication practicality; for example, design families such as transmons and flux qubits embody different balances of these factors and thus different implications for system-level performance and scalability [3, 5]. Because QEC and fault-tolerant constructions impose substantial overhead in physical qubits and operations, reducing the base physical error rate through improved materials, fabrication methods, cryogenics, and control electronics remains a priority: lowering physical error rates reduces the necessary QEC overhead and makes fault-tolerant thresholds more attainable in practical devices [3, 5].",
    "## Distributed Quantum Computing and the Quantum Internet\n\nDistributed quantum computing envisions an architecture analogous to classical Internet computing in which geographically distributed quantum processing units (QPUs) are interconnected by entanglement-enabled links to perform joint quantum computation and deliver quantum services across distances [4]. The essential enabling primitives for such a quantum network include entanglement distribution, entanglement swapping, quantum repeaters, entanglement distillation, and quantum error correction, each employed to establish and maintain high-fidelity entanglement between remote nodes over lossy channels [4]. These primitives together determine the capacity and reliability of the quantum links, and they must be orchestrated to extend usable entanglement over long distances for computational tasks [4].\n\nDistributed quantum computation offers several concrete use-cases that leverage shared entanglement: implementing non-local gates by consuming entanglement, supporting delegated quantum computation where weaker clients offload tasks to more capable remote servers, enabling blind quantum computing for privacy-preserving delegation, and allowing clients to verify delegated computations by embedding hidden tests within the delegated workload [4]. Practical progress toward a quantum Internet has been demonstrated through satellite-based entanglement distribution and fiber-based links that achieve long-distance entanglement, but current limitations in entanglement generation rates, achievable fidelities, and the immature development of repeaters remain bottlenecks for scaling to Internet-wide quantum connectivity [4]. Consequently, realizing Internet-scale distributed quantum computing requires both continued development of network primitives and careful co-design of algorithms and partitioning strategies that account for link resource constraints [4].\n\n### Delegated, blind and verifiable quantum computation\n\nDelegated quantum computation enables clients with limited quantum capabilities—ranging from nearly classical devices to simple quantum hardware—to offload quantum tasks to more powerful remote servers by transmitting encrypted quantum inputs using techniques such as the quantum one-time pad, thereby preventing the server from learning the client’s inputs [4]. Blind quantum computing formalizes this privacy-preserving delegation by combining encryption of inputs with protocol structures that keep the computation itself hidden from the server, allowing the client to obtain results without revealing sensitive information to the server [4]. To ensure correctness, verification techniques require the client to embed hidden tests within the delegated computation so that a dishonest server cannot distinguish tests from genuine computation; successful passage of these tests provides the client with statistical assurance that the service performed the computation correctly [4].\n\nThe emergence of quantum cloud services has already made remote access to QPUs available through industry offerings, and current research focuses on designing protocols that provide privacy, verifiability, and efficient use of communication and entanglement resources in such delegated settings [4]. These research efforts address both theoretical protocol guarantees and practical considerations for deploying delegation schemes on near-term quantum hardware accessible via cloud interfaces [4].\n\n### Network-level challenges and opportunities\n\nAt the network level, the feasibility of distributed quantum algorithms depends critically on entanglement generation rate, fidelity, and loss management; these link-level metrics determine whether a computation can proceed without being bottlenecked by network resources and influence how circuits should be partitioned across QPUs to trade local computation for entanglement consumption [4]. Partitioning strategies must therefore balance the overhead of creating and maintaining remote entanglement against the savings obtained from distributing computational load, with the goal of minimizing total resource consumption while preserving algorithmic correctness [4].\n\nBeyond distributed computation, a quantum Internet promises applications in quantum cryptography, enhanced quantum sensing, and quantum positioning systems, and it may ultimately be integrated with edge and fog computing paradigms as well as Internet-of-Things scenarios, although such integrations remain speculative and are an active area of interest [4]. The evolving landscape of network hardware demonstrations—satellite links, fiber channels, and ongoing repeater research—highlights both near-term opportunities for specialized long-distance quantum links and the persistent challenges that must be overcome to realize broad, Internet-scale quantum networking capabilities [4].",
    "## Unconventional and Theoretical Models\n\nUnconventional quantum-computing platforms seek to exploit particle statistics and other nonstandard physical effects—such as fermionic or bosonic carriers, anyonic excitations, Bose–Einstein condensates, and speculative nonlinear modifications of quantum mechanics—as alternative means to perform computation or simulation, potentially offering algorithmic or simulation advantages beyond conventional gate-model architectures [5]. These proposals aim to leverage intrinsic physical structure or symmetries to realize operations or state spaces that differ qualitatively from those of qubit-based systems, thereby opening new routes to computational tasks that may be difficult for standard architectures [5].\n\nEncodings that reflect the underlying particle statistics are especially prominent among these approaches. By mapping logical information directly onto the natural degrees of freedom of a physical system, such encodings can simplify representation and algorithm design for problems whose physics matches the chosen carriers; for example, fermionic encodings can be well matched to electronic structure problems, while bosonic carriers such as photons or phonons are well suited for communications and for computational models that exploit bosonic mode structure [5]. More exotic proposals include the use of non-abelian anyons to realize topologically protected operations that are intrinsically less sensitive to certain local errors, and speculative ideas—such as hypothetical nonlinearities in quantum mechanics—that, if realized, could imply computational capabilities beyond the conventional quantum model; however, such conjectures remain hypothetical and unestablished [5].\n\nWhile these unconventional and theoretical models present novel opportunities for algorithmic gain, they also confront substantial physical and foundational challenges. Practical limitations such as decoherence, the difficulty of engineering suitable condensed-matter phases, and foundational uncertainties about possible modifications of quantum mechanics must be addressed before potential advantages can be realized in working devices [5]. The balance of representational alignment, experimental feasibility, and error resilience will determine which of these approaches can mature into scalable computational platforms.\n\n### Fermionic and bosonic encodings\n\nFermionic encodings represent logical information in the occupation of fermionic modes, making them a natural choice for simulating fermionic systems such as electrons; for certain lattice fermion simulations in two or more spatial dimensions, architectures that natively incorporate fermionic antisymmetrization can exhibit asymptotic performance advantages relative to bosonic or qubit encodings because the required antisymmetry is built into the physical degrees of freedom [5]. This representational alignment can simplify mappings from target problems to hardware, potentially reducing overhead for specific simulation tasks [5].\n\nBosonic encodings exploit bosonic mode degrees of freedom and are typically implemented with carriers such as photons or phonons; these carriers are particularly appropriate for communication-oriented tasks and for computation models that take advantage of bosonic mode structure [5]. Practical challenges affect both classes of encodings: fermionic platforms may experience severe decoherence and other implementation difficulties that limit near-term practicality, while certain bosonic systems—examples include Bose–Einstein condensates—have limitations as computational resources because ensembles in which many bosons occupy the same quantum state do not necessarily explore a larger effective Hilbert space than a single particle in that state, constraining their straightforward usefulness for scaling computational resources [5]. These considerations emphasize the need to evaluate both physical feasibility and representational efficiency when assessing fermionic and bosonic approaches [5].\n\n### Topological (anyonic) quantum computation\n\nAnyons—quasiparticle excitations that can arise in two-dimensional systems—acquire nontrivial phases upon exchange, and in the non-abelian case the exchange (braiding) of such excitations enacts unitary transformations on a degenerate ground-state manifold; these braiding operations can, in principle, realize a set of logical gates for quantum computation while encoding information nonlocally, which reduces sensitivity to local noise and errors [5]. The nonlocal storage and topologically protected nature of the operations are the principal reasons non-abelian anyons are considered promising for intrinsically robust quantum processing [5].\n\nTopological quantum-computation approaches therefore offer a compelling route toward fault-tolerant processing, but they depend on the experimental realization of exotic condensed-matter phases that support the requisite anyonic excitations; creating, isolating, and controlling such phases and their braiding operations remains the subject of active experimental development [5, 2]. Consequently, while topological models provide attractive conceptual advantages for robustness and error suppression, their practical utility will be determined by progress in realizing and controlling the necessary two-dimensional anyonic systems in the laboratory [5, 2].",
    "## Alternative theoretical/formal perspectives and modeling\n\nThe gauge or geometrical view represents quantum states as noncommutative connections and interprets gauge transformations as the operational analogues of unitary gates, thereby providing an equivalent geometric model to conventional quantum circuits [6]. This perspective frames computation in terms of transformations of connection-like objects rather than sequences of discrete gate operations, suggesting that certain physical processes naturally described as gauge operations could be harnessed to perform computation [6].  \n\nThis geometrical model has been shown to reproduce standard quantum algorithms: for example, the gauge-based formalism can emulate the Deutsch–Jozsa algorithm, with sequences of gauge transforms combined with measurement probabilities yielding the same outcomes as circuit-based implementations [6]. The formal equivalence between the gauge description and the circuit model implies that algorithmic correctness and observable statistics carry over between the formalisms, allowing established algorithmic results to be mapped into the geometric language [6].  \n\nConceptually, adopting alternative formalisms such as the gauge/geometrical viewpoint can reveal new implementation strategies by identifying operations that are naturally simple in the geometric picture (i.e., certain gauge transforms) but may be nontrivial when expressed as sequences of discrete gates. This reframing has potential implications for error mitigation and for the architectural design of novel quantum processors, insofar as it highlights different low-level operations and control paradigms that could be exploited in hardware or control-layer abstractions [6].  \n\n### Practical and conceptual outcomes of geometric reformulation\n\nThe geometric reformulation provides a different perspective for encoding information—by treating computational states as gauge states—and for implementing operations by realizing gauge transforms as computational steps, which in turn may suggest alternative control paradigms or hardware abstractions distinct from conventional gate-based controllers [6]. Framing computation in terms of gauge degrees of freedom encourages consideration of physical platforms where such transformations are native or easier to implement, thereby informing pragmatic choices about how to realize quantum operations in practice [6].  \n\nBecause the geometric/gauge model is formally equivalent to the standard circuit model, algorithmic results, correctness proofs, and complexity analyses developed for circuits remain applicable when translated into the geometric setting; this ensures that theoretical guarantees and performance expectations transfer between the two formalisms [6]. The formal equivalence also facilitates comparative analysis of implementation pathways, enabling researchers to identify where the geometric viewpoint may offer practical advantages or simplifications relative to conventional gate decompositions [6].",
    "## Historical timeline and milestones\n\nThe conceptual foundations of quantum computing emerged in a series of theoretical contributions beginning in the 1980s. Benioff formulated an early model of a quantum Turing machine in 1980, Feynman proposed in 1981/82 that quantum systems could be used to simulate quantum dynamics more efficiently than classical machines, and Deutsch introduced a model of a universal quantum computer in 1985; these works collectively established the theoretical framework for subsequent developments in the field [2, 1]. Building on these foundations, the 1990s produced seminal algorithmic results that demonstrated rigorous separations between classical and quantum computational power: Simon's 1993 oracle result exhibited an exponential separation in query complexity, Shor's 1994 algorithm showed that integer factoring could be performed in polynomial time on a quantum computer, and Grover's 1996 algorithm introduced amplitude-amplification methods for quadratic-speedup in unstructured search problems [2].\n\nExperimental progress and community development proceeded in parallel with theoretical advances. From the late 1990s onward, small-scale implementations of quantum gates and demonstrations of elementary quantum algorithms were realized across multiple physical platforms, including nuclear magnetic resonance (NMR), trapped ions, and superconducting circuits, marking the transition from abstract models to laboratory demonstrations [2]. The expansion of interest and capability was reinforced by sustained growth in government and industry investments, workshops, and public funding calls beginning in the mid-1990s, which accelerated both theoretical and experimental research directions and fostered a broader research community engaged with quantum information science [2]. A notable milestone in this trajectory was a 2019 claim of quantum supremacy made by Google in collaboration with NASA, reporting results from a 54-qubit superconducting device; this announcement stimulated significant discussion concerning the appropriate metrics for supremacy and the comparison to classical supercomputer performance, underscoring the nuance involved in evaluating practical quantum advantage [2, 1].\n\n### Key experimental milestones\n\nLate-1990s experiments demonstrated early, small-scale quantum processors and partial implementations of key algorithms. Between approximately 1998 and 2000, researchers implemented small NMR-based processors and executed portions of Shor's algorithm on physically realized qubits, providing concrete proof-of-principle that algorithmic ideas could be mapped onto experimental apparatuses; contemporaneous work also showed experimental progress in superconducting circuits and in proposals for superconducting qubits during 1999–2000 [2].\n\nTwo decades after those early demonstrations, a high-profile experimental claim in 2019 marked a different stage of scale and public visibility. Google announced that a 54-qubit superconducting processor had achieved a task characterized as quantum supremacy; subsequent debate focused on the assumptions and estimates used to compare the quantum device's performance with that of classical supercomputers, illustrating that claims of supremacy depend critically on benchmarking methodology and on the distinction between demonstrating a milestone and delivering broadly useful computational advantage [1]."
  ],
  "criticism": {
    "0": "PASS",
    "1": "PASS",
    "2": "1) Unsupported/specific technical claims in the superconducting-qubit subsection\n- Problematic statements:\n  - \"deliver shaped pulses and perform dispersive measurement\"\n  - \"dispersive measurement\" (and \"shaped pulses\") presented as factual specifics of control/readout.\n- Why problematic:\n  - The INPUT JSON (topic t3.1) supports \"controlled by microwave electronics\" and \"readout\" but does not mention \"shaped pulses\" or \"dispersive measurement\" by name. These are technical details not present in the provided summaries and therefore are unsupported by the input.\n- Where to check in the JSON:\n  - t3.1 \"Superconducting qubits (design, Josephson junctions, archetypes)\" (references: 3).\n- Actionable fix:\n  - Either remove these specific terms or replace them with the broader, supported phrasing from the JSON (e.g., \"controlled by microwave electronics\" and \"readout\" / \"measurement\") unless you can cite a source that explicitly supports \"shaped pulses\" and \"dispersive measurement.\"\n\n2) Unsupported technical claim in the photonics sentence\n- Problematic statement:\n  - \"photon loss and efficient two-photon interactions pose practical hurdles for scalable, universal photonic quantum computing\"\n- Why problematic:\n  - The INPUT JSON (t3.2) states photonic systems are \"good for communication and room-temperature operations\" and references linear-optical computing and boson sampling (sources 1,5). It does not explicitly list \"photon loss\" or \"efficient two-photon interactions\" as the hurdles in the provided summaries. Those are plausible technical issues but are not present in the supplied material and thus are unsupported here.\n- Where to check in the JSON:\n  - t3.2 \"Other platforms: trapped ions, NMR, photonics, topological systems\" (references: 1,2,5).\n- Actionable fix:\n  - Either remove the specific hurdle claims or rephrase to match the JSON (e.g., \"practical hurdles remain for scalable, universal photonic quantum computing\" without enumerating specific mechanisms), or add a citation that supports the specific hurdles.\n\n3) Minor phrasing/detail additions\n- Problematic statements:\n  - Use of \"shaped pulses\" (already mentioned above) and the example hybrid strategy phrasing (\"using photonics for communication with matter-based qubits for processing\") — the latter is reasonable but more specific than the JSON explicitly states.\n- Why problematic:\n  - The INPUT JSON supports hybrid strategies in spirit via trade-offs and roles of photonics in communication (t3, t5), but the exact example is an extrapolation not verbatim in the provided summaries.\n- Where to check:\n  - t3 and t5.\n- Actionable fix:\n  - If keeping the hybrid example, slightly hedge it (e.g., \"for example, photonics is often proposed for communication roles while matter-based qubits are seen as processing elements\") or cite an appropriate source.\n\nSeverity summary\n- These are not contradictions of the JSON but are additions of technical specifics that are not present in the provided input. They should be revised to align strictly with the input document: remove or generalize the unsupported technical terms (dispersive measurement, shaped pulses, two-photon interactions, explicit enumeration of photon loss) unless you can add supporting citations from the sources listed in the INPUT JSON.",
    "3": "PASS",
    "4": "PASS",
    "5": "Issue (minor, single unsupported phrase):\n\n- The phrase \"continuous-variable representations\" in the \"Fermionic and bosonic encodings\" paragraph is not supported by the provided INPUT JSON. The JSON's t6.1 summary points mention bosonic carriers (photons, phonons) and their suitability for communication and some computation models (source 5), but they do not introduce or discuss \"continuous-variable representations.\" \n\nWhy this is problematic:\n- It introduces a technical term/claim that is not present in the referenced source summaries, so it goes beyond the supplied knowledge base and could mislead readers about what the source material states.\n\nWhere to fix (actionable):\n- Remove \"and continuous-variable representations\" or rephrase to match the INPUT JSON wording (e.g., refer simply to \"bosonic mode structure\" or \"bosonic mode degrees of freedom\" as in t6.1). If the author intends to keep \"continuous-variable\" they should supply a supporting source from the provided list or an additional reference.",
    "6": "PASS",
    "7": "PASS"
  },
  "is_criticized": true
}