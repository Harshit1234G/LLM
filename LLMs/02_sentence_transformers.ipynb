{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5aba67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b0d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa02b7f8",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f686f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0506c776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python\\LLM\\llm_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c84664bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03387694,  0.0919416 ,  0.04870139, ..., -0.01439267,\n",
       "        -0.02754978,  0.04475824],\n",
       "       [ 0.00504997,  0.06316976,  0.01415726, ...,  0.04035439,\n",
       "         0.07584126,  0.09087352],\n",
       "       [-0.00248314,  0.091517  ,  0.04838616, ..., -0.0264111 ,\n",
       "        -0.07529819,  0.02803203],\n",
       "       [-0.01629126,  0.10406609,  0.09740777, ...,  0.00676729,\n",
       "        -0.08788462,  0.03404385]], shape=(4, 384), dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a05de646",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = model.similarity(embeddings, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35d0ffa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.6946, 0.9429, 0.2569],\n",
       "        [0.6946, 1.0000, 0.6211, 0.2491],\n",
       "        [0.9429, 0.6211, 1.0000, 0.2106],\n",
       "        [0.2569, 0.2491, 0.2106, 1.0000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c117ea",
   "metadata": {},
   "source": [
    "## Reranker\n",
    "1. Calculates a similarity score given pairs of texts.\n",
    "2. Generally provides superior performance compared to a Sentence Transformer (a.k.a. bi-encoder) model.\n",
    "3. Often slower than a Sentence Transformer model, as it requires computation for each pair rather than each text.\n",
    "4. Due to the previous 2 characteristics, Cross Encoders are often used to re-rank the top-k results from a Sentence Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76a0cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34ec74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d21a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The texts for which to predict similarity scores\n",
    "query = \"How many people live in Berlin?\"\n",
    "\n",
    "passages = [\n",
    "    \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\",\n",
    "    \"Berlin has a yearly total of about 135 million day visitors, making it one of the most-visited cities in the European Union.\",\n",
    "    \"In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f018f316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python\\LLM\\llm_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([8.607141 , 5.506264 , 6.3529844], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_encoder.predict([(query, passage) for passage in passages])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532f96c3",
   "metadata": {},
   "source": [
    "We can also do the same using the `rank` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bee5281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python\\LLM\\llm_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'corpus_id': 0, 'score': np.float32(8.607141)},\n",
       " {'corpus_id': 2, 'score': np.float32(6.3529844)},\n",
       " {'corpus_id': 1, 'score': np.float32(5.506264)}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank = cross_encoder.rank(query, passages)\n",
    "rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb83a0",
   "metadata": {},
   "source": [
    "## Sparse Encoders\n",
    "1. Calculates sparse vector representations where most dimensions are zero\n",
    "2. Provides efficiency benefits for large-scale retrieval systems due to the sparse nature of embeddings\n",
    "3. Often more interpretable than dense embeddings, with non-zero dimensions corresponding to specific tokens\n",
    "4. Complementary to dense embeddings, enabling hybrid search systems that combine the strengths of both approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7227e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SparseEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f2d58c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python\\LLM\\llm_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Harshit\\.cache\\huggingface\\hub\\models--naver--splade-cocondenser-ensembledistil. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model = SparseEncoder('naver/splade-cocondenser-ensembledistil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b849d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python\\LLM\\llm_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06882b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[    0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     1,     1,     1,     1,\n",
       "                            1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                            1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                            1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                            1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                            1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                            1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                            1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                            1,     1,     2,     2,     2,     2,     2,     2,\n",
       "                            2,     2,     2,     2,     2,     2,     2,     2,\n",
       "                            2,     2,     2,     2,     2,     2,     2,     2,\n",
       "                            2,     2,     2,     2,     2,     2,     2,     2,\n",
       "                            2,     2,     2,     2,     2,     2,     2,     2,\n",
       "                            2,     2,     2,     2,     2,     2,     2,     2,\n",
       "                            2,     2,     2,     2,     2,     2,     2,     2,\n",
       "                            2,     2,     2,     2,     3,     3,     3,     3,\n",
       "                            3,     3,     3,     3,     3,     3,     3,     3,\n",
       "                            3,     3,     3,     3,     3,     3,     3,     3,\n",
       "                            3,     3,     3,     3,     3,     3,     3,     3,\n",
       "                            3,     3,     3,     3,     3,     3,     3,     3,\n",
       "                            3,     3,     3,     3,     3,     3,     3,     3],\n",
       "                       [ 1037,  2008,  2040,  2056,  2108,  2111,  2166,  2190,\n",
       "                         2204,  2283,  2293,  2360,  2619,  2643,  2711,  2726,\n",
       "                         2767,  2868,  3112,  3407,  3520,  3521,  3574,  3581,\n",
       "                         3835,  4111,  4205,  4422,  4471,  4489,  4676,  4695,\n",
       "                         4706,  4756,  4847,  4913,  4918,  4963,  5074,  5165,\n",
       "                         5177,  5379,  5381,  5580,  5629,  5637,  5639,  6060,\n",
       "                         6108,  6128,  6180,  6213,  6330,  6425,  6426,  6517,\n",
       "                         6569,  6754,  6796,  7063,  7084,  7150,  7177,  7239,\n",
       "                         7243,  7530,  7574,  7603,  7627,  7733,  8242,  8404,\n",
       "                         8562,  8809,  8925,  9895,  9967, 11883, 12511, 12831,\n",
       "                        13301, 13670, 14686, 15398,  1037,  2002,  2008,  2040,\n",
       "                         2056,  2190,  2204,  2293,  2643,  2711,  2748,  2767,\n",
       "                         2839,  2868,  3226,  3297,  3407,  3521,  3581,  3585,\n",
       "                         3835,  3899,  4111,  4172,  4368,  4743,  4756,  4918,\n",
       "                         4963,  5006,  5074,  5248,  5292,  5379,  5580,  5629,\n",
       "                         5637,  5639,  6060,  6071,  6077,  6108,  6128,  6180,\n",
       "                         6330,  6517,  7087,  7239,  7530,  8404,  8562,  8809,\n",
       "                         8843,  8925,  8937,  9004,  9576, 10131, 10140, 12852,\n",
       "                        20716, 26568,  1037,  2008,  2040,  2056,  2108,  2111,\n",
       "                         2158,  2190,  2200,  2204,  2293,  2360,  2428,  2619,\n",
       "                         2643,  2711,  2748,  2767,  2868,  3112,  3124,  3376,\n",
       "                         3407,  3492,  3521,  3835,  4471,  4756,  4918,  4963,\n",
       "                         5186,  5379,  5381,  5629,  5637,  5639,  5798,  6108,\n",
       "                         6180,  6330,  6517,  6569,  6754,  7150,  7177,  7239,\n",
       "                         7243,  7603,  8242,  8404,  8562,  8958,  9895, 12511,\n",
       "                        13301, 14686, 15398, 22193,  2085,  2154,  2204,  2420,\n",
       "                         2422,  2621,  2651,  2782,  2783,  2851,  3103,  3267,\n",
       "                         3467,  3533,  3679,  3835,  4040,  4189,  4408,  4422,\n",
       "                         4465,  4542,  4633,  4860,  5529,  5708,  5798,  6108,\n",
       "                         6209,  6302,  6425,  6426,  6888,  7150,  7483,  8242,\n",
       "                         8404,  8709,  9325,  9448,  9609, 11559, 12342, 18883]]),\n",
       "       values=tensor([5.3225e-01, 7.1872e-01, 4.6645e-01, 1.9014e-01,\n",
       "                      2.4640e-01, 1.3364e+00, 5.0283e-02, 4.0348e-01,\n",
       "                      5.8376e-01, 4.7200e-02, 8.3554e-02, 5.2584e-02,\n",
       "                      8.2587e-01, 5.7583e-01, 2.1716e+00, 2.3777e-01,\n",
       "                      2.7906e-01, 5.7958e-01, 8.4526e-02, 3.1279e+00,\n",
       "                      1.6571e-01, 6.2068e-01, 1.6066e-01, 8.1622e-03,\n",
       "                      2.0866e-01, 2.7693e-02, 3.8660e-02, 1.2299e-01,\n",
       "                      2.4335e-01, 1.1342e-02, 1.6785e-01, 1.1195e-01,\n",
       "                      6.0971e-02, 3.1253e-01, 1.3671e-01, 3.9299e-02,\n",
       "                      1.8767e-01, 6.2464e-01, 4.5621e-02, 2.1183e-02,\n",
       "                      6.4090e-04, 8.0093e-03, 1.4778e+00, 1.7445e-01,\n",
       "                      2.6667e-01, 2.8485e-01, 2.7935e-01, 3.5943e-01,\n",
       "                      2.8832e-01, 3.4090e-02, 6.3688e-01, 4.5699e-02,\n",
       "                      3.4040e-01, 8.2391e-02, 1.4659e-01, 3.3361e-01,\n",
       "                      3.9152e-01, 2.9823e-01, 1.0370e-01, 6.7244e-02,\n",
       "                      9.2042e-02, 3.8755e-01, 7.2520e-02, 1.8671e-01,\n",
       "                      6.7149e-01, 3.3250e-02, 1.0617e-01, 2.3532e-01,\n",
       "                      3.2487e-03, 6.5954e-02, 1.9054e-01, 1.6158e+00,\n",
       "                      4.7394e-01, 1.3721e-01, 3.1565e-02, 4.3890e-01,\n",
       "                      2.4273e-02, 5.0155e-02, 6.3098e-02, 6.4235e-01,\n",
       "                      9.7032e-02, 2.6891e-02, 4.2478e-02, 1.2239e-01,\n",
       "                      4.7187e-01, 6.6751e-02, 6.9197e-01, 7.6335e-02,\n",
       "                      1.3866e-02, 3.1600e-01, 4.5975e-01, 1.4911e-01,\n",
       "                      1.6368e-01, 3.5905e-01, 2.5534e-01, 3.8556e-01,\n",
       "                      3.5014e-01, 5.5295e-01, 2.7781e-02, 2.4190e-02,\n",
       "                      3.1281e+00, 2.6278e-01, 2.4213e-02, 6.6465e-01,\n",
       "                      1.3679e-01, 2.6057e+00, 7.9063e-01, 6.4469e-02,\n",
       "                      3.7645e-02, 5.1847e-01, 2.1548e-01, 3.3849e-01,\n",
       "                      2.2357e-01, 5.1477e-02, 1.2662e-01, 1.5228e-01,\n",
       "                      1.5964e-01, 4.8447e-01, 3.9006e-01, 1.7336e-01,\n",
       "                      1.8293e-01, 1.1016e-01, 2.1933e-01, 3.6651e-02,\n",
       "                      2.2908e+00, 2.5498e-01, 3.6667e-01, 4.3060e-01,\n",
       "                      2.1285e-01, 3.2051e-01, 1.3665e-01, 1.8107e-01,\n",
       "                      1.2335e-01, 1.3649e+00, 4.3966e-01, 1.2103e-02,\n",
       "                      8.2142e-01, 2.5954e-01, 2.1135e-02, 8.5634e-01,\n",
       "                      1.8289e-01, 3.0918e-01, 1.0121e-01, 1.2556e-01,\n",
       "                      7.5899e-02, 2.3175e-01, 3.8128e-01, 3.6481e-01,\n",
       "                      4.6321e-01, 2.8423e-01, 5.2163e-02, 1.1921e+00,\n",
       "                      1.7666e-01, 4.2727e-01, 2.4069e+00, 6.4893e-01,\n",
       "                      3.6065e-02, 1.5561e-01, 6.3890e-01, 8.3386e-01,\n",
       "                      4.8428e-01, 1.9940e+00, 4.3587e-02, 2.9715e-01,\n",
       "                      6.0029e-01, 1.3999e-01, 6.0227e-02, 9.2943e-02,\n",
       "                      3.0424e+00, 1.0962e-02, 4.6808e-01, 4.4300e-01,\n",
       "                      1.7049e-02, 1.9945e-01, 1.2235e-01, 4.3575e-01,\n",
       "                      6.1308e-01, 4.5237e-02, 1.2592e+00, 2.9568e-01,\n",
       "                      3.3298e-01, 1.3368e-01, 5.3102e-02, 8.9899e-02,\n",
       "                      7.1720e-01, 8.4035e-02, 3.3821e-01, 1.2377e-01,\n",
       "                      1.8320e-01, 2.1378e-01, 2.5473e-01, 4.2252e-02,\n",
       "                      3.7944e-01, 2.9351e-02, 3.2200e-01, 1.4369e+00,\n",
       "                      3.0127e-01, 1.4638e-01, 4.1161e-01, 1.4753e-01,\n",
       "                      4.4205e-02, 1.2302e-01, 5.6165e-02, 3.2990e-02,\n",
       "                      1.5328e+00, 2.1079e+00, 2.7834e-01, 9.3323e-01,\n",
       "                      2.7788e-01, 1.0115e+00, 2.6351e+00, 2.0381e-01,\n",
       "                      7.9401e-01, 8.3704e-01, 1.0531e+00, 6.8654e-02,\n",
       "                      6.8546e-01, 2.3220e-01, 1.0415e-01, 7.1008e-02,\n",
       "                      1.9646e-01, 1.5488e-03, 1.7065e+00, 3.3263e-02,\n",
       "                      8.4300e-02, 6.5091e-01, 1.2776e+00, 4.8459e-01,\n",
       "                      2.1894e-02, 4.9068e-02, 1.2022e-01, 3.8904e-01,\n",
       "                      2.0668e-01, 1.5230e-01, 8.8120e-02, 2.9523e-01,\n",
       "                      1.2100e-01, 2.4307e-02, 8.0717e-01, 5.9658e-01,\n",
       "                      3.8958e-01, 6.3995e-02, 6.2725e-02, 1.3474e-02,\n",
       "                      3.9588e-01, 2.9729e+00, 5.3218e-02, 8.7000e-01]),\n",
       "       size=(4, 30522), nnz=248, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6307c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
