{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8a211d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "Â  <a href=\"https://ollama.com\">\n",
    "    <img alt=\"ollama\" width=\"240\" src=\"https://github.com/ollama/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7\">\n",
    "  </a>\n",
    "</div>\n",
    "\n",
    "# Ollama\n",
    "\n",
    "Get up and running with large language models.\n",
    "\n",
    "### macOS\n",
    "\n",
    "[Download](https://ollama.com/download/Ollama.dmg)\n",
    "\n",
    "### Windows\n",
    "\n",
    "[Download](https://ollama.com/download/OllamaSetup.exe)\n",
    "\n",
    "### Linux\n",
    "\n",
    "```shell\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "[Manual install instructions](https://github.com/ollama/ollama/blob/main/docs/linux.md)\n",
    "\n",
    "### Docker\n",
    "\n",
    "The official [Ollama Docker image](https://hub.docker.com/r/ollama/ollama) `ollama/ollama` is available on Docker Hub.\n",
    "\n",
    "### Libraries\n",
    "\n",
    "- [ollama-python](https://github.com/ollama/ollama-python)\n",
    "- [ollama-js](https://github.com/ollama/ollama-js)\n",
    "\n",
    "### Community\n",
    "\n",
    "- [Discord](https://discord.gg/ollama)\n",
    "- [Reddit](https://reddit.com/r/ollama)\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "To run and chat with [Gemma 3](https://ollama.com/library/gemma3):\n",
    "\n",
    "```shell\n",
    "ollama run gemma3\n",
    "```\n",
    "\n",
    "## Model library\n",
    "\n",
    "Ollama supports a list of models available on [ollama.com/library](https://ollama.com/library 'ollama model library')\n",
    "\n",
    "Here are some example models that can be downloaded:\n",
    "\n",
    "| Model              | Parameters | Size  | Download                         |\n",
    "| ------------------ | ---------- | ----- | -------------------------------- |\n",
    "| Gemma 3            | 1B         | 815MB | `ollama run gemma3:1b`           |\n",
    "| Gemma 3            | 4B         | 3.3GB | `ollama run gemma3`              |\n",
    "| Gemma 3            | 12B        | 8.1GB | `ollama run gemma3:12b`          |\n",
    "| Gemma 3            | 27B        | 17GB  | `ollama run gemma3:27b`          |\n",
    "| QwQ                | 32B        | 20GB  | `ollama run qwq`                 |\n",
    "| DeepSeek-R1        | 7B         | 4.7GB | `ollama run deepseek-r1`         |\n",
    "| DeepSeek-R1        | 671B       | 404GB | `ollama run deepseek-r1:671b`    |\n",
    "| Llama 4            | 109B       | 67GB  | `ollama run llama4:scout`        |\n",
    "| Llama 4            | 400B       | 245GB | `ollama run llama4:maverick`     |\n",
    "| Llama 3.3          | 70B        | 43GB  | `ollama run llama3.3`            |\n",
    "| Llama 3.2          | 3B         | 2.0GB | `ollama run llama3.2`            |\n",
    "| Llama 3.2          | 1B         | 1.3GB | `ollama run llama3.2:1b`         |\n",
    "| Llama 3.2 Vision   | 11B        | 7.9GB | `ollama run llama3.2-vision`     |\n",
    "| Llama 3.2 Vision   | 90B        | 55GB  | `ollama run llama3.2-vision:90b` |\n",
    "| Llama 3.1          | 8B         | 4.7GB | `ollama run llama3.1`            |\n",
    "| Llama 3.1          | 405B       | 231GB | `ollama run llama3.1:405b`       |\n",
    "| Phi 4              | 14B        | 9.1GB | `ollama run phi4`                |\n",
    "| Phi 4 Mini         | 3.8B       | 2.5GB | `ollama run phi4-mini`           |\n",
    "| Mistral            | 7B         | 4.1GB | `ollama run mistral`             |\n",
    "| Moondream 2        | 1.4B       | 829MB | `ollama run moondream`           |\n",
    "| Neural Chat        | 7B         | 4.1GB | `ollama run neural-chat`         |\n",
    "| Starling           | 7B         | 4.1GB | `ollama run starling-lm`         |\n",
    "| Code Llama         | 7B         | 3.8GB | `ollama run codellama`           |\n",
    "| Llama 2 Uncensored | 7B         | 3.8GB | `ollama run llama2-uncensored`   |\n",
    "| LLaVA              | 7B         | 4.5GB | `ollama run llava`               |\n",
    "| Granite-3.3         | 8B         | 4.9GB | `ollama run granite3.3`          |\n",
    "\n",
    "> [!NOTE]\n",
    "> You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.\n",
    "\n",
    "## Customize a model\n",
    "\n",
    "### Import from GGUF\n",
    "\n",
    "Ollama supports importing GGUF models in the Modelfile:\n",
    "\n",
    "1. Create a file named `Modelfile`, with a `FROM` instruction with the local filepath to the model you want to import.\n",
    "\n",
    "   ```\n",
    "   FROM ./vicuna-33b.Q4_0.gguf\n",
    "   ```\n",
    "\n",
    "2. Create the model in Ollama\n",
    "\n",
    "   ```shell\n",
    "   ollama create example -f Modelfile\n",
    "   ```\n",
    "\n",
    "3. Run the model\n",
    "\n",
    "   ```shell\n",
    "   ollama run example\n",
    "   ```\n",
    "\n",
    "### Import from Safetensors\n",
    "\n",
    "See the [guide](docs/import.md) on importing models for more information.\n",
    "\n",
    "### Customize a prompt\n",
    "\n",
    "Models from the Ollama library can be customized with a prompt. For example, to customize the `llama3.2` model:\n",
    "\n",
    "```shell\n",
    "ollama pull llama3.2\n",
    "```\n",
    "\n",
    "Create a `Modelfile`:\n",
    "\n",
    "```\n",
    "FROM llama3.2\n",
    "\n",
    "# set the temperature to 1 [higher is more creative, lower is more coherent]\n",
    "PARAMETER temperature 1\n",
    "\n",
    "# set the system message\n",
    "SYSTEM \"\"\"\n",
    "You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Next, create and run the model:\n",
    "\n",
    "```\n",
    "ollama create mario -f ./Modelfile\n",
    "ollama run mario\n",
    ">>> hi\n",
    "Hello! It's your friend Mario.\n",
    "```\n",
    "\n",
    "For more information on working with a Modelfile, see the [Modelfile](docs/modelfile.md) documentation.\n",
    "\n",
    "## CLI Reference\n",
    "\n",
    "### Create a model\n",
    "\n",
    "`ollama create` is used to create a model from a Modelfile.\n",
    "\n",
    "```shell\n",
    "ollama create mymodel -f ./Modelfile\n",
    "```\n",
    "\n",
    "### Pull a model\n",
    "\n",
    "```shell\n",
    "ollama pull llama3.2\n",
    "```\n",
    "\n",
    "> This command can also be used to update a local model. Only the diff will be pulled.\n",
    "\n",
    "### Remove a model\n",
    "\n",
    "```shell\n",
    "ollama rm llama3.2\n",
    "```\n",
    "\n",
    "### Copy a model\n",
    "\n",
    "```shell\n",
    "ollama cp llama3.2 my-model\n",
    "```\n",
    "\n",
    "### Multiline input\n",
    "\n",
    "For multiline input, you can wrap text with `\"\"\"`:\n",
    "\n",
    "```\n",
    ">>> \"\"\"Hello,\n",
    "... world!\n",
    "... \"\"\"\n",
    "I'm a basic program that prints the famous \"Hello, world!\" message to the console.\n",
    "```\n",
    "\n",
    "### Multimodal models\n",
    "\n",
    "```\n",
    "ollama run llava \"What's in this image? /Users/jmorgan/Desktop/smile.png\"\n",
    "```\n",
    "\n",
    "> **Output**: The image features a yellow smiley face, which is likely the central focus of the picture.\n",
    "\n",
    "### Pass the prompt as an argument\n",
    "\n",
    "```shell\n",
    "ollama run llama3.2 \"Summarize this file: $(cat README.md)\"\n",
    "```\n",
    "\n",
    "> **Output**: Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications.\n",
    "\n",
    "### Show model information\n",
    "\n",
    "```shell\n",
    "ollama show llama3.2\n",
    "```\n",
    "\n",
    "### List models on your computer\n",
    "\n",
    "```shell\n",
    "ollama list\n",
    "```\n",
    "\n",
    "### List which models are currently loaded\n",
    "\n",
    "```shell\n",
    "ollama ps\n",
    "```\n",
    "\n",
    "### Stop a model which is currently running\n",
    "\n",
    "```shell\n",
    "ollama stop llama3.2\n",
    "```\n",
    "\n",
    "### Start Ollama\n",
    "\n",
    "`ollama serve` is used when you want to start ollama without running the desktop application.\n",
    "\n",
    "## Building\n",
    "\n",
    "See the [developer guide](https://github.com/ollama/ollama/blob/main/docs/development.md)\n",
    "\n",
    "### Running local builds\n",
    "\n",
    "Next, start the server:\n",
    "\n",
    "```shell\n",
    "./ollama serve\n",
    "```\n",
    "\n",
    "Finally, in a separate shell, run a model:\n",
    "\n",
    "```shell\n",
    "./ollama run llama3.2\n",
    "```\n",
    "\n",
    "## REST API\n",
    "\n",
    "Ollama has a REST API for running and managing models.\n",
    "\n",
    "### Generate a response\n",
    "\n",
    "```shell\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"llama3.2\",\n",
    "  \"prompt\":\"Why is the sky blue?\"\n",
    "}'\n",
    "```\n",
    "\n",
    "### Chat with a model\n",
    "\n",
    "```shell\n",
    "curl http://localhost:11434/api/chat -d '{\n",
    "  \"model\": \"llama3.2\",\n",
    "  \"messages\": [\n",
    "    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n",
    "  ]\n",
    "}'\n",
    "```\n",
    "\n",
    "See the [API documentation](./docs/api.md) for all endpoints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b421f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
